train.py  --name  4class-resnet-car-cat-chair-horse  --dataroot  ./datasets/ForenSynths_train_val  --classes  car,cat,chair,horse  --batch_size  32  --delr_freq  10  --lr  0.0002  --niter  50
cwd: /home/chenyuheng/Code/NPR-DeepfakeDetection
2025_03_01_14_35_29 Train loss: 0.7026922106742859 at step: 400 lr 0.0002
2025_03_01_14_36_02 Train loss: 0.10322199761867523 at step: 800 lr 0.0002
2025_03_01_14_36_36 Train loss: 0.1031353697180748 at step: 1200 lr 0.0002
2025_03_01_14_37_09 Train loss: 0.03780873492360115 at step: 1600 lr 0.0002
2025_03_01_14_37_43 Train loss: 0.28335511684417725 at step: 2000 lr 0.0002
2025_03_01_14_38_16 Train loss: 0.0071293069049716 at step: 2400 lr 0.0002
2025_03_01_14_38_50 Train loss: 0.011439025402069092 at step: 2800 lr 0.0002
2025_03_01_14_39_24 Train loss: 0.09196258336305618 at step: 3200 lr 0.0002
2025_03_01_14_39_57 Train loss: 0.00923626683652401 at step: 3600 lr 0.0002
2025_03_01_14_40_31 Train loss: 0.0029521691612899303 at step: 4000 lr 0.0002
2025_03_01_14_41_05 Train loss: 0.0006278851651586592 at step: 4400 lr 0.0002
(Val @ epoch 0) acc: 0.998125; ap: 0.9999953202930174
2025_03_01_14_41_40 Train loss: 0.004821326583623886 at step: 4800 lr 0.0002
2025_03_01_14_42_14 Train loss: 0.0012475720141083002 at step: 5200 lr 0.0002
2025_03_01_14_42_48 Train loss: 0.001842627301812172 at step: 5600 lr 0.0002
2025_03_01_14_43_21 Train loss: 0.006848514080047607 at step: 6000 lr 0.0002
2025_03_01_14_43_55 Train loss: 0.029844990000128746 at step: 6400 lr 0.0002
2025_03_01_14_44_29 Train loss: 0.00017049851885531098 at step: 6800 lr 0.0002
2025_03_01_14_45_03 Train loss: 0.000418461742810905 at step: 7200 lr 0.0002
2025_03_01_14_45_37 Train loss: 6.295516504906118e-05 at step: 7600 lr 0.0002
2025_03_01_14_46_11 Train loss: 0.05083782970905304 at step: 8000 lr 0.0002
2025_03_01_14_46_44 Train loss: 0.03198133036494255 at step: 8400 lr 0.0002
2025_03_01_14_47_18 Train loss: 0.0004317697894293815 at step: 8800 lr 0.0002
(Val @ epoch 1) acc: 0.998125; ap: 0.9999922360248448
2025_03_01_14_47_54 Train loss: 0.0011471589095890522 at step: 9200 lr 0.0002
2025_03_01_14_48_28 Train loss: 0.016902446746826172 at step: 9600 lr 0.0002
2025_03_01_14_49_02 Train loss: 0.01769006811082363 at step: 10000 lr 0.0002
2025_03_01_14_49_36 Train loss: 0.029140092432498932 at step: 10400 lr 0.0002
2025_03_01_14_50_10 Train loss: 0.0008435230702161789 at step: 10800 lr 0.0002
2025_03_01_14_50_44 Train loss: 7.316103619814385e-06 at step: 11200 lr 0.0002
2025_03_01_14_51_18 Train loss: 0.005299501121044159 at step: 11600 lr 0.0002
2025_03_01_14_51_52 Train loss: 0.00016206485452130437 at step: 12000 lr 0.0002
2025_03_01_14_52_26 Train loss: 0.0007136479835025966 at step: 12400 lr 0.0002
2025_03_01_14_53_00 Train loss: 0.001179744373075664 at step: 12800 lr 0.0002
2025_03_01_14_53_34 Train loss: 5.5431223699997645e-06 at step: 13200 lr 0.0002
(Val @ epoch 2) acc: 0.999375; ap: 1.0
2025_03_01_14_54_10 Train loss: 0.2369202971458435 at step: 13600 lr 0.0002
2025_03_01_14_54_44 Train loss: 0.0001442541106371209 at step: 14000 lr 0.0002
2025_03_01_14_55_18 Train loss: 0.00011048670421587303 at step: 14400 lr 0.0002
2025_03_01_14_55_52 Train loss: 0.001197683741338551 at step: 14800 lr 0.0002
2025_03_01_14_56_26 Train loss: 0.007027783896774054 at step: 15200 lr 0.0002
2025_03_01_14_57_00 Train loss: 5.219264858169481e-05 at step: 15600 lr 0.0002
2025_03_01_14_57_34 Train loss: 0.001432055258192122 at step: 16000 lr 0.0002
2025_03_01_14_58_08 Train loss: 0.0015171471750363708 at step: 16400 lr 0.0002
2025_03_01_14_58_42 Train loss: 0.003972274716943502 at step: 16800 lr 0.0002
2025_03_01_14_59_16 Train loss: 0.0019039928447455168 at step: 17200 lr 0.0002
2025_03_01_14_59_50 Train loss: 0.00023878077627159655 at step: 17600 lr 0.0002
2025_03_01_15_00_24 Train loss: 0.0008753452566452324 at step: 18000 lr 0.0002
(Val @ epoch 3) acc: 0.99875; ap: 0.9999906947890819
2025_03_01_15_01_00 Train loss: 0.047060299664735794 at step: 18400 lr 0.0002
2025_03_01_15_01_34 Train loss: 0.003019958036020398 at step: 18800 lr 0.0002
2025_03_01_15_02_08 Train loss: 0.0007659387774765491 at step: 19200 lr 0.0002
2025_03_01_15_02_42 Train loss: 0.0019020515028387308 at step: 19600 lr 0.0002
2025_03_01_15_03_17 Train loss: 1.9594685909396503e-06 at step: 20000 lr 0.0002
2025_03_01_15_03_51 Train loss: 0.0004074484168086201 at step: 20400 lr 0.0002
2025_03_01_15_04_25 Train loss: 8.552811777917668e-06 at step: 20800 lr 0.0002
2025_03_01_15_04_59 Train loss: 1.5952229659887962e-05 at step: 21200 lr 0.0002
2025_03_01_15_05_33 Train loss: 0.0011155889369547367 at step: 21600 lr 0.0002
2025_03_01_15_06_07 Train loss: 0.0003869500942528248 at step: 22000 lr 0.0002
2025_03_01_15_06_41 Train loss: 0.09788475930690765 at step: 22400 lr 0.0002
(Val @ epoch 4) acc: 0.99875; ap: 1.0
2025_03_01_15_07_17 Train loss: 0.004289053846150637 at step: 22800 lr 0.0002
2025_03_01_15_07_51 Train loss: 0.00020115237566642463 at step: 23200 lr 0.0002
2025_03_01_15_08_25 Train loss: 0.00014954789367038757 at step: 23600 lr 0.0002
2025_03_01_15_08_59 Train loss: 2.129935637640301e-05 at step: 24000 lr 0.0002
2025_03_01_15_09_33 Train loss: 0.00012922563473694026 at step: 24400 lr 0.0002
2025_03_01_15_10_07 Train loss: 0.02732364647090435 at step: 24800 lr 0.0002
2025_03_01_15_10_41 Train loss: 0.00014001451199874282 at step: 25200 lr 0.0002
2025_03_01_15_11_15 Train loss: 2.7454789233161137e-06 at step: 25600 lr 0.0002
2025_03_01_15_11_49 Train loss: 4.4849068217445165e-05 at step: 26000 lr 0.0002
2025_03_01_15_12_23 Train loss: 0.00013942540681455284 at step: 26400 lr 0.0002
2025_03_01_15_12_57 Train loss: 5.960449698250159e-07 at step: 26800 lr 0.0002
(Val @ epoch 5) acc: 1.0; ap: 1.0
2025_03_01_15_13_33 Train loss: 5.945394150330685e-06 at step: 27200 lr 0.0002
2025_03_01_15_14_07 Train loss: 6.276929980231216e-06 at step: 27600 lr 0.0002
2025_03_01_15_14_41 Train loss: 0.0014159390702843666 at step: 28000 lr 0.0002
2025_03_01_15_15_16 Train loss: 0.008968156762421131 at step: 28400 lr 0.0002
2025_03_01_15_15_50 Train loss: 0.004057952668517828 at step: 28800 lr 0.0002
2025_03_01_15_16_24 Train loss: 0.0005756521131843328 at step: 29200 lr 0.0002
2025_03_01_15_16_58 Train loss: 2.049405338766519e-05 at step: 29600 lr 0.0002
2025_03_01_15_17_32 Train loss: 3.1468771339859813e-05 at step: 30000 lr 0.0002
2025_03_01_15_18_06 Train loss: 6.485935591626912e-05 at step: 30400 lr 0.0002
2025_03_01_15_18_40 Train loss: 0.0016440509352833033 at step: 30800 lr 0.0002
2025_03_01_15_19_14 Train loss: 4.028704279335216e-05 at step: 31200 lr 0.0002
(Val @ epoch 6) acc: 1.0; ap: 1.0
2025_03_01_15_19_50 Train loss: 0.0010091083822771907 at step: 31600 lr 0.0002
2025_03_01_15_20_24 Train loss: 0.0038914650212973356 at step: 32000 lr 0.0002
2025_03_01_15_20_58 Train loss: 1.0894606930378359e-05 at step: 32400 lr 0.0002
2025_03_01_15_21_32 Train loss: 0.0004146297287661582 at step: 32800 lr 0.0002
2025_03_01_15_22_06 Train loss: 3.6208780329616275e-06 at step: 33200 lr 0.0002
2025_03_01_15_22_40 Train loss: 0.018231818452477455 at step: 33600 lr 0.0002
2025_03_01_15_23_14 Train loss: 0.00033707727561704814 at step: 34000 lr 0.0002
2025_03_01_15_23_48 Train loss: 1.5033733689051587e-05 at step: 34400 lr 0.0002
2025_03_01_15_24_22 Train loss: 2.2837883079773746e-05 at step: 34800 lr 0.0002
2025_03_01_15_24_56 Train loss: 0.000829921627882868 at step: 35200 lr 0.0002
2025_03_01_15_25_30 Train loss: 5.466999937198125e-05 at step: 35600 lr 0.0002
2025_03_01_15_26_04 Train loss: 0.0036985441111028194 at step: 36000 lr 0.0002
(Val @ epoch 7) acc: 1.0; ap: 1.0
2025_03_01_15_26_40 Train loss: 0.00016502132348250598 at step: 36400 lr 0.0002
2025_03_01_15_27_14 Train loss: 0.00027772964676842093 at step: 36800 lr 0.0002
2025_03_01_15_27_48 Train loss: 2.928021103798528e-06 at step: 37200 lr 0.0002
2025_03_01_15_28_22 Train loss: 3.471897343843011e-06 at step: 37600 lr 0.0002
2025_03_01_15_28_56 Train loss: 0.0006182421930134296 at step: 38000 lr 0.0002
2025_03_01_15_29_30 Train loss: 5.7958470279118046e-05 at step: 38400 lr 0.0002
2025_03_01_15_30_04 Train loss: 2.644913593030651e-06 at step: 38800 lr 0.0002
2025_03_01_15_30_38 Train loss: 9.326179861091077e-05 at step: 39200 lr 0.0002
2025_03_01_15_31_12 Train loss: 0.01221520733088255 at step: 39600 lr 0.0002
2025_03_01_15_31_46 Train loss: 0.01563468761742115 at step: 40000 lr 0.0002
2025_03_01_15_32_20 Train loss: 1.6307152691297233e-05 at step: 40400 lr 0.0002
(Val @ epoch 8) acc: 0.99875; ap: 1.0
2025_03_01_15_32_56 Train loss: 5.587916120930458e-07 at step: 40800 lr 0.0002
2025_03_01_15_33_30 Train loss: 3.22225532727316e-05 at step: 41200 lr 0.0002
2025_03_01_15_34_04 Train loss: 6.749209569534287e-05 at step: 41600 lr 0.0002
2025_03_01_15_34_38 Train loss: 2.7939631763729267e-07 at step: 42000 lr 0.0002
2025_03_01_15_35_13 Train loss: 0.0027764751575887203 at step: 42400 lr 0.0002
2025_03_01_15_35_47 Train loss: 8.679867278260645e-07 at step: 42800 lr 0.0002
2025_03_01_15_36_21 Train loss: 2.992741974594537e-05 at step: 43200 lr 0.0002
2025_03_01_15_36_55 Train loss: 0.00018667754193302244 at step: 43600 lr 0.0002
2025_03_01_15_37_29 Train loss: 6.981752085266635e-05 at step: 44000 lr 0.0002
2025_03_01_15_38_03 Train loss: 0.000277740356978029 at step: 44400 lr 0.0002
2025_03_01_15_38_37 Train loss: 2.980229396598588e-07 at step: 44800 lr 0.0002
(Val @ epoch 9) acc: 1.0; ap: 1.0
2025_03_01_15_39_13 Train loss: 1.527741551399231e-05 at step: 45200 lr 0.0002
2025_03_01_15_39_48 Train loss: 2.728949039010331e-05 at step: 45600 lr 0.0002
2025_03_01_15_40_22 Train loss: 5.389947909861803e-05 at step: 46000 lr 0.0002
2025_03_01_15_40_56 Train loss: 0.004993345122784376 at step: 46400 lr 0.0002
2025_03_01_15_41_30 Train loss: 3.150042539346032e-05 at step: 46800 lr 0.0002
2025_03_01_15_42_04 Train loss: 0.000143988843774423 at step: 47200 lr 0.0002
2025_03_01_15_42_38 Train loss: 1.656775566516444e-05 at step: 47600 lr 0.0002
2025_03_01_15_43_13 Train loss: 3.628298145486042e-06 at step: 48000 lr 0.0002
2025_03_01_15_43_47 Train loss: 0.0031325281597673893 at step: 48400 lr 0.0002
2025_03_01_15_44_21 Train loss: 9.597518510418013e-05 at step: 48800 lr 0.0002
2025_03_01_15_44_55 Train loss: 1.251684125236352e-06 at step: 49200 lr 0.0002
2025_03_01_15_45_22 changing lr at the end of epoch 10, iters 49511
*************************
Changing lr from 0.0002 to 0.00018
*************************
(Val @ epoch 10) acc: 0.998125; ap: 1.0
2025_03_01_15_45_31 Train loss: 0.0001644227304495871 at step: 49600 lr 0.00018
2025_03_01_15_46_05 Train loss: 2.9714457923546433e-05 at step: 50000 lr 0.00018
2025_03_01_15_46_39 Train loss: 1.579606941959355e-05 at step: 50400 lr 0.00018
2025_03_01_15_47_14 Train loss: 1.4421269952435978e-05 at step: 50800 lr 0.00018
2025_03_01_15_47_48 Train loss: 5.494326160260243e-06 at step: 51200 lr 0.00018
2025_03_01_15_48_22 Train loss: 5.159203738003271e-06 at step: 51600 lr 0.00018
2025_03_01_15_48_56 Train loss: 1.5646195095087023e-07 at step: 52000 lr 0.00018
2025_03_01_15_49_31 Train loss: 0.00014750307309441268 at step: 52400 lr 0.00018
2025_03_01_15_50_05 Train loss: 1.758291887199448e-06 at step: 52800 lr 0.00018
2025_03_01_15_50_39 Train loss: 1.8066170014208183e-05 at step: 53200 lr 0.00018
2025_03_01_15_51_13 Train loss: 0.0 at step: 53600 lr 0.00018
2025_03_01_15_51_47 Train loss: 8.529889782948885e-06 at step: 54000 lr 0.00018
(Val @ epoch 11) acc: 1.0; ap: 1.0
2025_03_01_15_52_23 Train loss: 1.089488978323061e-05 at step: 54400 lr 0.00018
2025_03_01_15_52_58 Train loss: 1.0058275279334339e-07 at step: 54800 lr 0.00018
2025_03_01_15_53_32 Train loss: 0.0005340672214515507 at step: 55200 lr 0.00018
2025_03_01_15_54_06 Train loss: 0.00012989214155822992 at step: 55600 lr 0.00018
2025_03_01_15_54_40 Train loss: 1.7609265341889113e-05 at step: 56000 lr 0.00018
2025_03_01_15_55_14 Train loss: 4.153422651143046e-06 at step: 56400 lr 0.00018
2025_03_01_15_55_49 Train loss: 5.1630013331305236e-06 at step: 56800 lr 0.00018
2025_03_01_15_56_23 Train loss: 0.002195041161030531 at step: 57200 lr 0.00018
2025_03_01_15_56_57 Train loss: 1.9743976054087398e-07 at step: 57600 lr 0.00018
2025_03_01_15_57_31 Train loss: 1.8774991303871502e-06 at step: 58000 lr 0.00018
2025_03_01_15_58_05 Train loss: 0.001023025601170957 at step: 58400 lr 0.00018
(Val @ epoch 12) acc: 0.999375; ap: 1.0
2025_03_01_15_58_41 Train loss: 0.0008257558802142739 at step: 58800 lr 0.00018
2025_03_01_15_59_16 Train loss: 1.3150000768291648e-06 at step: 59200 lr 0.00018
2025_03_01_15_59_50 Train loss: 0.0016071735881268978 at step: 59600 lr 0.00018
2025_03_01_16_00_24 Train loss: 0.00011722736235242337 at step: 60000 lr 0.00018
2025_03_01_16_00_58 Train loss: 7.084731805662159e-06 at step: 60400 lr 0.00018
2025_03_01_16_01_32 Train loss: 2.976445784952375e-06 at step: 60800 lr 0.00018
2025_03_01_16_02_07 Train loss: 7.215076493594097e-06 at step: 61200 lr 0.00018
2025_03_01_16_02_41 Train loss: 2.715647269724286e-06 at step: 61600 lr 0.00018
2025_03_01_16_03_15 Train loss: 3.1328386285167653e-06 at step: 62000 lr 0.00018
2025_03_01_16_03_49 Train loss: 2.01592811208684e-05 at step: 62400 lr 0.00018
2025_03_01_16_04_23 Train loss: 1.8626446163239052e-08 at step: 62800 lr 0.00018
(Val @ epoch 13) acc: 1.0; ap: 1.0
2025_03_01_16_04_59 Train loss: 0.0009410757920704782 at step: 63200 lr 0.00018
2025_03_01_16_05_33 Train loss: 3.725290076417309e-09 at step: 63600 lr 0.00018
2025_03_01_16_06_07 Train loss: 7.556690252386034e-05 at step: 64000 lr 0.00018
2025_03_01_16_06_41 Train loss: 2.0116156065341784e-06 at step: 64400 lr 0.00018
2025_03_01_16_07_15 Train loss: 2.7194511176276137e-07 at step: 64800 lr 0.00018
2025_03_01_16_07_49 Train loss: 0.003265253035351634 at step: 65200 lr 0.00018
2025_03_01_16_08_23 Train loss: 5.04737135997857e-06 at step: 65600 lr 0.00018
2025_03_01_16_08_57 Train loss: 3.725289587919178e-08 at step: 66000 lr 0.00018
2025_03_01_16_09_32 Train loss: 3.948789810692688e-07 at step: 66400 lr 0.00018
2025_03_01_16_10_06 Train loss: 2.7587037038756534e-05 at step: 66800 lr 0.00018
2025_03_01_16_10_40 Train loss: 8.232788673012692e-07 at step: 67200 lr 0.00018
(Val @ epoch 14) acc: 0.999375; ap: 1.0
2025_03_01_16_11_15 Train loss: 2.0489036955950723e-07 at step: 67600 lr 0.00018
2025_03_01_16_11_49 Train loss: 1.415609034438603e-07 at step: 68000 lr 0.00018
2025_03_01_16_12_24 Train loss: 2.086158019665163e-07 at step: 68400 lr 0.00018
2025_03_01_16_12_58 Train loss: 0.00023773427528794855 at step: 68800 lr 0.00018
2025_03_01_16_13_32 Train loss: 6.966242267480993e-07 at step: 69200 lr 0.00018
2025_03_01_16_14_06 Train loss: 2.7939603342019836e-07 at step: 69600 lr 0.00018
2025_03_01_16_14_40 Train loss: 0.0 at step: 70000 lr 0.00018
2025_03_01_16_15_14 Train loss: 0.0016282942378893495 at step: 70400 lr 0.00018
2025_03_01_16_15_48 Train loss: 7.07804588273575e-08 at step: 70800 lr 0.00018
2025_03_01_16_16_22 Train loss: 0.09435064345598221 at step: 71200 lr 0.00018
2025_03_01_16_16_56 Train loss: 0.0003097612061537802 at step: 71600 lr 0.00018
2025_03_01_16_17_30 Train loss: 2.2314186480798526e-06 at step: 72000 lr 0.00018
(Val @ epoch 15) acc: 1.0; ap: 1.0
2025_03_01_16_18_06 Train loss: 2.2724215398284286e-07 at step: 72400 lr 0.00018
2025_03_01_16_18_40 Train loss: 1.1175869119028903e-08 at step: 72800 lr 0.00018
2025_03_01_16_19_14 Train loss: 3.9286936953430995e-05 at step: 73200 lr 0.00018
2025_03_01_16_19_48 Train loss: 9.387638328917092e-07 at step: 73600 lr 0.00018
2025_03_01_16_20_22 Train loss: 8.892107143765315e-05 at step: 74000 lr 0.00018
2025_03_01_16_20_56 Train loss: 1.4480704521702137e-05 at step: 74400 lr 0.00018
2025_03_01_16_21_30 Train loss: 5.140859116181673e-07 at step: 74800 lr 0.00018
2025_03_01_16_22_04 Train loss: 2.931752533186227e-06 at step: 75200 lr 0.00018
2025_03_01_16_22_38 Train loss: 2.72477791440906e-05 at step: 75600 lr 0.00018
2025_03_01_16_23_12 Train loss: 1.3424350072455127e-05 at step: 76000 lr 0.00018
2025_03_01_16_23_46 Train loss: 0.0002718997420743108 at step: 76400 lr 0.00018
(Val @ epoch 16) acc: 0.995; ap: 0.9998638344226758
2025_03_01_16_24_22 Train loss: 0.17666204273700714 at step: 76800 lr 0.00018
2025_03_01_16_24_56 Train loss: 3.094268686254509e-05 at step: 77200 lr 0.00018
2025_03_01_16_25_30 Train loss: 3.4272619586772635e-07 at step: 77600 lr 0.00018
2025_03_01_16_26_04 Train loss: 3.333371569169685e-05 at step: 78000 lr 0.00018
2025_03_01_16_26_38 Train loss: 0.0001121664245147258 at step: 78400 lr 0.00018
2025_03_01_16_27_12 Train loss: 5.997666221446707e-07 at step: 78800 lr 0.00018
2025_03_01_16_27_46 Train loss: 4.503620402829256e-06 at step: 79200 lr 0.00018
2025_03_01_16_28_20 Train loss: 1.4901159417490817e-08 at step: 79600 lr 0.00018
2025_03_01_16_28_54 Train loss: 2.175545432692161e-06 at step: 80000 lr 0.00018
2025_03_01_16_29_28 Train loss: 4.0975551200972404e-06 at step: 80400 lr 0.00018
2025_03_01_16_30_02 Train loss: 9.313218640727428e-08 at step: 80800 lr 0.00018
(Val @ epoch 17) acc: 0.999375; ap: 1.0
2025_03_01_16_30_38 Train loss: 4.996398638468236e-05 at step: 81200 lr 0.00018
2025_03_01_16_31_12 Train loss: 1.0496994946151972e-05 at step: 81600 lr 0.00018
2025_03_01_16_31_46 Train loss: 4.6193468961064355e-07 at step: 82000 lr 0.00018
2025_03_01_16_32_20 Train loss: 6.101449798734393e-06 at step: 82400 lr 0.00018
2025_03_01_16_32_54 Train loss: 2.5926944999810075e-06 at step: 82800 lr 0.00018
2025_03_01_16_33_28 Train loss: 1.8978320440510288e-05 at step: 83200 lr 0.00018
2025_03_01_16_34_02 Train loss: 0.00043250637827441096 at step: 83600 lr 0.00018
2025_03_01_16_34_36 Train loss: 3.352759492258883e-08 at step: 84000 lr 0.00018
2025_03_01_16_35_10 Train loss: 0.0005267604719847441 at step: 84400 lr 0.00018
2025_03_01_16_35_49 Train loss: 8.540983799321111e-06 at step: 84800 lr 0.00018
2025_03_01_16_36_23 Train loss: 3.4606159715622198e-06 at step: 85200 lr 0.00018
(Val @ epoch 18) acc: 1.0; ap: 1.0
2025_03_01_16_36_59 Train loss: 4.060310857312288e-06 at step: 85600 lr 0.00018
2025_03_01_16_37_33 Train loss: 6.332989954671575e-08 at step: 86000 lr 0.00018
2025_03_01_16_38_07 Train loss: 1.1175870007207322e-08 at step: 86400 lr 0.00018
2025_03_01_16_38_41 Train loss: 2.3096714585335576e-07 at step: 86800 lr 0.00018
2025_03_01_16_39_15 Train loss: 1.9752804291783832e-05 at step: 87200 lr 0.00018
2025_03_01_16_39_49 Train loss: 0.006696887314319611 at step: 87600 lr 0.00018
2025_03_01_16_40_23 Train loss: 0.0005379460635595024 at step: 88000 lr 0.00018
2025_03_01_16_40_57 Train loss: 2.264929389639292e-05 at step: 88400 lr 0.00018
2025_03_01_16_41_31 Train loss: 0.0007106423145160079 at step: 88800 lr 0.00018
2025_03_01_16_42_05 Train loss: 1.8626446163239052e-08 at step: 89200 lr 0.00018
2025_03_01_16_42_39 Train loss: 4.5558431338577066e-06 at step: 89600 lr 0.00018
2025_03_01_16_43_13 Train loss: 7.338771865761373e-07 at step: 90000 lr 0.00018
(Val @ epoch 19) acc: 1.0; ap: 1.0
2025_03_01_16_43_49 Train loss: 0.005061328411102295 at step: 90400 lr 0.00018
2025_03_01_16_44_23 Train loss: 0.0 at step: 90800 lr 0.00018
2025_03_01_16_44_57 Train loss: 5.2791488997172564e-05 at step: 91200 lr 0.00018
2025_03_01_16_45_31 Train loss: 3.2782452308310894e-07 at step: 91600 lr 0.00018
2025_03_01_16_46_05 Train loss: 1.4900896303515765e-06 at step: 92000 lr 0.00018
2025_03_01_16_46_39 Train loss: 0.00034390337532386184 at step: 92400 lr 0.00018
2025_03_01_16_47_13 Train loss: 1.8216335320175858e-06 at step: 92800 lr 0.00018
2025_03_01_16_47_47 Train loss: 1.1026796755686519e-06 at step: 93200 lr 0.00018
2025_03_01_16_48_21 Train loss: 0.0007887267274782062 at step: 93600 lr 0.00018
2025_03_01_16_48_55 Train loss: 2.618801772769075e-06 at step: 94000 lr 0.00018
2025_03_01_16_49_29 Train loss: 2.65606286120601e-06 at step: 94400 lr 0.00018
2025_03_01_16_49_40 changing lr at the end of epoch 20, iters 94521
*************************
Changing lr from 0.00018 to 0.000162
*************************
(Val @ epoch 20) acc: 1.0; ap: 1.0
2025_03_01_16_50_05 Train loss: 0.0006877729902043939 at step: 94800 lr 0.000162
2025_03_01_16_50_39 Train loss: 2.7194087124371435e-06 at step: 95200 lr 0.000162
2025_03_01_16_51_13 Train loss: 1.1175870007207322e-08 at step: 95600 lr 0.000162
2025_03_01_16_51_47 Train loss: 6.806279270676896e-05 at step: 96000 lr 0.000162
2025_03_01_16_52_21 Train loss: 3.725290076417309e-09 at step: 96400 lr 0.000162
2025_03_01_16_52_55 Train loss: 0.02367974817752838 at step: 96800 lr 0.000162
2025_03_01_16_53_29 Train loss: 0.00018357127555646002 at step: 97200 lr 0.000162
2025_03_01_16_54_03 Train loss: 0.00022064738732296973 at step: 97600 lr 0.000162
2025_03_01_16_54_37 Train loss: 1.4901157641133977e-08 at step: 98000 lr 0.000162
2025_03_01_16_55_11 Train loss: 5.215402154590265e-08 at step: 98400 lr 0.000162
2025_03_01_16_55_45 Train loss: 2.265242437715642e-05 at step: 98800 lr 0.000162
(Val @ epoch 21) acc: 1.0; ap: 1.0
2025_03_01_16_56_21 Train loss: 1.4901157641133977e-08 at step: 99200 lr 0.000162
2025_03_01_16_56_55 Train loss: 0.001606750418432057 at step: 99600 lr 0.000162
2025_03_01_16_57_29 Train loss: 0.00022793773678131402 at step: 100000 lr 0.000162
2025_03_01_16_58_03 Train loss: 5.557643362408271e-06 at step: 100400 lr 0.000162
2025_03_01_16_58_37 Train loss: 2.153145715055871e-06 at step: 100800 lr 0.000162
2025_03_01_16_59_11 Train loss: 5.2906470955349505e-05 at step: 101200 lr 0.000162
2025_03_01_16_59_45 Train loss: 4.5349090214585885e-05 at step: 101600 lr 0.000162
2025_03_01_17_00_19 Train loss: 0.0 at step: 102000 lr 0.000162
2025_03_01_17_00_53 Train loss: 3.496539284242317e-05 at step: 102400 lr 0.000162
2025_03_01_17_01_27 Train loss: 4.384359272080474e-06 at step: 102800 lr 0.000162
2025_03_01_17_02_01 Train loss: 3.725288166833707e-08 at step: 103200 lr 0.000162
(Val @ epoch 22) acc: 1.0; ap: 1.0
2025_03_01_17_02_36 Train loss: 3.725290076417309e-09 at step: 103600 lr 0.000162
2025_03_01_17_03_10 Train loss: 5.512949428521097e-06 at step: 104000 lr 0.000162
2025_03_01_17_03_44 Train loss: 3.7252888773764425e-08 at step: 104400 lr 0.000162
2025_03_01_17_04_18 Train loss: 1.4156083238958672e-07 at step: 104800 lr 0.000162
2025_03_01_17_04_52 Train loss: 7.078047303821222e-08 at step: 105200 lr 0.000162
2025_03_01_17_05_26 Train loss: 0.00010705641034292057 at step: 105600 lr 0.000162
2025_03_01_17_06_00 Train loss: 5.099525878904387e-06 at step: 106000 lr 0.000162
2025_03_01_17_06_34 Train loss: 1.5742940377094783e-05 at step: 106400 lr 0.000162
2025_03_01_17_07_09 Train loss: 0.0 at step: 106800 lr 0.000162
2025_03_01_17_07_43 Train loss: 3.725290076417309e-09 at step: 107200 lr 0.000162
2025_03_01_17_08_17 Train loss: 2.2351738238057806e-08 at step: 107600 lr 0.000162
2025_03_01_17_08_51 Train loss: 2.1680439203919377e-06 at step: 108000 lr 0.000162
(Val @ epoch 23) acc: 1.0; ap: 1.0
2025_03_01_17_09_27 Train loss: 2.425107595627196e-06 at step: 108400 lr 0.000162
2025_03_01_17_10_01 Train loss: 1.1362038776496775e-06 at step: 108800 lr 0.000162
2025_03_01_17_10_35 Train loss: 7.466560782631859e-05 at step: 109200 lr 0.000162
2025_03_01_17_11_09 Train loss: 2.220194573965273e-06 at step: 109600 lr 0.000162
2025_03_01_17_11_43 Train loss: 4.518450623436365e-06 at step: 110000 lr 0.000162
2025_03_01_17_12_17 Train loss: 1.1920907638796052e-07 at step: 110400 lr 0.000162
2025_03_01_17_12_51 Train loss: 3.725290076417309e-09 at step: 110800 lr 0.000162
2025_03_01_17_13_25 Train loss: 1.1175869119028903e-08 at step: 111200 lr 0.000162
2025_03_01_17_13_59 Train loss: 2.2351736461700966e-08 at step: 111600 lr 0.000162
2025_03_01_17_14_33 Train loss: 0.000443868397269398 at step: 112000 lr 0.000162
2025_03_01_17_15_07 Train loss: 2.1952886527287774e-05 at step: 112400 lr 0.000162
(Val @ epoch 24) acc: 1.0; ap: 1.0
2025_03_01_17_15_43 Train loss: 3.558237585821189e-05 at step: 112800 lr 0.000162
2025_03_01_17_16_17 Train loss: 1.4901157641133977e-08 at step: 113200 lr 0.000162
2025_03_01_17_16_51 Train loss: 9.536673246657301e-07 at step: 113600 lr 0.000162
2025_03_01_17_17_25 Train loss: 1.3038501833761984e-07 at step: 114000 lr 0.000162
2025_03_01_17_17_59 Train loss: 2.905719043155841e-07 at step: 114400 lr 0.000162
2025_03_01_17_18_33 Train loss: 0.0029735825955867767 at step: 114800 lr 0.000162
2025_03_01_17_19_07 Train loss: 1.9371466919437808e-07 at step: 115200 lr 0.000162
2025_03_01_17_19_41 Train loss: 3.613520789258473e-07 at step: 115600 lr 0.000162
2025_03_01_17_20_15 Train loss: 6.425588253478054e-06 at step: 116000 lr 0.000162
2025_03_01_17_20_49 Train loss: 1.8777343939291313e-05 at step: 116400 lr 0.000162
2025_03_01_17_21_23 Train loss: 7.297505817405181e-06 at step: 116800 lr 0.000162
(Val @ epoch 25) acc: 0.999375; ap: 1.0
2025_03_01_17_21_59 Train loss: 2.410171873634681e-06 at step: 117200 lr 0.000162
2025_03_01_17_22_33 Train loss: 3.996980922238436e-06 at step: 117600 lr 0.000162
2025_03_01_17_23_07 Train loss: 9.31322148289837e-08 at step: 118000 lr 0.000162
2025_03_01_17_23_41 Train loss: 0.07199861109256744 at step: 118400 lr 0.000162
2025_03_01_17_24_15 Train loss: 1.0058280253133489e-07 at step: 118800 lr 0.000162
2025_03_01_17_24_49 Train loss: 0.0004632652853615582 at step: 119200 lr 0.000162
2025_03_01_17_25_23 Train loss: 2.734264626269578e-06 at step: 119600 lr 0.000162
2025_03_01_17_25_57 Train loss: 6.146673285911675e-07 at step: 120000 lr 0.000162
2025_03_01_17_26_31 Train loss: 7.4505797087454084e-09 at step: 120400 lr 0.000162
2025_03_01_17_27_05 Train loss: 2.3896538550616242e-05 at step: 120800 lr 0.000162
2025_03_01_17_27_39 Train loss: 5.4869597079232335e-06 at step: 121200 lr 0.000162
(Val @ epoch 26) acc: 1.0; ap: 1.0
2025_03_01_17_28_15 Train loss: 1.4901157641133977e-08 at step: 121600 lr 0.000162
2025_03_01_17_28_49 Train loss: 3.7997730828465137e-07 at step: 122000 lr 0.000162
2025_03_01_17_29_23 Train loss: 0.007272007875144482 at step: 122400 lr 0.000162
2025_03_01_17_29_57 Train loss: 3.6512152291834354e-05 at step: 122800 lr 0.000162
2025_03_01_17_30_31 Train loss: 1.218149463966256e-06 at step: 123200 lr 0.000162
2025_03_01_17_31_06 Train loss: 2.1009975625929656e-06 at step: 123600 lr 0.000162
2025_03_01_17_31_40 Train loss: 4.470347647611561e-08 at step: 124000 lr 0.000162
2025_03_01_17_32_14 Train loss: 2.7555042834137566e-05 at step: 124400 lr 0.000162
2025_03_01_17_32_48 Train loss: 0.0006649672286584973 at step: 124800 lr 0.000162
2025_03_01_17_33_22 Train loss: 0.005007821135222912 at step: 125200 lr 0.000162
2025_03_01_17_33_56 Train loss: 4.425488441484049e-06 at step: 125600 lr 0.000162
2025_03_01_17_34_30 Train loss: 1.0319000693925773e-06 at step: 126000 lr 0.000162
(Val @ epoch 27) acc: 1.0; ap: 1.0
2025_03_01_17_35_05 Train loss: 7.4505797087454084e-09 at step: 126400 lr 0.000162
2025_03_01_17_35_40 Train loss: 6.332988533586104e-08 at step: 126800 lr 0.000162
2025_03_01_17_36_14 Train loss: 6.07535775998258e-06 at step: 127200 lr 0.000162
2025_03_01_17_36_48 Train loss: 0.0 at step: 127600 lr 0.000162
2025_03_01_17_37_22 Train loss: 3.773931530304253e-05 at step: 128000 lr 0.000162
2025_03_01_17_37_56 Train loss: 0.00012647070980165154 at step: 128400 lr 0.000162
2025_03_01_17_38_30 Train loss: 8.940691031966708e-08 at step: 128800 lr 0.000162
2025_03_01_17_39_04 Train loss: 1.5124347783057601e-06 at step: 129200 lr 0.000162
2025_03_01_17_39_38 Train loss: 0.0006084828055463731 at step: 129600 lr 0.000162
2025_03_01_17_40_12 Train loss: 3.1292381663661217e-07 at step: 130000 lr 0.000162
2025_03_01_17_40_46 Train loss: 2.302180746482918e-06 at step: 130400 lr 0.000162
(Val @ epoch 28) acc: 1.0; ap: 1.0
2025_03_01_17_41_22 Train loss: 3.7252892326478104e-08 at step: 130800 lr 0.000162
2025_03_01_17_41_56 Train loss: 0.0028810284566134214 at step: 131200 lr 0.000162
2025_03_01_17_42_30 Train loss: 4.246543085173471e-06 at step: 131600 lr 0.000162
2025_03_01_17_43_04 Train loss: 3.352760202801619e-08 at step: 132000 lr 0.000162
2025_03_01_17_43_38 Train loss: 5.15035426360555e-05 at step: 132400 lr 0.000162
2025_03_01_17_44_12 Train loss: 6.33298924412884e-08 at step: 132800 lr 0.000162
2025_03_01_17_44_46 Train loss: 4.5075688603901654e-07 at step: 133200 lr 0.000162
2025_03_01_17_45_20 Train loss: 9.967289770429488e-06 at step: 133600 lr 0.000162
2025_03_01_17_45_54 Train loss: 1.1063916645071004e-06 at step: 134000 lr 0.000162
2025_03_01_17_46_28 Train loss: 3.017473773070378e-07 at step: 134400 lr 0.000162
2025_03_01_17_47_02 Train loss: 5.319422143656993e-06 at step: 134800 lr 0.000162
(Val @ epoch 29) acc: 1.0; ap: 1.0
2025_03_01_17_47_37 Train loss: 1.3038493307249155e-07 at step: 135200 lr 0.000162
2025_03_01_17_48_11 Train loss: 5.960460214282648e-08 at step: 135600 lr 0.000162
2025_03_01_17_48_45 Train loss: 0.0 at step: 136000 lr 0.000162
2025_03_01_17_49_19 Train loss: 3.252062469982775e-06 at step: 136400 lr 0.000162
2025_03_01_17_49_53 Train loss: 1.4901157641133977e-08 at step: 136800 lr 0.000162
2025_03_01_17_50_27 Train loss: 1.1361928500264185e-06 at step: 137200 lr 0.000162
2025_03_01_17_51_01 Train loss: 9.665576362749562e-05 at step: 137600 lr 0.000162
2025_03_01_17_51_35 Train loss: 1.2740263173327548e-06 at step: 138000 lr 0.000162
2025_03_01_17_52_09 Train loss: 5.2154042862184724e-08 at step: 138400 lr 0.000162
2025_03_01_17_52_43 Train loss: 1.1175869119028903e-08 at step: 138800 lr 0.000162
2025_03_01_17_53_17 Train loss: 0.0 at step: 139200 lr 0.000162
2025_03_01_17_53_45 changing lr at the end of epoch 30, iters 139531
*************************
Changing lr from 0.000162 to 0.00014580000000000002
*************************
(Val @ epoch 30) acc: 0.99625; ap: 0.9999968827930175
2025_03_01_17_53_53 Train loss: 2.0781972125405446e-05 at step: 139600 lr 0.00014580000000000002
2025_03_01_17_54_27 Train loss: 1.2293446616240544e-07 at step: 140000 lr 0.00014580000000000002
2025_03_01_17_55_01 Train loss: 3.725290076417309e-09 at step: 140400 lr 0.00014580000000000002
2025_03_01_17_55_35 Train loss: 1.4901157641133977e-08 at step: 140800 lr 0.00014580000000000002
2025_03_01_17_56_09 Train loss: 6.332990665214311e-08 at step: 141200 lr 0.00014580000000000002
2025_03_01_17_56_43 Train loss: 7.513008313253522e-06 at step: 141600 lr 0.00014580000000000002
2025_03_01_17_57_17 Train loss: 1.8253881250984705e-07 at step: 142000 lr 0.00014580000000000002
2025_03_01_17_57_51 Train loss: 0.0 at step: 142400 lr 0.00014580000000000002
2025_03_01_17_58_25 Train loss: 5.3001178457634524e-05 at step: 142800 lr 0.00014580000000000002
2025_03_01_17_58_59 Train loss: 3.881511474901345e-06 at step: 143200 lr 0.00014580000000000002
2025_03_01_17_59_33 Train loss: 5.010597305954434e-05 at step: 143600 lr 0.00014580000000000002
2025_03_01_18_00_07 Train loss: 3.166487658745609e-07 at step: 144000 lr 0.00014580000000000002
(Val @ epoch 31) acc: 1.0; ap: 1.0
2025_03_01_18_00_43 Train loss: 9.933771252690349e-06 at step: 144400 lr 0.00014580000000000002
2025_03_01_18_01_17 Train loss: 0.0009051868109963834 at step: 144800 lr 0.00014580000000000002
2025_03_01_18_01_51 Train loss: 1.1920909059881524e-07 at step: 145200 lr 0.00014580000000000002
2025_03_01_18_02_25 Train loss: 2.8312157951404515e-07 at step: 145600 lr 0.00014580000000000002
2025_03_01_18_02_59 Train loss: 4.610755786416121e-05 at step: 146000 lr 0.00014580000000000002
2025_03_01_18_03_33 Train loss: 0.0 at step: 146400 lr 0.00014580000000000002
2025_03_01_18_04_07 Train loss: 4.411257759784348e-05 at step: 146800 lr 0.00014580000000000002
2025_03_01_18_04_41 Train loss: 0.000507040589582175 at step: 147200 lr 0.00014580000000000002
2025_03_01_18_05_15 Train loss: 1.4901157641133977e-08 at step: 147600 lr 0.00014580000000000002
2025_03_01_18_05_49 Train loss: 1.0803323391428421e-07 at step: 148000 lr 0.00014580000000000002
2025_03_01_18_06_23 Train loss: 3.725290076417309e-09 at step: 148400 lr 0.00014580000000000002
(Val @ epoch 32) acc: 1.0; ap: 1.0
2025_03_01_18_06_59 Train loss: 2.9802308176840597e-08 at step: 148800 lr 0.00014580000000000002
2025_03_01_18_07_33 Train loss: 1.4528610847719392e-07 at step: 149200 lr 0.00014580000000000002
2025_03_01_18_08_07 Train loss: 1.22934338264713e-07 at step: 149600 lr 0.00014580000000000002
2025_03_01_18_08_41 Train loss: 8.605304060438357e-07 at step: 150000 lr 0.00014580000000000002
2025_03_01_18_09_15 Train loss: 9.643288649385795e-06 at step: 150400 lr 0.00014580000000000002
2025_03_01_18_09_49 Train loss: 0.0014500749530270696 at step: 150800 lr 0.00014580000000000002
2025_03_01_18_10_23 Train loss: 3.7997858726157574e-07 at step: 151200 lr 0.00014580000000000002
2025_03_01_18_10_58 Train loss: 7.450580152834618e-09 at step: 151600 lr 0.00014580000000000002
2025_03_01_18_11_32 Train loss: 1.7881342273540213e-07 at step: 152000 lr 0.00014580000000000002
2025_03_01_18_12_06 Train loss: 0.00023299921303987503 at step: 152400 lr 0.00014580000000000002
2025_03_01_18_12_40 Train loss: 3.725290076417309e-09 at step: 152800 lr 0.00014580000000000002
(Val @ epoch 33) acc: 0.999375; ap: 1.0
2025_03_01_18_13_16 Train loss: 0.0 at step: 153200 lr 0.00014580000000000002
2025_03_01_18_13_50 Train loss: 0.0 at step: 153600 lr 0.00014580000000000002
2025_03_01_18_14_24 Train loss: 8.195635814445268e-08 at step: 154000 lr 0.00014580000000000002
2025_03_01_18_14_59 Train loss: 3.725290076417309e-09 at step: 154400 lr 0.00014580000000000002
2025_03_01_18_15_33 Train loss: 0.0002307387621840462 at step: 154800 lr 0.00014580000000000002
2025_03_01_18_16_07 Train loss: 6.705516142346823e-08 at step: 155200 lr 0.00014580000000000002
2025_03_01_18_16_41 Train loss: 7.073525466694264e-06 at step: 155600 lr 0.00014580000000000002
2025_03_01_18_17_15 Train loss: 0.00015972432447597384 at step: 156000 lr 0.00014580000000000002
2025_03_01_18_17_50 Train loss: 1.1920916165308881e-07 at step: 156400 lr 0.00014580000000000002
2025_03_01_18_18_24 Train loss: 1.1175870007207322e-08 at step: 156800 lr 0.00014580000000000002
2025_03_01_18_18_58 Train loss: 7.17824004823342e-05 at step: 157200 lr 0.00014580000000000002
(Val @ epoch 34) acc: 1.0; ap: 1.0
2025_03_01_18_19_34 Train loss: 7.636752457074181e-07 at step: 157600 lr 0.00014580000000000002
2025_03_01_18_20_08 Train loss: 3.725290076417309e-09 at step: 158000 lr 0.00014580000000000002
2025_03_01_18_20_42 Train loss: 1.9371454129668564e-07 at step: 158400 lr 0.00014580000000000002
2025_03_01_18_21_17 Train loss: 1.1175870007207322e-08 at step: 158800 lr 0.00014580000000000002
2025_03_01_18_21_51 Train loss: 3.725288166833707e-08 at step: 159200 lr 0.00014580000000000002
2025_03_01_18_22_25 Train loss: 4.470318515359395e-07 at step: 159600 lr 0.00014580000000000002
2025_03_01_18_22_59 Train loss: 7.450572780953735e-08 at step: 160000 lr 0.00014580000000000002
2025_03_01_18_23_33 Train loss: 3.725290076417309e-09 at step: 160400 lr 0.00014580000000000002
2025_03_01_18_24_08 Train loss: 4.470316810056829e-07 at step: 160800 lr 0.00014580000000000002
2025_03_01_18_24_42 Train loss: 2.719450549193425e-07 at step: 161200 lr 0.00014580000000000002
2025_03_01_18_25_16 Train loss: 3.162341454299167e-05 at step: 161600 lr 0.00014580000000000002
2025_03_01_18_25_50 Train loss: 4.515879845712334e-05 at step: 162000 lr 0.00014580000000000002
(Val @ epoch 35) acc: 1.0; ap: 1.0
2025_03_01_18_26_26 Train loss: 1.1175870007207322e-08 at step: 162400 lr 0.00014580000000000002
2025_03_01_18_27_01 Train loss: 3.2482842016179347e-06 at step: 162800 lr 0.00014580000000000002
2025_03_01_18_27_35 Train loss: 2.7193436835659668e-06 at step: 163200 lr 0.00014580000000000002
2025_03_01_18_28_09 Train loss: 2.980219164783193e-07 at step: 163600 lr 0.00014580000000000002
2025_03_01_18_28_43 Train loss: 6.332987112500632e-08 at step: 164000 lr 0.00014580000000000002
2025_03_01_18_29_17 Train loss: 5.032347689848393e-05 at step: 164400 lr 0.00014580000000000002
2025_03_01_18_29_52 Train loss: 1.6800657931526075e-06 at step: 164800 lr 0.00014580000000000002
2025_03_01_18_30_26 Train loss: 8.838863323035184e-06 at step: 165200 lr 0.00014580000000000002
2025_03_01_18_31_00 Train loss: 3.725290076417309e-09 at step: 165600 lr 0.00014580000000000002
2025_03_01_18_31_34 Train loss: 3.725290076417309e-09 at step: 166000 lr 0.00014580000000000002
2025_03_01_18_32_08 Train loss: 0.0 at step: 166400 lr 0.00014580000000000002
(Val @ epoch 36) acc: 0.999375; ap: 1.0
2025_03_01_18_32_44 Train loss: 1.8626446163239052e-08 at step: 166800 lr 0.00014580000000000002
2025_03_01_18_33_19 Train loss: 1.173444729829498e-06 at step: 167200 lr 0.00014580000000000002
2025_03_01_18_33_53 Train loss: 1.862644793959589e-08 at step: 167600 lr 0.00014580000000000002
2025_03_01_18_34_27 Train loss: 9.904247235681396e-06 at step: 168000 lr 0.00014580000000000002
2025_03_01_18_35_01 Train loss: 3.725290076417309e-09 at step: 168400 lr 0.00014580000000000002
2025_03_01_18_35_36 Train loss: 0.0 at step: 168800 lr 0.00014580000000000002
2025_03_01_18_36_10 Train loss: 1.080333191794125e-07 at step: 169200 lr 0.00014580000000000002
2025_03_01_18_36_44 Train loss: 0.0 at step: 169600 lr 0.00014580000000000002
2025_03_01_18_37_18 Train loss: 1.4156076133531315e-07 at step: 170000 lr 0.00014580000000000002
2025_03_01_18_37_53 Train loss: 3.725290076417309e-09 at step: 170400 lr 0.00014580000000000002
2025_03_01_18_38_27 Train loss: 1.896134108392289e-06 at step: 170800 lr 0.00014580000000000002
(Val @ epoch 37) acc: 1.0; ap: 1.0
2025_03_01_18_39_03 Train loss: 8.046523589655408e-07 at step: 171200 lr 0.00014580000000000002
2025_03_01_18_39_37 Train loss: 1.0132719125977019e-06 at step: 171600 lr 0.00014580000000000002
2025_03_01_18_40_11 Train loss: 7.4505797087454084e-09 at step: 172000 lr 0.00014580000000000002
2025_03_01_18_40_46 Train loss: 0.006883256137371063 at step: 172400 lr 0.00014580000000000002
2025_03_01_18_41_20 Train loss: 0.0 at step: 172800 lr 0.00014580000000000002
2025_03_01_18_41_54 Train loss: 2.0600341485987883e-06 at step: 173200 lr 0.00014580000000000002
2025_03_01_18_42_28 Train loss: 0.0 at step: 173600 lr 0.00014580000000000002
2025_03_01_18_43_03 Train loss: 8.414415788138285e-06 at step: 174000 lr 0.00014580000000000002
2025_03_01_18_43_37 Train loss: 7.07804588273575e-08 at step: 174400 lr 0.00014580000000000002
2025_03_01_18_44_11 Train loss: 3.725290076417309e-09 at step: 174800 lr 0.00014580000000000002
2025_03_01_18_44_45 Train loss: 7.4505797087454084e-09 at step: 175200 lr 0.00014580000000000002
(Val @ epoch 38) acc: 1.0; ap: 1.0
2025_03_01_18_45_21 Train loss: 2.9802308176840597e-08 at step: 175600 lr 0.00014580000000000002
2025_03_01_18_45_56 Train loss: 3.352760202801619e-08 at step: 176000 lr 0.00014580000000000002
2025_03_01_18_46_30 Train loss: 1.4901159417490817e-08 at step: 176400 lr 0.00014580000000000002
2025_03_01_18_47_05 Train loss: 3.032312406503479e-06 at step: 176800 lr 0.00014580000000000002
2025_03_01_18_47_39 Train loss: 3.615026798797771e-05 at step: 177200 lr 0.00014580000000000002
2025_03_01_18_48_13 Train loss: 9.482485620537773e-05 at step: 177600 lr 0.00014580000000000002
2025_03_01_18_48_47 Train loss: 0.0002804925316013396 at step: 178000 lr 0.00014580000000000002
2025_03_01_18_49_22 Train loss: 6.866369221825153e-05 at step: 178400 lr 0.00014580000000000002
2025_03_01_18_49_56 Train loss: 0.0 at step: 178800 lr 0.00014580000000000002
2025_03_01_18_50_30 Train loss: 0.0 at step: 179200 lr 0.00014580000000000002
2025_03_01_18_51_05 Train loss: 4.589260697684949e-06 at step: 179600 lr 0.00014580000000000002
2025_03_01_18_51_39 Train loss: 1.5496823380090063e-06 at step: 180000 lr 0.00014580000000000002
(Val @ epoch 39) acc: 0.999375; ap: 1.0
2025_03_01_18_52_15 Train loss: 0.0 at step: 180400 lr 0.00014580000000000002
2025_03_01_18_52_49 Train loss: 2.5704397899062315e-07 at step: 180800 lr 0.00014580000000000002
2025_03_01_18_53_23 Train loss: 0.0 at step: 181200 lr 0.00014580000000000002
2025_03_01_18_53_58 Train loss: 3.62819446309004e-05 at step: 181600 lr 0.00014580000000000002
2025_03_01_18_54_32 Train loss: 0.0 at step: 182000 lr 0.00014580000000000002
2025_03_01_18_55_06 Train loss: 5.215402509861633e-08 at step: 182400 lr 0.00014580000000000002
2025_03_01_18_55_40 Train loss: 3.725290076417309e-09 at step: 182800 lr 0.00014580000000000002
2025_03_01_18_56_15 Train loss: 7.073527285683667e-06 at step: 183200 lr 0.00014580000000000002
2025_03_01_18_56_49 Train loss: 2.9802315282267955e-08 at step: 183600 lr 0.00014580000000000002
2025_03_01_18_57_23 Train loss: 1.0756792107713409e-05 at step: 184000 lr 0.00014580000000000002
2025_03_01_18_57_58 Train loss: 4.433071296716662e-07 at step: 184400 lr 0.00014580000000000002
2025_03_01_18_58_10 changing lr at the end of epoch 40, iters 184541
*************************
Changing lr from 0.00014580000000000005 to 0.00013122000000000003
*************************
(Val @ epoch 40) acc: 1.0; ap: 1.0
2025_03_01_18_58_34 Train loss: 0.0 at step: 184800 lr 0.00013122000000000003
2025_03_01_18_59_08 Train loss: 0.0 at step: 185200 lr 0.00013122000000000003
2025_03_01_18_59_42 Train loss: 6.146699433884351e-07 at step: 185600 lr 0.00013122000000000003
2025_03_01_19_00_16 Train loss: 4.50744937552372e-06 at step: 186000 lr 0.00013122000000000003
2025_03_01_19_00_51 Train loss: 9.375205081596505e-06 at step: 186400 lr 0.00013122000000000003
2025_03_01_19_01_25 Train loss: 1.5307445210055448e-05 at step: 186800 lr 0.00013122000000000003
2025_03_01_19_01_59 Train loss: 3.725290076417309e-09 at step: 187200 lr 0.00013122000000000003
2025_03_01_19_02_33 Train loss: 3.2409857908533013e-07 at step: 187600 lr 0.00013122000000000003
2025_03_01_19_03_08 Train loss: 7.078048014363958e-08 at step: 188000 lr 0.00013122000000000003
2025_03_01_19_03_42 Train loss: 3.725290076417309e-09 at step: 188400 lr 0.00013122000000000003
2025_03_01_19_04_16 Train loss: 3.725290076417309e-09 at step: 188800 lr 0.00013122000000000003
(Val @ epoch 41) acc: 1.0; ap: 1.0
2025_03_01_19_04_52 Train loss: 3.4196684737253236e-06 at step: 189200 lr 0.00013122000000000003
2025_03_01_19_05_26 Train loss: 1.9110202629235573e-06 at step: 189600 lr 0.00013122000000000003
2025_03_01_19_06_01 Train loss: 4.003984577138908e-05 at step: 190000 lr 0.00013122000000000003
2025_03_01_19_06_35 Train loss: 0.0 at step: 190400 lr 0.00013122000000000003
2025_03_01_19_07_09 Train loss: 0.0 at step: 190800 lr 0.00013122000000000003
2025_03_01_19_07_43 Train loss: 0.0 at step: 191200 lr 0.00013122000000000003
2025_03_01_19_08_17 Train loss: 0.0 at step: 191600 lr 0.00013122000000000003
2025_03_01_19_08_52 Train loss: 3.725289587919178e-08 at step: 192000 lr 0.00013122000000000003
2025_03_01_19_09_26 Train loss: 2.793960618419078e-07 at step: 192400 lr 0.00013122000000000003
2025_03_01_19_10_00 Train loss: 0.00016104955284390599 at step: 192800 lr 0.00013122000000000003
2025_03_01_19_10_34 Train loss: 7.4505797087454084e-09 at step: 193200 lr 0.00013122000000000003
(Val @ epoch 42) acc: 1.0; ap: 1.0
2025_03_01_19_11_10 Train loss: 6.221173407539027e-07 at step: 193600 lr 0.00013122000000000003
2025_03_01_19_11_44 Train loss: 6.817223834332253e-07 at step: 194000 lr 0.00013122000000000003
2025_03_01_19_12_18 Train loss: 6.332987823043368e-08 at step: 194400 lr 0.00013122000000000003
2025_03_01_19_12_52 Train loss: 3.725290076417309e-09 at step: 194800 lr 0.00013122000000000003
2025_03_01_19_13_26 Train loss: 3.725290076417309e-09 at step: 195200 lr 0.00013122000000000003
2025_03_01_19_14_00 Train loss: 3.2037340247370594e-07 at step: 195600 lr 0.00013122000000000003
2025_03_01_19_14_34 Train loss: 2.12707254831912e-06 at step: 196000 lr 0.00013122000000000003
2025_03_01_19_15_08 Train loss: 0.0 at step: 196400 lr 0.00013122000000000003
2025_03_01_19_15_42 Train loss: 0.0 at step: 196800 lr 0.00013122000000000003
2025_03_01_19_16_16 Train loss: 2.581519765953999e-06 at step: 197200 lr 0.00013122000000000003
2025_03_01_19_16_50 Train loss: 2.2812904717284255e-05 at step: 197600 lr 0.00013122000000000003
2025_03_01_19_17_24 Train loss: 1.300100620937883e-06 at step: 198000 lr 0.00013122000000000003
(Val @ epoch 43) acc: 1.0; ap: 1.0
2025_03_01_19_18_00 Train loss: 2.2351734685344127e-08 at step: 198400 lr 0.00013122000000000003
2025_03_01_19_18_34 Train loss: 0.0 at step: 198800 lr 0.00013122000000000003
2025_03_01_19_19_08 Train loss: 2.6829371563508175e-05 at step: 199200 lr 0.00013122000000000003
2025_03_01_19_19_42 Train loss: 0.0 at step: 199600 lr 0.00013122000000000003
2025_03_01_19_20_16 Train loss: 1.7508837402147037e-07 at step: 200000 lr 0.00013122000000000003
2025_03_01_19_20_50 Train loss: 2.235166220998508e-07 at step: 200400 lr 0.00013122000000000003
2025_03_01_19_21_24 Train loss: 0.0012979120947420597 at step: 200800 lr 0.00013122000000000003
2025_03_01_19_21_58 Train loss: 1.266596996174485e-07 at step: 201200 lr 0.00013122000000000003
2025_03_01_19_22_32 Train loss: 1.6018721282762272e-07 at step: 201600 lr 0.00013122000000000003
2025_03_01_19_23_06 Train loss: 3.624497367127333e-06 at step: 202000 lr 0.00013122000000000003
2025_03_01_19_23_40 Train loss: 3.650765165730263e-07 at step: 202400 lr 0.00013122000000000003
(Val @ epoch 44) acc: 1.0; ap: 1.0
2025_03_01_19_24_16 Train loss: 2.9802308176840597e-08 at step: 202800 lr 0.00013122000000000003
2025_03_01_19_24_50 Train loss: 1.1175869119028903e-08 at step: 203200 lr 0.00013122000000000003
2025_03_01_19_25_24 Train loss: 7.4505797087454084e-09 at step: 203600 lr 0.00013122000000000003
2025_03_01_19_25_58 Train loss: 2.570442063642986e-07 at step: 204000 lr 0.00013122000000000003
2025_03_01_19_26_32 Train loss: 7.4505797087454084e-09 at step: 204400 lr 0.00013122000000000003
2025_03_01_19_27_06 Train loss: 5.96040820255439e-07 at step: 204800 lr 0.00013122000000000003
2025_03_01_19_27_40 Train loss: 0.0006697023054584861 at step: 205200 lr 0.00013122000000000003
2025_03_01_19_28_14 Train loss: 0.005953995045274496 at step: 205600 lr 0.00013122000000000003
2025_03_01_19_28_48 Train loss: 0.0 at step: 206000 lr 0.00013122000000000003
2025_03_01_19_29_22 Train loss: 7.4505797087454084e-09 at step: 206400 lr 0.00013122000000000003
2025_03_01_19_29_56 Train loss: 3.725290076417309e-09 at step: 206800 lr 0.00013122000000000003
(Val @ epoch 45) acc: 1.0; ap: 1.0
2025_03_01_19_30_32 Train loss: 6.705516142346823e-08 at step: 207200 lr 0.00013122000000000003
2025_03_01_19_31_06 Train loss: 0.0 at step: 207600 lr 0.00013122000000000003
2025_03_01_19_31_40 Train loss: 0.0 at step: 208000 lr 0.00013122000000000003
2025_03_01_19_32_14 Train loss: 2.473376298439689e-05 at step: 208400 lr 0.00013122000000000003
2025_03_01_19_32_48 Train loss: 3.725290076417309e-09 at step: 208800 lr 0.00013122000000000003
2025_03_01_19_33_22 Train loss: 3.576264191451628e-07 at step: 209200 lr 0.00013122000000000003
2025_03_01_19_33_56 Train loss: 0.0 at step: 209600 lr 0.00013122000000000003
2025_03_01_19_34_30 Train loss: 5.178131914362893e-07 at step: 210000 lr 0.00013122000000000003
2025_03_01_19_35_04 Train loss: 7.189727284639957e-07 at step: 210400 lr 0.00013122000000000003
2025_03_01_19_35_38 Train loss: 1.8626446163239052e-08 at step: 210800 lr 0.00013122000000000003
2025_03_01_19_36_12 Train loss: 3.2782455150481837e-07 at step: 211200 lr 0.00013122000000000003
(Val @ epoch 46) acc: 1.0; ap: 1.0
2025_03_01_19_36_48 Train loss: 0.0 at step: 211600 lr 0.00013122000000000003
2025_03_01_19_37_22 Train loss: 7.4505797087454084e-09 at step: 212000 lr 0.00013122000000000003
2025_03_01_19_37_56 Train loss: 1.4901159417490817e-08 at step: 212400 lr 0.00013122000000000003
2025_03_01_19_38_30 Train loss: 0.00010584943811409175 at step: 212800 lr 0.00013122000000000003
2025_03_01_19_39_04 Train loss: 0.00015078927390277386 at step: 213200 lr 0.00013122000000000003
2025_03_01_19_39_38 Train loss: 1.899892225765143e-07 at step: 213600 lr 0.00013122000000000003
2025_03_01_19_40_12 Train loss: 9.201331749864039e-07 at step: 214000 lr 0.00013122000000000003
2025_03_01_19_40_46 Train loss: 3.725290076417309e-09 at step: 214400 lr 0.00013122000000000003
2025_03_01_19_41_20 Train loss: 7.580410056107212e-06 at step: 214800 lr 0.00013122000000000003
2025_03_01_19_41_54 Train loss: 4.6938339437474497e-07 at step: 215200 lr 0.00013122000000000003
2025_03_01_19_42_28 Train loss: 2.2351734685344127e-08 at step: 215600 lr 0.00013122000000000003
2025_03_01_19_43_02 Train loss: 5.2447699090407696e-06 at step: 216000 lr 0.00013122000000000003
(Val @ epoch 47) acc: 1.0; ap: 1.0
2025_03_01_19_43_38 Train loss: 6.370181608872372e-07 at step: 216400 lr 0.00013122000000000003
2025_03_01_19_44_12 Train loss: 1.043080430918053e-07 at step: 216800 lr 0.00013122000000000003
2025_03_01_19_44_46 Train loss: 6.712259164487477e-06 at step: 217200 lr 0.00013122000000000003
2025_03_01_19_45_20 Train loss: 2.2351734685344127e-08 at step: 217600 lr 0.00013122000000000003
2025_03_01_19_45_54 Train loss: 1.6763760868343525e-07 at step: 218000 lr 0.00013122000000000003
2025_03_01_19_46_28 Train loss: 7.562271093775053e-07 at step: 218400 lr 0.00013122000000000003
2025_03_01_19_47_02 Train loss: 7.1966101131692994e-06 at step: 218800 lr 0.00013122000000000003
2025_03_01_19_47_36 Train loss: 1.1175870007207322e-08 at step: 219200 lr 0.00013122000000000003
2025_03_01_19_48_10 Train loss: 7.4505797087454084e-09 at step: 219600 lr 0.00013122000000000003
2025_03_01_19_48_44 Train loss: 0.0 at step: 220000 lr 0.00013122000000000003
2025_03_01_19_49_18 Train loss: 0.0 at step: 220400 lr 0.00013122000000000003
(Val @ epoch 48) acc: 1.0; ap: 1.0
2025_03_01_19_49_53 Train loss: 0.0005520356935448945 at step: 220800 lr 0.00013122000000000003
2025_03_01_19_50_27 Train loss: 0.0 at step: 221200 lr 0.00013122000000000003
2025_03_01_19_51_01 Train loss: 3.352760558072987e-08 at step: 221600 lr 0.00013122000000000003
2025_03_01_19_51_35 Train loss: 0.0 at step: 222000 lr 0.00013122000000000003
2025_03_01_19_52_09 Train loss: 1.1175869119028903e-08 at step: 222400 lr 0.00013122000000000003
2025_03_01_19_52_43 Train loss: 0.0002912489580921829 at step: 222800 lr 0.00013122000000000003
2025_03_01_19_53_17 Train loss: 8.94068392653935e-08 at step: 223200 lr 0.00013122000000000003
2025_03_01_19_53_51 Train loss: 1.1622691999946255e-06 at step: 223600 lr 0.00013122000000000003
2025_03_01_19_54_25 Train loss: 3.6507748291114694e-07 at step: 224000 lr 0.00013122000000000003
2025_03_01_19_54_59 Train loss: 9.040046279551461e-06 at step: 224400 lr 0.00013122000000000003
2025_03_01_19_55_33 Train loss: 3.725290076417309e-09 at step: 224800 lr 0.00013122000000000003
(Val @ epoch 49) acc: 1.0; ap: 1.0
*************************
2025_03_01_19_55_56
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 94.8; ap: 99.2
(2 stylegan2 ) acc: 97.4; ap: 99.9
(3 biggan    ) acc: 74.0; ap: 86.5
(4 cyclegan  ) acc: 63.0; ap: 92.6
(5 stargan   ) acc: 76.0; ap: 88.7
(6 gaugan    ) acc: 68.0; ap: 82.6
(7 deepfake  ) acc: 63.1; ap: 64.6
(8 Mean      ) acc: 79.5; ap: 89.3
*************************
2025_03_01_19_57_01
Saving model ./checkpoints/4class-resnet-car-cat-chair-horse2025_03_01_14_34_55/model_epoch_last.pth
