train.py  --name  4class-resnet-car-cat-chair-horse  --dataroot  ./datasets/ForenSynths_train_val  --classes  car,cat,chair,horse  --batch_size  32  --delr_freq  10  --lr  0.0002  --niter  50
cwd: /home/chenyuheng/Code/NPR-DeepfakeDetection
2024_12_30_15_56_02 Train loss: 0.19138115644454956 at step: 400 lr 0.0002
2024_12_30_15_56_37 Train loss: 0.11357692629098892 at step: 800 lr 0.0002
2024_12_30_15_57_13 Train loss: 0.008900239132344723 at step: 1200 lr 0.0002
2024_12_30_15_57_49 Train loss: 0.04684516787528992 at step: 1600 lr 0.0002
2024_12_30_15_58_24 Train loss: 0.004266864620149136 at step: 2000 lr 0.0002
2024_12_30_15_59_00 Train loss: 0.005767344031482935 at step: 2400 lr 0.0002
2024_12_30_15_59_36 Train loss: 0.0027188588865101337 at step: 2800 lr 0.0002
2024_12_30_16_00_12 Train loss: 0.004908679984509945 at step: 3200 lr 0.0002
2024_12_30_16_00_47 Train loss: 0.011007518507540226 at step: 3600 lr 0.0002
2024_12_30_16_01_23 Train loss: 0.0033917035907506943 at step: 4000 lr 0.0002
2024_12_30_16_01_59 Train loss: 0.0010076970793306828 at step: 4400 lr 0.0002
(Val @ epoch 0) acc: 0.97875; ap: 0.9994274955079901
2024_12_30_16_02_37 Train loss: 0.0009692429448477924 at step: 4800 lr 0.0002
2024_12_30_16_03_13 Train loss: 0.005369824822992086 at step: 5200 lr 0.0002
2024_12_30_16_03_49 Train loss: 0.0007177416700869799 at step: 5600 lr 0.0002
2024_12_30_16_04_25 Train loss: 0.002577464561909437 at step: 6000 lr 0.0002
2024_12_30_16_05_01 Train loss: 0.0003711203462444246 at step: 6400 lr 0.0002
2024_12_30_16_05_37 Train loss: 0.0013691228814423084 at step: 6800 lr 0.0002
2024_12_30_16_06_13 Train loss: 0.005151798948645592 at step: 7200 lr 0.0002
2024_12_30_16_06_49 Train loss: 0.03636341169476509 at step: 7600 lr 0.0002
2024_12_30_16_07_25 Train loss: 0.006899116560816765 at step: 8000 lr 0.0002
2024_12_30_16_08_01 Train loss: 0.016991695389151573 at step: 8400 lr 0.0002
2024_12_30_16_08_37 Train loss: 0.009554850868880749 at step: 8800 lr 0.0002
(Val @ epoch 1) acc: 0.994375; ap: 0.9999859393090279
2024_12_30_16_09_15 Train loss: 0.00014774789451621473 at step: 9200 lr 0.0002
2024_12_30_16_09_51 Train loss: 0.0003514306736178696 at step: 9600 lr 0.0002
2024_12_30_16_10_27 Train loss: 0.013979491777718067 at step: 10000 lr 0.0002
2024_12_30_16_11_03 Train loss: 0.002808802295476198 at step: 10400 lr 0.0002
2024_12_30_16_11_39 Train loss: 0.000743051350582391 at step: 10800 lr 0.0002
2024_12_30_16_12_15 Train loss: 0.0043254150077700615 at step: 11200 lr 0.0002
2024_12_30_16_12_51 Train loss: 0.00012757012154906988 at step: 11600 lr 0.0002
2024_12_30_16_13_27 Train loss: 0.0014445597771555185 at step: 12000 lr 0.0002
2024_12_30_16_14_03 Train loss: 0.0018153081182390451 at step: 12400 lr 0.0002
2024_12_30_16_14_39 Train loss: 0.0003425259201321751 at step: 12800 lr 0.0002
2024_12_30_16_15_15 Train loss: 0.006903343368321657 at step: 13200 lr 0.0002
(Val @ epoch 2) acc: 0.99875; ap: 1.0
2024_12_30_16_15_53 Train loss: 5.65903537790291e-05 at step: 13600 lr 0.0002
2024_12_30_16_16_29 Train loss: 0.00018456586985848844 at step: 14000 lr 0.0002
2024_12_30_16_17_05 Train loss: 0.0014236840652301908 at step: 14400 lr 0.0002
2024_12_30_16_17_41 Train loss: 0.010196147486567497 at step: 14800 lr 0.0002
2024_12_30_16_18_17 Train loss: 0.00045234945719130337 at step: 15200 lr 0.0002
2024_12_30_16_18_53 Train loss: 0.00036867393646389246 at step: 15600 lr 0.0002
2024_12_30_16_19_29 Train loss: 6.21840008534491e-05 at step: 16000 lr 0.0002
2024_12_30_16_20_05 Train loss: 0.0008032589685171843 at step: 16400 lr 0.0002
2024_12_30_16_20_41 Train loss: 0.00456862011924386 at step: 16800 lr 0.0002
2024_12_30_16_21_17 Train loss: 0.018030300736427307 at step: 17200 lr 0.0002
2024_12_30_16_21_53 Train loss: 0.0002703713544178754 at step: 17600 lr 0.0002
2024_12_30_16_22_29 Train loss: 0.013798788189888 at step: 18000 lr 0.0002
(Val @ epoch 3) acc: 0.999375; ap: 1.0
2024_12_30_16_23_07 Train loss: 0.008933988399803638 at step: 18400 lr 0.0002
2024_12_30_16_23_43 Train loss: 1.3539962537834072e-06 at step: 18800 lr 0.0002
2024_12_30_16_24_19 Train loss: 0.00015041252481751144 at step: 19200 lr 0.0002
2024_12_30_16_24_55 Train loss: 0.0004094062896911055 at step: 19600 lr 0.0002
2024_12_30_16_25_31 Train loss: 0.017959564924240112 at step: 20000 lr 0.0002
2024_12_30_16_26_07 Train loss: 0.0002467119484208524 at step: 20400 lr 0.0002
2024_12_30_16_26_43 Train loss: 1.4041253962204792e-05 at step: 20800 lr 0.0002
2024_12_30_16_27_19 Train loss: 0.011103147640824318 at step: 21200 lr 0.0002
2024_12_30_16_27_55 Train loss: 0.002430262975394726 at step: 21600 lr 0.0002
2024_12_30_16_28_31 Train loss: 0.0029364819638431072 at step: 22000 lr 0.0002
2024_12_30_16_29_08 Train loss: 0.0006947795627638698 at step: 22400 lr 0.0002
(Val @ epoch 4) acc: 0.998125; ap: 0.9999937460790771
2024_12_30_16_29_45 Train loss: 0.0003768651804421097 at step: 22800 lr 0.0002
2024_12_30_16_30_22 Train loss: 6.561189366038889e-06 at step: 23200 lr 0.0002
2024_12_30_16_30_58 Train loss: 0.0014008245198056102 at step: 23600 lr 0.0002
2024_12_30_16_31_34 Train loss: 4.434781294548884e-05 at step: 24000 lr 0.0002
2024_12_30_16_32_10 Train loss: 9.185877570416778e-05 at step: 24400 lr 0.0002
2024_12_30_16_32_46 Train loss: 6.1543978517875075e-06 at step: 24800 lr 0.0002
2024_12_30_16_33_22 Train loss: 3.496976205497049e-06 at step: 25200 lr 0.0002
2024_12_30_16_33_58 Train loss: 0.0001945177064044401 at step: 25600 lr 0.0002
2024_12_30_16_34_34 Train loss: 0.0014282457996159792 at step: 26000 lr 0.0002
2024_12_30_16_35_10 Train loss: 0.004396062809973955 at step: 26400 lr 0.0002
2024_12_30_16_35_46 Train loss: 0.0004388539819046855 at step: 26800 lr 0.0002
(Val @ epoch 5) acc: 1.0; ap: 1.0
2024_12_30_16_36_24 Train loss: 0.021731823682785034 at step: 27200 lr 0.0002
2024_12_30_16_37_00 Train loss: 0.0033154061529785395 at step: 27600 lr 0.0002
2024_12_30_16_37_37 Train loss: 0.00032209212076850235 at step: 28000 lr 0.0002
2024_12_30_16_38_13 Train loss: 9.437062544748187e-05 at step: 28400 lr 0.0002
2024_12_30_16_38_49 Train loss: 0.0008947310270741582 at step: 28800 lr 0.0002
2024_12_30_16_39_25 Train loss: 1.3208098607719876e-05 at step: 29200 lr 0.0002
2024_12_30_16_40_01 Train loss: 0.00046597764594480395 at step: 29600 lr 0.0002
2024_12_30_16_40_37 Train loss: 7.792859832989052e-05 at step: 30000 lr 0.0002
2024_12_30_16_41_13 Train loss: 7.985494448803365e-05 at step: 30400 lr 0.0002
2024_12_30_16_41_49 Train loss: 0.023429611697793007 at step: 30800 lr 0.0002
2024_12_30_16_42_25 Train loss: 0.0005903312121517956 at step: 31200 lr 0.0002
(Val @ epoch 6) acc: 0.999375; ap: 0.9999968827930175
2024_12_30_16_43_03 Train loss: 6.659942300757393e-06 at step: 31600 lr 0.0002
2024_12_30_16_43_39 Train loss: 0.00011779431952163577 at step: 32000 lr 0.0002
2024_12_30_16_44_15 Train loss: 0.0005111545906402171 at step: 32400 lr 0.0002
2024_12_30_16_44_51 Train loss: 7.045931124594063e-05 at step: 32800 lr 0.0002
2024_12_30_16_45_27 Train loss: 0.017558522522449493 at step: 33200 lr 0.0002
2024_12_30_16_46_04 Train loss: 1.078035438695224e-05 at step: 33600 lr 0.0002
2024_12_30_16_46_40 Train loss: 0.0002945004671346396 at step: 34000 lr 0.0002
2024_12_30_16_47_16 Train loss: 4.780706603924045e-06 at step: 34400 lr 0.0002
2024_12_30_16_47_52 Train loss: 0.0007037508767098188 at step: 34800 lr 0.0002
2024_12_30_16_48_28 Train loss: 5.9485908423084766e-05 at step: 35200 lr 0.0002
2024_12_30_16_49_04 Train loss: 0.00010225367441307753 at step: 35600 lr 0.0002
2024_12_30_16_49_40 Train loss: 1.565223283250816e-05 at step: 36000 lr 0.0002
(Val @ epoch 7) acc: 0.999375; ap: 1.0
2024_12_30_16_50_18 Train loss: 0.0007799548329785466 at step: 36400 lr 0.0002
2024_12_30_16_50_54 Train loss: 5.864966442459263e-05 at step: 36800 lr 0.0002
2024_12_30_16_51_30 Train loss: 8.211348358599935e-06 at step: 37200 lr 0.0002
2024_12_30_16_52_06 Train loss: 9.253483767679427e-06 at step: 37600 lr 0.0002
2024_12_30_16_52_42 Train loss: 2.4822404611768434e-06 at step: 38000 lr 0.0002
2024_12_30_16_53_18 Train loss: 5.0533722969703376e-05 at step: 38400 lr 0.0002
2024_12_30_16_53_54 Train loss: 3.317308801342733e-05 at step: 38800 lr 0.0002
2024_12_30_16_54_30 Train loss: 0.00018078027642332017 at step: 39200 lr 0.0002
2024_12_30_16_55_06 Train loss: 0.020659487694501877 at step: 39600 lr 0.0002
2024_12_30_16_55_42 Train loss: 1.808242086553946e-05 at step: 40000 lr 0.0002
2024_12_30_16_56_19 Train loss: 0.02241402491927147 at step: 40400 lr 0.0002
(Val @ epoch 8) acc: 0.9975; ap: 0.9999937675124533
2024_12_30_16_56_56 Train loss: 0.0034757154062390327 at step: 40800 lr 0.0002
2024_12_30_16_57_33 Train loss: 7.499684215872549e-07 at step: 41200 lr 0.0002
2024_12_30_16_58_09 Train loss: 0.0012005107710137963 at step: 41600 lr 0.0002
2024_12_30_16_58_45 Train loss: 2.6057685317937285e-05 at step: 42000 lr 0.0002
2024_12_30_16_59_21 Train loss: 0.00028426392236724496 at step: 42400 lr 0.0002
2024_12_30_16_59_57 Train loss: 0.00044469552813097835 at step: 42800 lr 0.0002
2024_12_30_17_00_33 Train loss: 0.09003396332263947 at step: 43200 lr 0.0002
2024_12_30_17_01_09 Train loss: 0.0009835062082856894 at step: 43600 lr 0.0002
2024_12_30_17_01_45 Train loss: 2.1123074475326575e-05 at step: 44000 lr 0.0002
2024_12_30_17_02_21 Train loss: 4.715633622254245e-06 at step: 44400 lr 0.0002
2024_12_30_17_02_57 Train loss: 3.90474742744118e-05 at step: 44800 lr 0.0002
(Val @ epoch 9) acc: 1.0; ap: 1.0
2024_12_30_17_03_35 Train loss: 0.000771275837905705 at step: 45200 lr 0.0002
2024_12_30_17_04_11 Train loss: 1.935642558237305e-06 at step: 45600 lr 0.0002
2024_12_30_17_04_47 Train loss: 0.0001689133932814002 at step: 46000 lr 0.0002
2024_12_30_17_05_23 Train loss: 0.00029686311609111726 at step: 46400 lr 0.0002
2024_12_30_17_05_59 Train loss: 0.0008144904859364033 at step: 46800 lr 0.0002
2024_12_30_17_06_35 Train loss: 2.1798614397994243e-05 at step: 47200 lr 0.0002
2024_12_30_17_07_11 Train loss: 4.666715540224686e-05 at step: 47600 lr 0.0002
2024_12_30_17_07_47 Train loss: 0.00022837916912976652 at step: 48000 lr 0.0002
2024_12_30_17_08_23 Train loss: 4.598092800733866e-06 at step: 48400 lr 0.0002
2024_12_30_17_08_59 Train loss: 7.427102445944911e-06 at step: 48800 lr 0.0002
2024_12_30_17_09_35 Train loss: 0.003943015821278095 at step: 49200 lr 0.0002
2024_12_30_17_10_03 changing lr at the end of epoch 10, iters 49511
*************************
Changing lr from 0.0002 to 0.00018
*************************
(Val @ epoch 10) acc: 1.0; ap: 1.0
2024_12_30_17_10_13 Train loss: 2.0665199826908065e-06 at step: 49600 lr 0.00018
2024_12_30_17_10_49 Train loss: 0.009371229447424412 at step: 50000 lr 0.00018
2024_12_30_17_11_25 Train loss: 7.195992657216266e-05 at step: 50400 lr 0.00018
2024_12_30_17_12_01 Train loss: 4.892559445579536e-06 at step: 50800 lr 0.00018
2024_12_30_17_12_37 Train loss: 3.150737757096067e-05 at step: 51200 lr 0.00018
2024_12_30_17_13_13 Train loss: 0.00021490332437679172 at step: 51600 lr 0.00018
2024_12_30_17_13_49 Train loss: 0.04840594902634621 at step: 52000 lr 0.00018
2024_12_30_17_14_25 Train loss: 1.4930205338714586e-07 at step: 52400 lr 0.00018
2024_12_30_17_15_02 Train loss: 1.3085739738016855e-05 at step: 52800 lr 0.00018
2024_12_30_17_15_38 Train loss: 1.3971666703582741e-06 at step: 53200 lr 0.00018
2024_12_30_17_16_14 Train loss: 4.597545193973929e-05 at step: 53600 lr 0.00018
2024_12_30_17_16_50 Train loss: 1.0962652368107229e-06 at step: 54000 lr 0.00018
(Val @ epoch 11) acc: 1.0; ap: 1.0
2024_12_30_17_17_28 Train loss: 7.224739874800434e-06 at step: 54400 lr 0.00018
2024_12_30_17_18_04 Train loss: 8.459485252387822e-05 at step: 54800 lr 0.00018
2024_12_30_17_18_40 Train loss: 1.6996149270198657e-06 at step: 55200 lr 0.00018
2024_12_30_17_19_16 Train loss: 9.832701471168548e-05 at step: 55600 lr 0.00018
2024_12_30_17_19_52 Train loss: 0.00019974559836555272 at step: 56000 lr 0.00018
2024_12_30_17_20_28 Train loss: 3.7552326830336824e-06 at step: 56400 lr 0.00018
2024_12_30_17_21_04 Train loss: 0.05850234627723694 at step: 56800 lr 0.00018
2024_12_30_17_21_40 Train loss: 9.30989529024373e-07 at step: 57200 lr 0.00018
2024_12_30_17_22_16 Train loss: 0.005670133512467146 at step: 57600 lr 0.00018
2024_12_30_17_22_52 Train loss: 1.346361113974126e-05 at step: 58000 lr 0.00018
2024_12_30_17_23_28 Train loss: 0.0001497624471085146 at step: 58400 lr 0.00018
(Val @ epoch 12) acc: 0.999375; ap: 1.0
2024_12_30_17_24_06 Train loss: 3.716238097695168e-06 at step: 58800 lr 0.00018
2024_12_30_17_24_42 Train loss: 2.8922298952238634e-05 at step: 59200 lr 0.00018
2024_12_30_17_25_18 Train loss: 3.0413608328672126e-05 at step: 59600 lr 0.00018
2024_12_30_17_25_54 Train loss: 1.010503910947591e-05 at step: 60000 lr 0.00018
2024_12_30_17_26_30 Train loss: 4.479324218209513e-07 at step: 60400 lr 0.00018
2024_12_30_17_27_06 Train loss: 0.0001535438932478428 at step: 60800 lr 0.00018
2024_12_30_17_27_42 Train loss: 0.00012862890434917063 at step: 61200 lr 0.00018
2024_12_30_17_28_18 Train loss: 0.0006983131170272827 at step: 61600 lr 0.00018
2024_12_30_17_28_55 Train loss: 0.00010998172365361825 at step: 62000 lr 0.00018
2024_12_30_17_29_31 Train loss: 7.085917786753271e-06 at step: 62400 lr 0.00018
2024_12_30_17_30_07 Train loss: 1.4535278751282021e-05 at step: 62800 lr 0.00018
(Val @ epoch 13) acc: 1.0; ap: 1.0
2024_12_30_17_30_45 Train loss: 3.1934643629938364e-05 at step: 63200 lr 0.00018
2024_12_30_17_31_21 Train loss: 0.18982142210006714 at step: 63600 lr 0.00018
2024_12_30_17_31_57 Train loss: 9.906298510031775e-05 at step: 64000 lr 0.00018
2024_12_30_17_32_33 Train loss: 4.963111859979108e-05 at step: 64400 lr 0.00018
2024_12_30_17_33_09 Train loss: 2.4256696633528918e-05 at step: 64800 lr 0.00018
2024_12_30_17_33_45 Train loss: 2.542358515711385e-06 at step: 65200 lr 0.00018
2024_12_30_17_34_21 Train loss: 3.414260208955966e-05 at step: 65600 lr 0.00018
2024_12_30_17_34_57 Train loss: 1.6272002540063113e-05 at step: 66000 lr 0.00018
2024_12_30_17_35_33 Train loss: 4.884795271209441e-05 at step: 66400 lr 0.00018
2024_12_30_17_36_09 Train loss: 9.301896852775826e-08 at step: 66800 lr 0.00018
2024_12_30_17_36_45 Train loss: 7.092969099176116e-06 at step: 67200 lr 0.00018
(Val @ epoch 14) acc: 1.0; ap: 1.0
2024_12_30_17_37_23 Train loss: 0.002964956220239401 at step: 67600 lr 0.00018
2024_12_30_17_37_59 Train loss: 2.0948261862940853e-06 at step: 68000 lr 0.00018
2024_12_30_17_38_35 Train loss: 0.0001069605496013537 at step: 68400 lr 0.00018
2024_12_30_17_39_11 Train loss: 9.693397441878915e-05 at step: 68800 lr 0.00018
2024_12_30_17_39_48 Train loss: 4.841390932597278e-07 at step: 69200 lr 0.00018
2024_12_30_17_40_24 Train loss: 1.8665596712708066e-07 at step: 69600 lr 0.00018
2024_12_30_17_41_00 Train loss: 2.982751468039169e-08 at step: 70000 lr 0.00018
2024_12_30_17_41_36 Train loss: 2.5960019911508425e-07 at step: 70400 lr 0.00018
2024_12_30_17_42_12 Train loss: 8.081615874289128e-07 at step: 70800 lr 0.00018
2024_12_30_17_42_48 Train loss: 5.784394306829199e-05 at step: 71200 lr 0.00018
2024_12_30_17_43_24 Train loss: 2.980268831720423e-08 at step: 71600 lr 0.00018
2024_12_30_17_44_00 Train loss: 0.0009951881365850568 at step: 72000 lr 0.00018
(Val @ epoch 15) acc: 0.99875; ap: 1.0
2024_12_30_17_44_38 Train loss: 0.08583981543779373 at step: 72400 lr 0.00018
2024_12_30_17_45_14 Train loss: 7.355974048550706e-06 at step: 72800 lr 0.00018
2024_12_30_17_45_50 Train loss: 1.2441419130482245e-05 at step: 73200 lr 0.00018
2024_12_30_17_46_26 Train loss: 0.00018034006643574685 at step: 73600 lr 0.00018
2024_12_30_17_47_02 Train loss: 0.002310861134901643 at step: 74000 lr 0.00018
2024_12_30_17_47_38 Train loss: 7.733967208878312e-07 at step: 74400 lr 0.00018
2024_12_30_17_48_15 Train loss: 5.12270389663172e-07 at step: 74800 lr 0.00018
2024_12_30_17_48_51 Train loss: 1.703808493402903e-06 at step: 75200 lr 0.00018
2024_12_30_17_49_27 Train loss: 4.369500675238669e-05 at step: 75600 lr 0.00018
2024_12_30_17_50_03 Train loss: 0.00643120426684618 at step: 76000 lr 0.00018
2024_12_30_17_50_39 Train loss: 3.1566088409817894e-07 at step: 76400 lr 0.00018
(Val @ epoch 16) acc: 1.0; ap: 1.0
2024_12_30_17_51_17 Train loss: 1.958733264473267e-06 at step: 76800 lr 0.00018
2024_12_30_17_51_53 Train loss: 0.01820952072739601 at step: 77200 lr 0.00018
2024_12_30_17_52_29 Train loss: 7.900670425442513e-06 at step: 77600 lr 0.00018
2024_12_30_17_53_05 Train loss: 1.3411254258244298e-06 at step: 78000 lr 0.00018
2024_12_30_17_53_41 Train loss: 5.0139842642238364e-05 at step: 78400 lr 0.00018
2024_12_30_17_54_17 Train loss: 4.6630688302684575e-05 at step: 78800 lr 0.00018
2024_12_30_17_54_53 Train loss: 2.6476353014004417e-05 at step: 79200 lr 0.00018
2024_12_30_17_55_30 Train loss: 5.594765389105305e-05 at step: 79600 lr 0.00018
2024_12_30_17_56_06 Train loss: 1.243337578671344e-06 at step: 80000 lr 0.00018
2024_12_30_17_56_42 Train loss: 7.988835022842977e-06 at step: 80400 lr 0.00018
2024_12_30_17_57_18 Train loss: 1.204608395255491e-07 at step: 80800 lr 0.00018
(Val @ epoch 17) acc: 0.999375; ap: 1.0
2024_12_30_17_57_56 Train loss: 6.91419927534298e-06 at step: 81200 lr 0.00018
2024_12_30_17_58_32 Train loss: 4.686086867877748e-06 at step: 81600 lr 0.00018
2024_12_30_17_59_08 Train loss: 1.7657943317317404e-05 at step: 82000 lr 0.00018
2024_12_30_17_59_44 Train loss: 2.3260741727426648e-05 at step: 82400 lr 0.00018
2024_12_30_18_00_20 Train loss: 5.398761004471453e-06 at step: 82800 lr 0.00018
2024_12_30_18_00_56 Train loss: 2.2228989564609947e-06 at step: 83200 lr 0.00018
2024_12_30_18_01_32 Train loss: 2.3548005628981628e-05 at step: 83600 lr 0.00018
2024_12_30_18_02_08 Train loss: 8.522649750375422e-07 at step: 84000 lr 0.00018
2024_12_30_18_02_45 Train loss: 7.39871302357642e-06 at step: 84400 lr 0.00018
2024_12_30_18_03_21 Train loss: 0.0029787139501422644 at step: 84800 lr 0.00018
2024_12_30_18_03_57 Train loss: 1.1563302905415185e-05 at step: 85200 lr 0.00018
(Val @ epoch 18) acc: 1.0; ap: 1.0
2024_12_30_18_04_35 Train loss: 7.749028974046723e-09 at step: 85600 lr 0.00018
2024_12_30_18_05_11 Train loss: 1.3135777408024296e-05 at step: 86000 lr 0.00018
2024_12_30_18_05_47 Train loss: 2.9362012355704792e-05 at step: 86400 lr 0.00018
2024_12_30_18_06_23 Train loss: 2.1338166789064417e-07 at step: 86800 lr 0.00018
2024_12_30_18_06_59 Train loss: 2.5455297873122618e-05 at step: 87200 lr 0.00018
2024_12_30_18_07_35 Train loss: 5.097394023323432e-06 at step: 87600 lr 0.00018
2024_12_30_18_08_11 Train loss: 1.1801108712461428e-06 at step: 88000 lr 0.00018
2024_12_30_18_08_47 Train loss: 1.766323111951351e-05 at step: 88400 lr 0.00018
2024_12_30_18_09_23 Train loss: 1.0134614967682865e-05 at step: 88800 lr 0.00018
2024_12_30_18_09_59 Train loss: 1.2842321666539647e-05 at step: 89200 lr 0.00018
2024_12_30_18_10_35 Train loss: 2.4146567739080638e-05 at step: 89600 lr 0.00018
2024_12_30_18_11_11 Train loss: 5.8346300647826865e-05 at step: 90000 lr 0.00018
(Val @ epoch 19) acc: 0.999375; ap: 0.9999968827930175
2024_12_30_18_11_49 Train loss: 0.0038405200466513634 at step: 90400 lr 0.00018
2024_12_30_18_12_25 Train loss: 9.478745050728321e-05 at step: 90800 lr 0.00018
2024_12_30_18_13_01 Train loss: 0.00018120901950169355 at step: 91200 lr 0.00018
2024_12_30_18_13_38 Train loss: 0.00017704637139104307 at step: 91600 lr 0.00018
2024_12_30_18_14_14 Train loss: 5.046606020187028e-06 at step: 92000 lr 0.00018
2024_12_30_18_14_50 Train loss: 0.0016698548570275307 at step: 92400 lr 0.00018
2024_12_30_18_15_26 Train loss: 2.0792458599316888e-05 at step: 92800 lr 0.00018
2024_12_30_18_16_02 Train loss: 9.541479084873572e-05 at step: 93200 lr 0.00018
2024_12_30_18_16_38 Train loss: 1.1983281922312017e-07 at step: 93600 lr 0.00018
2024_12_30_18_17_14 Train loss: 0.0003617035399656743 at step: 94000 lr 0.00018
2024_12_30_18_17_50 Train loss: 7.879616532591172e-06 at step: 94400 lr 0.00018
2024_12_30_18_18_01 changing lr at the end of epoch 20, iters 94521
*************************
Changing lr from 0.00018 to 0.000162
*************************
(Val @ epoch 20) acc: 1.0; ap: 1.0
2024_12_30_18_18_28 Train loss: 3.57658393568272e-07 at step: 94800 lr 0.000162
2024_12_30_18_19_04 Train loss: 5.195121502765687e-06 at step: 95200 lr 0.000162
2024_12_30_18_19_40 Train loss: 4.103038463654229e-06 at step: 95600 lr 0.000162
2024_12_30_18_20_16 Train loss: 4.928109319735086e-06 at step: 96000 lr 0.000162
2024_12_30_18_20_52 Train loss: 0.0004913745797239244 at step: 96400 lr 0.000162
2024_12_30_18_21_28 Train loss: 9.338911866052513e-10 at step: 96800 lr 0.000162
2024_12_30_18_22_04 Train loss: 4.462043216335587e-05 at step: 97200 lr 0.000162
2024_12_30_18_22_41 Train loss: 0.3187897801399231 at step: 97600 lr 0.000162
2024_12_30_18_23_17 Train loss: 1.3099564057483803e-05 at step: 98000 lr 0.000162
2024_12_30_18_23_53 Train loss: 2.0109052911720937e-07 at step: 98400 lr 0.000162
2024_12_30_18_24_29 Train loss: 9.438081178814173e-06 at step: 98800 lr 0.000162
(Val @ epoch 21) acc: 0.999375; ap: 1.0
2024_12_30_18_25_07 Train loss: 4.948788046021946e-07 at step: 99200 lr 0.000162
2024_12_30_18_25_43 Train loss: 4.882418913609854e-10 at step: 99600 lr 0.000162
2024_12_30_18_26_19 Train loss: 1.5819515697401698e-09 at step: 100000 lr 0.000162
2024_12_30_18_26_55 Train loss: 0.3224447965621948 at step: 100400 lr 0.000162
2024_12_30_18_27_31 Train loss: 2.9445234758895822e-05 at step: 100800 lr 0.000162
2024_12_30_18_28_07 Train loss: 4.0100948694998806e-07 at step: 101200 lr 0.000162
2024_12_30_18_28_43 Train loss: 1.2397813407005742e-05 at step: 101600 lr 0.000162
2024_12_30_18_29_19 Train loss: 8.8333308667643e-07 at step: 102000 lr 0.000162
2024_12_30_18_29_55 Train loss: 1.7346073022395103e-08 at step: 102400 lr 0.000162
2024_12_30_18_30_32 Train loss: 5.961150577604712e-07 at step: 102800 lr 0.000162
2024_12_30_18_31_08 Train loss: 0.0005309506668709219 at step: 103200 lr 0.000162
(Val @ epoch 22) acc: 1.0; ap: 1.0
2024_12_30_18_31_46 Train loss: 4.4815243427365203e-07 at step: 103600 lr 0.000162
2024_12_30_18_32_22 Train loss: 0.0007327890489250422 at step: 104000 lr 0.000162
2024_12_30_18_32_58 Train loss: 9.140901966020465e-06 at step: 104400 lr 0.000162
2024_12_30_18_33_34 Train loss: 2.87842539137273e-07 at step: 104800 lr 0.000162
2024_12_30_18_34_10 Train loss: 2.8220923923072405e-05 at step: 105200 lr 0.000162
2024_12_30_18_34_46 Train loss: 1.0587827091512736e-05 at step: 105600 lr 0.000162
2024_12_30_18_35_22 Train loss: 2.513577783247456e-05 at step: 106000 lr 0.000162
2024_12_30_18_35_58 Train loss: 9.996349945140537e-06 at step: 106400 lr 0.000162
2024_12_30_18_36_34 Train loss: 4.6523081209670636e-07 at step: 106800 lr 0.000162
2024_12_30_18_37_10 Train loss: 0.00026149279437959194 at step: 107200 lr 0.000162
2024_12_30_18_37_47 Train loss: 1.0129434485861566e-05 at step: 107600 lr 0.000162
2024_12_30_18_38_23 Train loss: 3.1901295187708456e-06 at step: 108000 lr 0.000162
(Val @ epoch 23) acc: 1.0; ap: 1.0
2024_12_30_18_39_01 Train loss: 6.059603663288726e-08 at step: 108400 lr 0.000162
2024_12_30_18_39_37 Train loss: 9.385680499462978e-08 at step: 108800 lr 0.000162
2024_12_30_18_40_13 Train loss: 0.0013005041982978582 at step: 109200 lr 0.000162
2024_12_30_18_40_49 Train loss: 3.984212980867596e-06 at step: 109600 lr 0.000162
2024_12_30_18_41_25 Train loss: 8.951830750447698e-06 at step: 110000 lr 0.000162
2024_12_30_18_42_01 Train loss: 2.120101498803706e-07 at step: 110400 lr 0.000162
2024_12_30_18_42_37 Train loss: 3.635883331298828e-06 at step: 110800 lr 0.000162
2024_12_30_18_43_13 Train loss: 3.722474062328729e-08 at step: 111200 lr 0.000162
2024_12_30_18_43_49 Train loss: 1.8464598952050437e-06 at step: 111600 lr 0.000162
2024_12_30_18_44_25 Train loss: 8.785724639892578e-05 at step: 112000 lr 0.000162
2024_12_30_18_45_01 Train loss: 2.4376589635721757e-07 at step: 112400 lr 0.000162
(Val @ epoch 24) acc: 0.999375; ap: 0.9999984394506867
2024_12_30_18_45_39 Train loss: 1.2201071513118222e-06 at step: 112800 lr 0.000162
2024_12_30_18_46_15 Train loss: 0.0008311665151268244 at step: 113200 lr 0.000162
2024_12_30_18_46_52 Train loss: 6.606259983499285e-09 at step: 113600 lr 0.000162
2024_12_30_18_47_28 Train loss: 0.00014531626948155463 at step: 114000 lr 0.000162
2024_12_30_18_48_04 Train loss: 7.808211194060277e-06 at step: 114400 lr 0.000162
2024_12_30_18_48_40 Train loss: 1.5484936000120797e-07 at step: 114800 lr 0.000162
2024_12_30_18_49_16 Train loss: 2.9152495972795123e-09 at step: 115200 lr 0.000162
2024_12_30_18_49_52 Train loss: 0.00013459433102980256 at step: 115600 lr 0.000162
2024_12_30_18_50_28 Train loss: 5.194657569518313e-05 at step: 116000 lr 0.000162
2024_12_30_18_51_04 Train loss: 5.834474450239213e-06 at step: 116400 lr 0.000162
2024_12_30_18_51_40 Train loss: 4.181509939371608e-05 at step: 116800 lr 0.000162
(Val @ epoch 25) acc: 1.0; ap: 1.0
2024_12_30_18_52_18 Train loss: 2.0853540263487957e-05 at step: 117200 lr 0.000162
2024_12_30_18_52_54 Train loss: 3.2727767163009958e-09 at step: 117600 lr 0.000162
2024_12_30_18_53_31 Train loss: 0.0124183539301157 at step: 118000 lr 0.000162
2024_12_30_18_54_07 Train loss: 2.4616718292236328e-05 at step: 118400 lr 0.000162
2024_12_30_18_54_43 Train loss: 9.777939339983277e-06 at step: 118800 lr 0.000162
2024_12_30_18_55_19 Train loss: 0.000711353961378336 at step: 119200 lr 0.000162
2024_12_30_18_55_55 Train loss: 1.1544171485411425e-07 at step: 119600 lr 0.000162
2024_12_30_18_56_31 Train loss: 6.820810085628182e-05 at step: 120000 lr 0.000162
2024_12_30_18_57_07 Train loss: 0.0004139983793720603 at step: 120400 lr 0.000162
2024_12_30_18_57_43 Train loss: 8.09271057278238e-07 at step: 120800 lr 0.000162
2024_12_30_18_58_20 Train loss: 1.91395861293131e-06 at step: 121200 lr 0.000162
(Val @ epoch 26) acc: 1.0; ap: 1.0
2024_12_30_18_58_58 Train loss: 1.297697281188448e-06 at step: 121600 lr 0.000162
2024_12_30_18_59_34 Train loss: 7.406746362903505e-07 at step: 122000 lr 0.000162
2024_12_30_19_00_10 Train loss: 3.84244485758245e-05 at step: 122400 lr 0.000162
2024_12_30_19_00_46 Train loss: 2.857850311954735e-08 at step: 122800 lr 0.000162
2024_12_30_19_01_22 Train loss: 5.068823611509288e-06 at step: 123200 lr 0.000162
2024_12_30_19_01_58 Train loss: 1.434076239092974e-06 at step: 123600 lr 0.000162
2024_12_30_19_02_34 Train loss: 3.101190484966487e-11 at step: 124000 lr 0.000162
2024_12_30_19_03_10 Train loss: 8.642969646643905e-07 at step: 124400 lr 0.000162
2024_12_30_19_03_47 Train loss: 8.855731437051872e-08 at step: 124800 lr 0.000162
2024_12_30_19_04_23 Train loss: 5.036639777245e-06 at step: 125200 lr 0.000162
2024_12_30_19_04_59 Train loss: 1.84290684046573e-06 at step: 125600 lr 0.000162
2024_12_30_19_05_35 Train loss: 3.5762897709901154e-07 at step: 126000 lr 0.000162
(Val @ epoch 27) acc: 1.0; ap: 1.0
2024_12_30_19_06_13 Train loss: 3.974954740360026e-11 at step: 126400 lr 0.000162
2024_12_30_19_06_49 Train loss: 6.556592779816128e-07 at step: 126800 lr 0.000162
2024_12_30_19_07_25 Train loss: 8.489205356454477e-06 at step: 127200 lr 0.000162
2024_12_30_19_08_01 Train loss: 4.671534952649381e-06 at step: 127600 lr 0.000162
2024_12_30_19_08_38 Train loss: 5.798635811515851e-07 at step: 128000 lr 0.000162
2024_12_30_19_09_14 Train loss: 2.835753548424691e-06 at step: 128400 lr 0.000162
2024_12_30_19_09_50 Train loss: 0.0003224206739105284 at step: 128800 lr 0.000162
2024_12_30_19_10_26 Train loss: 2.0471822848033838e-10 at step: 129200 lr 0.000162
2024_12_30_19_11_02 Train loss: 7.437806459620333e-08 at step: 129600 lr 0.000162
2024_12_30_19_11_38 Train loss: 1.2116017842345173e-06 at step: 130000 lr 0.000162
2024_12_30_19_12_14 Train loss: 4.856824489252176e-07 at step: 130400 lr 0.000162
(Val @ epoch 28) acc: 1.0; ap: 1.0
2024_12_30_19_12_52 Train loss: 2.190584638128712e-07 at step: 130800 lr 0.000162
2024_12_30_19_13_28 Train loss: 2.734341197907497e-09 at step: 131200 lr 0.000162
2024_12_30_19_14_05 Train loss: 5.161452776292208e-08 at step: 131600 lr 0.000162
2024_12_30_19_14_41 Train loss: 9.352214647151413e-08 at step: 132000 lr 0.000162
2024_12_30_19_15_17 Train loss: 6.699662549181085e-07 at step: 132400 lr 0.000162
2024_12_30_19_15_53 Train loss: 5.4277677463687724e-08 at step: 132800 lr 0.000162
2024_12_30_19_16_29 Train loss: 1.2189151675556786e-05 at step: 133200 lr 0.000162
2024_12_30_19_17_05 Train loss: 2.8240637561793847e-07 at step: 133600 lr 0.000162
2024_12_30_19_17_41 Train loss: 1.997826302613248e-06 at step: 134000 lr 0.000162
2024_12_30_19_18_17 Train loss: 3.2638386215921855e-08 at step: 134400 lr 0.000162
2024_12_30_19_18_54 Train loss: 0.0001953990722540766 at step: 134800 lr 0.000162
(Val @ epoch 29) acc: 1.0; ap: 1.0
2024_12_30_19_19_31 Train loss: 1.687175881670555e-06 at step: 135200 lr 0.000162
2024_12_30_19_20_08 Train loss: 1.8179815697294543e-06 at step: 135600 lr 0.000162
2024_12_30_19_20_44 Train loss: 5.066567155154189e-06 at step: 136000 lr 0.000162
2024_12_30_19_21_20 Train loss: 7.408588409418826e-09 at step: 136400 lr 0.000162
2024_12_30_19_21_56 Train loss: 0.0006823763251304626 at step: 136800 lr 0.000162
2024_12_30_19_22_32 Train loss: 1.0087897862831596e-05 at step: 137200 lr 0.000162
2024_12_30_19_23_08 Train loss: 1.6813118008940364e-07 at step: 137600 lr 0.000162
2024_12_30_19_23_44 Train loss: 1.0463083981449017e-06 at step: 138000 lr 0.000162
2024_12_30_19_24_20 Train loss: 1.262129626411479e-05 at step: 138400 lr 0.000162
2024_12_30_19_24_56 Train loss: 2.3270451947610127e-06 at step: 138800 lr 0.000162
2024_12_30_19_25_32 Train loss: 3.01421982840111e-06 at step: 139200 lr 0.000162
2024_12_30_19_26_02 changing lr at the end of epoch 30, iters 139531
*************************
Changing lr from 0.000162 to 0.00014580000000000002
*************************
(Val @ epoch 30) acc: 1.0; ap: 1.0
2024_12_30_19_26_10 Train loss: 2.208964815508807e-07 at step: 139600 lr 0.00014580000000000002
2024_12_30_19_26_46 Train loss: 4.7305085900006816e-05 at step: 140000 lr 0.00014580000000000002
2024_12_30_19_27_22 Train loss: 1.9163524029863765e-06 at step: 140400 lr 0.00014580000000000002
2024_12_30_19_27_59 Train loss: 4.603073648468126e-06 at step: 140800 lr 0.00014580000000000002
2024_12_30_19_28_35 Train loss: 8.032809091673698e-06 at step: 141200 lr 0.00014580000000000002
2024_12_30_19_29_11 Train loss: 0.00028692951309494674 at step: 141600 lr 0.00014580000000000002
2024_12_30_19_29_47 Train loss: 2.1463407051669492e-07 at step: 142000 lr 0.00014580000000000002
2024_12_30_19_30_23 Train loss: 5.23624430570635e-06 at step: 142400 lr 0.00014580000000000002
2024_12_30_19_30_59 Train loss: 3.189410563209094e-06 at step: 142800 lr 0.00014580000000000002
2024_12_30_19_31_35 Train loss: 5.662446369569807e-07 at step: 143200 lr 0.00014580000000000002
2024_12_30_19_32_11 Train loss: 5.931450687057804e-06 at step: 143600 lr 0.00014580000000000002
2024_12_30_19_32_48 Train loss: 1.2183362230189232e-07 at step: 144000 lr 0.00014580000000000002
(Val @ epoch 31) acc: 1.0; ap: 1.0
2024_12_30_19_33_26 Train loss: 5.152369794814149e-06 at step: 144400 lr 0.00014580000000000002
2024_12_30_19_34_02 Train loss: 2.086162709247219e-07 at step: 144800 lr 0.00014580000000000002
2024_12_30_19_34_38 Train loss: 1.8046825061901473e-05 at step: 145200 lr 0.00014580000000000002
2024_12_30_19_35_14 Train loss: 1.233816692547407e-05 at step: 145600 lr 0.00014580000000000002
2024_12_30_19_35_50 Train loss: 8.651628741063178e-05 at step: 146000 lr 0.00014580000000000002
2024_12_30_19_36_26 Train loss: 3.486898549454054e-06 at step: 146400 lr 0.00014580000000000002
2024_12_30_19_37_02 Train loss: 4.899313239548064e-07 at step: 146800 lr 0.00014580000000000002
2024_12_30_19_37_38 Train loss: 8.989297839434585e-08 at step: 147200 lr 0.00014580000000000002
2024_12_30_19_38_15 Train loss: 1.5575722500216216e-05 at step: 147600 lr 0.00014580000000000002
2024_12_30_19_38_51 Train loss: 4.137783980695531e-05 at step: 148000 lr 0.00014580000000000002
2024_12_30_19_39_27 Train loss: 1.3394954589962538e-10 at step: 148400 lr 0.00014580000000000002
(Val @ epoch 32) acc: 1.0; ap: 1.0
2024_12_30_19_40_05 Train loss: 7.849885970195522e-14 at step: 148800 lr 0.00014580000000000002
2024_12_30_19_40_41 Train loss: 1.0121291554431727e-13 at step: 149200 lr 0.00014580000000000002
2024_12_30_19_41_17 Train loss: 0.00018294407345820218 at step: 149600 lr 0.00014580000000000002
2024_12_30_19_41_53 Train loss: 7.53397955577384e-07 at step: 150000 lr 0.00014580000000000002
2024_12_30_19_42_29 Train loss: 1.8270154100719083e-07 at step: 150400 lr 0.00014580000000000002
2024_12_30_19_43_06 Train loss: 0.00039093222585506737 at step: 150800 lr 0.00014580000000000002
2024_12_30_19_43_42 Train loss: 4.835928848478943e-07 at step: 151200 lr 0.00014580000000000002
2024_12_30_19_44_18 Train loss: 6.735324859619141e-06 at step: 151600 lr 0.00014580000000000002
2024_12_30_19_44_54 Train loss: 3.279397933764194e-11 at step: 152000 lr 0.00014580000000000002
2024_12_30_19_45_30 Train loss: 3.6510198242467595e-06 at step: 152400 lr 0.00014580000000000002
2024_12_30_19_46_06 Train loss: 2.8115988243371248e-05 at step: 152800 lr 0.00014580000000000002
(Val @ epoch 33) acc: 1.0; ap: 1.0
2024_12_30_19_46_44 Train loss: 1.7607359040994197e-05 at step: 153200 lr 0.00014580000000000002
2024_12_30_19_47_20 Train loss: 8.034361620445907e-09 at step: 153600 lr 0.00014580000000000002
2024_12_30_19_47_57 Train loss: 1.2504663970958063e-09 at step: 154000 lr 0.00014580000000000002
2024_12_30_19_48_33 Train loss: 0.00024523952743038535 at step: 154400 lr 0.00014580000000000002
2024_12_30_19_49_09 Train loss: 2.7036406891056686e-07 at step: 154800 lr 0.00014580000000000002
2024_12_30_19_49_45 Train loss: 1.1571767572604585e-05 at step: 155200 lr 0.00014580000000000002
2024_12_30_19_50_21 Train loss: 5.260176112642512e-05 at step: 155600 lr 0.00014580000000000002
2024_12_30_19_50_57 Train loss: 1.2814998626708984e-06 at step: 156000 lr 0.00014580000000000002
2024_12_30_19_51_33 Train loss: 5.966388272327094e-08 at step: 156400 lr 0.00014580000000000002
2024_12_30_19_52_10 Train loss: 0.003979031927883625 at step: 156800 lr 0.00014580000000000002
2024_12_30_19_52_46 Train loss: 4.5299530029296875e-06 at step: 157200 lr 0.00014580000000000002
(Val @ epoch 34) acc: 1.0; ap: 1.0
2024_12_30_19_53_24 Train loss: 0.002127947984263301 at step: 157600 lr 0.00014580000000000002
2024_12_30_19_54_00 Train loss: 3.5166865473001963e-06 at step: 158000 lr 0.00014580000000000002
2024_12_30_19_54_36 Train loss: 1.497401456163061e-07 at step: 158400 lr 0.00014580000000000002
2024_12_30_19_55_12 Train loss: 3.224650697575271e-07 at step: 158800 lr 0.00014580000000000002
2024_12_30_19_55_48 Train loss: 2.6656074624042958e-05 at step: 159200 lr 0.00014580000000000002
2024_12_30_19_56_25 Train loss: 5.3328512876760215e-05 at step: 159600 lr 0.00014580000000000002
2024_12_30_19_57_01 Train loss: 5.901983003882283e-13 at step: 160000 lr 0.00014580000000000002
2024_12_30_19_57_37 Train loss: 5.960464477539063e-08 at step: 160400 lr 0.00014580000000000002
2024_12_30_19_58_13 Train loss: 7.1285794547293335e-06 at step: 160800 lr 0.00014580000000000002
2024_12_30_19_58_49 Train loss: 2.652407783898525e-05 at step: 161200 lr 0.00014580000000000002
2024_12_30_19_59_25 Train loss: 4.516097033047117e-05 at step: 161600 lr 0.00014580000000000002
2024_12_30_20_00_02 Train loss: 5.965389846096514e-06 at step: 162000 lr 0.00014580000000000002
(Val @ epoch 35) acc: 0.999375; ap: 1.0
2024_12_30_20_00_40 Train loss: 3.540982618233102e-08 at step: 162400 lr 0.00014580000000000002
2024_12_30_20_01_16 Train loss: 2.9802322387695312e-08 at step: 162800 lr 0.00014580000000000002
2024_12_30_20_01_52 Train loss: 1.877318034360087e-08 at step: 163200 lr 0.00014580000000000002
2024_12_30_20_02_28 Train loss: 2.801422624543193e-06 at step: 163600 lr 0.00014580000000000002
2024_12_30_20_03_04 Train loss: 0.00012840350973419845 at step: 164000 lr 0.00014580000000000002
2024_12_30_20_03_40 Train loss: 0.00021783733973279595 at step: 164400 lr 0.00014580000000000002
2024_12_30_20_04_16 Train loss: 1.0470188271938241e-06 at step: 164800 lr 0.00014580000000000002
2024_12_30_20_04_53 Train loss: 2.5271301637985744e-06 at step: 165200 lr 0.00014580000000000002
2024_12_30_20_05_29 Train loss: 3.0509896532748826e-05 at step: 165600 lr 0.00014580000000000002
2024_12_30_20_06_05 Train loss: 3.7257212426311526e-08 at step: 166000 lr 0.00014580000000000002
2024_12_30_20_06_41 Train loss: 8.121145947370678e-05 at step: 166400 lr 0.00014580000000000002
(Val @ epoch 36) acc: 1.0; ap: 1.0
2024_12_30_20_07_19 Train loss: 3.0484688551268846e-08 at step: 166800 lr 0.00014580000000000002
2024_12_30_20_07_55 Train loss: 1.65677338372916e-05 at step: 167200 lr 0.00014580000000000002
2024_12_30_20_08_31 Train loss: 1.048653643920261e-07 at step: 167600 lr 0.00014580000000000002
2024_12_30_20_09_08 Train loss: 0.002548751188442111 at step: 168000 lr 0.00014580000000000002
2024_12_30_20_09_44 Train loss: 2.9802997403294285e-08 at step: 168400 lr 0.00014580000000000002
2024_12_30_20_10_20 Train loss: 1.284830233316825e-07 at step: 168800 lr 0.00014580000000000002
2024_12_30_20_10_56 Train loss: 0.0011183521710336208 at step: 169200 lr 0.00014580000000000002
2024_12_30_20_11_32 Train loss: 1.8170388804694682e-10 at step: 169600 lr 0.00014580000000000002
2024_12_30_20_12_08 Train loss: 2.1482007145356974e-11 at step: 170000 lr 0.00014580000000000002
2024_12_30_20_12_44 Train loss: 1.6779381439846475e-07 at step: 170400 lr 0.00014580000000000002
2024_12_30_20_13_21 Train loss: 2.9985071137161867e-07 at step: 170800 lr 0.00014580000000000002
(Val @ epoch 37) acc: 1.0; ap: 1.0
2024_12_30_20_13_59 Train loss: 2.866557835545791e-08 at step: 171200 lr 0.00014580000000000002
2024_12_30_20_14_35 Train loss: 2.635180407217774e-10 at step: 171600 lr 0.00014580000000000002
2024_12_30_20_15_11 Train loss: 8.280590435560953e-09 at step: 172000 lr 0.00014580000000000002
2024_12_30_20_15_47 Train loss: 3.0202873091411675e-08 at step: 172400 lr 0.00014580000000000002
2024_12_30_20_16_23 Train loss: 1.8462193906998436e-07 at step: 172800 lr 0.00014580000000000002
2024_12_30_20_16_59 Train loss: 9.986421900975984e-06 at step: 173200 lr 0.00014580000000000002
2024_12_30_20_17_35 Train loss: 4.732112586225412e-07 at step: 173600 lr 0.00014580000000000002
2024_12_30_20_18_11 Train loss: 1.9563995579119364e-07 at step: 174000 lr 0.00014580000000000002
2024_12_30_20_18_48 Train loss: 1.4847990187621257e-13 at step: 174400 lr 0.00014580000000000002
2024_12_30_20_19_24 Train loss: 1.9505132513586432e-05 at step: 174800 lr 0.00014580000000000002
2024_12_30_20_20_00 Train loss: 0.00044733157847076654 at step: 175200 lr 0.00014580000000000002
(Val @ epoch 38) acc: 1.0; ap: 1.0
2024_12_30_20_20_38 Train loss: 6.375227530952543e-05 at step: 175600 lr 0.00014580000000000002
2024_12_30_20_21_14 Train loss: 4.7700211780465906e-08 at step: 176000 lr 0.00014580000000000002
2024_12_30_20_21_50 Train loss: 1.0968992683046963e-06 at step: 176400 lr 0.00014580000000000002
2024_12_30_20_22_26 Train loss: 0.0010659173130989075 at step: 176800 lr 0.00014580000000000002
2024_12_30_20_23_02 Train loss: 7.020870640417343e-08 at step: 177200 lr 0.00014580000000000002
2024_12_30_20_23_39 Train loss: 8.055600630996196e-09 at step: 177600 lr 0.00014580000000000002
2024_12_30_20_24_15 Train loss: 2.6295582378210725e-10 at step: 178000 lr 0.00014580000000000002
2024_12_30_20_24_51 Train loss: 9.745370334712788e-06 at step: 178400 lr 0.00014580000000000002
2024_12_30_20_25_27 Train loss: 8.073718049672607e-08 at step: 178800 lr 0.00014580000000000002
2024_12_30_20_26_03 Train loss: 1.7848709887857694e-07 at step: 179200 lr 0.00014580000000000002
2024_12_30_20_26_39 Train loss: 4.827134785639897e-12 at step: 179600 lr 0.00014580000000000002
2024_12_30_20_27_15 Train loss: 2.393890667917731e-07 at step: 180000 lr 0.00014580000000000002
(Val @ epoch 39) acc: 1.0; ap: 1.0
2024_12_30_20_27_53 Train loss: 3.875863541225044e-08 at step: 180400 lr 0.00014580000000000002
2024_12_30_20_28_30 Train loss: 8.34602133181761e-07 at step: 180800 lr 0.00014580000000000002
2024_12_30_20_29_06 Train loss: 2.688055160404712e-11 at step: 181200 lr 0.00014580000000000002
2024_12_30_20_29_42 Train loss: 5.5087021166855266e-08 at step: 181600 lr 0.00014580000000000002
2024_12_30_20_30_18 Train loss: 1.3077627727398067e-06 at step: 182000 lr 0.00014580000000000002
2024_12_30_20_30_54 Train loss: 7.748603820800781e-06 at step: 182400 lr 0.00014580000000000002
2024_12_30_20_31_31 Train loss: 3.419303902774118e-05 at step: 182800 lr 0.00014580000000000002
2024_12_30_20_32_07 Train loss: 2.938201859592482e-08 at step: 183200 lr 0.00014580000000000002
2024_12_30_20_32_43 Train loss: 7.784860756032685e-09 at step: 183600 lr 0.00014580000000000002
2024_12_30_20_33_19 Train loss: 5.494695187735488e-08 at step: 184000 lr 0.00014580000000000002
2024_12_30_20_33_55 Train loss: 1.2551157446694106e-08 at step: 184400 lr 0.00014580000000000002
2024_12_30_20_34_08 changing lr at the end of epoch 40, iters 184541
*************************
Changing lr from 0.00014580000000000005 to 0.00013122000000000003
*************************
(Val @ epoch 40) acc: 1.0; ap: 1.0
2024_12_30_20_34_33 Train loss: 3.701396735777962e-07 at step: 184800 lr 0.00013122000000000003
2024_12_30_20_35_09 Train loss: 4.814354692683764e-09 at step: 185200 lr 0.00013122000000000003
2024_12_30_20_35_46 Train loss: 0.00015875753888394684 at step: 185600 lr 0.00013122000000000003
2024_12_30_20_36_22 Train loss: 1.1920928955078125e-07 at step: 186000 lr 0.00013122000000000003
2024_12_30_20_36_58 Train loss: 4.330230149207637e-05 at step: 186400 lr 0.00013122000000000003
2024_12_30_20_37_34 Train loss: 2.2336840629577637e-05 at step: 186800 lr 0.00013122000000000003
2024_12_30_20_38_10 Train loss: 2.8811359698011074e-07 at step: 187200 lr 0.00013122000000000003
2024_12_30_20_38_47 Train loss: 2.801706922017644e-10 at step: 187600 lr 0.00013122000000000003
2024_12_30_20_39_23 Train loss: 2.981032309889997e-08 at step: 188000 lr 0.00013122000000000003
2024_12_30_20_39_59 Train loss: 1.9504025061678476e-09 at step: 188400 lr 0.00013122000000000003
2024_12_30_20_40_35 Train loss: 2.139277643919968e-09 at step: 188800 lr 0.00013122000000000003
(Val @ epoch 41) acc: 1.0; ap: 1.0
2024_12_30_20_41_13 Train loss: 0.0008579716086387634 at step: 189200 lr 0.00013122000000000003
2024_12_30_20_41_49 Train loss: 7.981124326761346e-06 at step: 189600 lr 0.00013122000000000003
2024_12_30_20_42_26 Train loss: 1.8880060451920144e-05 at step: 190000 lr 0.00013122000000000003
2024_12_30_20_43_02 Train loss: 5.219071113060636e-07 at step: 190400 lr 0.00013122000000000003
2024_12_30_20_43_38 Train loss: 8.893703551393628e-09 at step: 190800 lr 0.00013122000000000003
2024_12_30_20_44_14 Train loss: 2.218548331711645e-07 at step: 191200 lr 0.00013122000000000003
2024_12_30_20_44_50 Train loss: 6.826224474565379e-09 at step: 191600 lr 0.00013122000000000003
2024_12_30_20_45_27 Train loss: 3.436765894093696e-08 at step: 192000 lr 0.00013122000000000003
2024_12_30_20_46_03 Train loss: 1.2397576298894819e-08 at step: 192400 lr 0.00013122000000000003
2024_12_30_20_46_39 Train loss: 3.2331599868484773e-06 at step: 192800 lr 0.00013122000000000003
2024_12_30_20_47_15 Train loss: 4.657524499407373e-08 at step: 193200 lr 0.00013122000000000003
(Val @ epoch 42) acc: 0.999375; ap: 1.0
2024_12_30_20_47_53 Train loss: 2.473592758178711e-05 at step: 193600 lr 0.00013122000000000003
2024_12_30_20_48_29 Train loss: 0.00023299455642700195 at step: 194000 lr 0.00013122000000000003
2024_12_30_20_49_05 Train loss: 9.179115295410156e-06 at step: 194400 lr 0.00013122000000000003
2024_12_30_20_49_42 Train loss: 3.808887871770139e-09 at step: 194800 lr 0.00013122000000000003
2024_12_30_20_50_18 Train loss: 5.205176876188489e-07 at step: 195200 lr 0.00013122000000000003
2024_12_30_20_50_54 Train loss: 6.556510925292969e-07 at step: 195600 lr 0.00013122000000000003
2024_12_30_20_51_30 Train loss: 3.017368754854033e-08 at step: 196000 lr 0.00013122000000000003
2024_12_30_20_52_06 Train loss: 8.392039489990566e-07 at step: 196400 lr 0.00013122000000000003
2024_12_30_20_52_43 Train loss: 2.3326778286758554e-09 at step: 196800 lr 0.00013122000000000003
2024_12_30_20_53_19 Train loss: 6.974130997150496e-07 at step: 197200 lr 0.00013122000000000003
2024_12_30_20_53_55 Train loss: 2.9802382073285116e-07 at step: 197600 lr 0.00013122000000000003
2024_12_30_20_54_31 Train loss: 2.9802436074533034e-07 at step: 198000 lr 0.00013122000000000003
(Val @ epoch 43) acc: 0.999375; ap: 1.0
2024_12_30_20_55_09 Train loss: 2.682209867543861e-07 at step: 198400 lr 0.00013122000000000003
2024_12_30_20_55_45 Train loss: 8.898758174780141e-09 at step: 198800 lr 0.00013122000000000003
2024_12_30_20_56_22 Train loss: 2.351490516520026e-14 at step: 199200 lr 0.00013122000000000003
2024_12_30_20_56_58 Train loss: 2.730230277969703e-11 at step: 199600 lr 0.00013122000000000003
2024_12_30_20_57_34 Train loss: 0.0003439761931076646 at step: 200000 lr 0.00013122000000000003
2024_12_30_20_58_10 Train loss: 2.980251068152029e-08 at step: 200400 lr 0.00013122000000000003
2024_12_30_20_58_46 Train loss: 6.657116318820044e-05 at step: 200800 lr 0.00013122000000000003
2024_12_30_20_59_23 Train loss: 1.3119113573267782e-09 at step: 201200 lr 0.00013122000000000003
2024_12_30_20_59_59 Train loss: 4.316445156860027e-08 at step: 201600 lr 0.00013122000000000003
2024_12_30_21_00_35 Train loss: 8.615662003275126e-12 at step: 202000 lr 0.00013122000000000003
2024_12_30_21_01_11 Train loss: 2.2670694488358123e-10 at step: 202400 lr 0.00013122000000000003
(Val @ epoch 44) acc: 0.999375; ap: 1.0
2024_12_30_21_01_49 Train loss: 1.0602446698371182e-09 at step: 202800 lr 0.00013122000000000003
2024_12_30_21_02_25 Train loss: 5.975353900566915e-08 at step: 203200 lr 0.00013122000000000003
2024_12_30_21_03_02 Train loss: 4.887580871582031e-06 at step: 203600 lr 0.00013122000000000003
2024_12_30_21_03_38 Train loss: 0.0008642420871183276 at step: 204000 lr 0.00013122000000000003
2024_12_30_21_04_14 Train loss: 0.00011890651512658224 at step: 204400 lr 0.00013122000000000003
2024_12_30_21_04_50 Train loss: 2.9875895961595234e-06 at step: 204800 lr 0.00013122000000000003
2024_12_30_21_05_26 Train loss: 1.8251367350785586e-07 at step: 205200 lr 0.00013122000000000003
2024_12_30_21_06_03 Train loss: 4.248320861766075e-10 at step: 205600 lr 0.00013122000000000003
2024_12_30_21_06_39 Train loss: 4.317571633549733e-09 at step: 206000 lr 0.00013122000000000003
2024_12_30_21_07_15 Train loss: 1.4904227896295197e-07 at step: 206400 lr 0.00013122000000000003
2024_12_30_21_07_51 Train loss: 0.004827752709388733 at step: 206800 lr 0.00013122000000000003
(Val @ epoch 45) acc: 1.0; ap: 1.0
2024_12_30_21_08_29 Train loss: 6.010713605064666e-06 at step: 207200 lr 0.00013122000000000003
2024_12_30_21_09_05 Train loss: 1.497557491347834e-07 at step: 207600 lr 0.00013122000000000003
2024_12_30_21_09_42 Train loss: 3.367573242485378e-08 at step: 208000 lr 0.00013122000000000003
2024_12_30_21_10_18 Train loss: 1.7525724454089253e-16 at step: 208400 lr 0.00013122000000000003
2024_12_30_21_10_54 Train loss: 5.277620402921457e-06 at step: 208800 lr 0.00013122000000000003
2024_12_30_21_11_30 Train loss: 3.2127234206313915e-10 at step: 209200 lr 0.00013122000000000003
2024_12_30_21_12_06 Train loss: 0.001450301962904632 at step: 209600 lr 0.00013122000000000003
2024_12_30_21_12_42 Train loss: 0.00014569635095540434 at step: 210000 lr 0.00013122000000000003
2024_12_30_21_13_19 Train loss: 8.19256547401892e-06 at step: 210400 lr 0.00013122000000000003
2024_12_30_21_13_55 Train loss: 0.0006862480659037828 at step: 210800 lr 0.00013122000000000003
2024_12_30_21_14_31 Train loss: 3.7745084835449916e-11 at step: 211200 lr 0.00013122000000000003
(Val @ epoch 46) acc: 1.0; ap: 1.0
2024_12_30_21_15_09 Train loss: 0.0008340799249708652 at step: 211600 lr 0.00013122000000000003
2024_12_30_21_15_45 Train loss: 0.0015074978582561016 at step: 212000 lr 0.00013122000000000003
2024_12_30_21_16_21 Train loss: 5.530366615857929e-05 at step: 212400 lr 0.00013122000000000003
2024_12_30_21_16_57 Train loss: 1.6872909327503294e-05 at step: 212800 lr 0.00013122000000000003
2024_12_30_21_17_34 Train loss: 1.2265544135914297e-09 at step: 213200 lr 0.00013122000000000003
2024_12_30_21_18_10 Train loss: 3.4868717193603516e-06 at step: 213600 lr 0.00013122000000000003
2024_12_30_21_18_46 Train loss: 2.535532246383809e-07 at step: 214000 lr 0.00013122000000000003
2024_12_30_21_19_22 Train loss: 4.3164933433148533e-11 at step: 214400 lr 0.00013122000000000003
2024_12_30_21_19_58 Train loss: 1.449229625194448e-12 at step: 214800 lr 0.00013122000000000003
2024_12_30_21_20_34 Train loss: 3.2557265512878075e-05 at step: 215200 lr 0.00013122000000000003
2024_12_30_21_21_11 Train loss: 4.2322216842194393e-08 at step: 215600 lr 0.00013122000000000003
2024_12_30_21_21_47 Train loss: 1.804349096801161e-07 at step: 216000 lr 0.00013122000000000003
(Val @ epoch 47) acc: 1.0; ap: 1.0
2024_12_30_21_22_25 Train loss: 1.4515721886709798e-06 at step: 216400 lr 0.00013122000000000003
2024_12_30_21_23_01 Train loss: 5.513556970981881e-06 at step: 216800 lr 0.00013122000000000003
2024_12_30_21_23_37 Train loss: 2.864862513884958e-10 at step: 217200 lr 0.00013122000000000003
2024_12_30_21_24_13 Train loss: 5.594251888396684e-06 at step: 217600 lr 0.00013122000000000003
2024_12_30_21_24_49 Train loss: 3.042743628611788e-06 at step: 218000 lr 0.00013122000000000003
2024_12_30_21_25_25 Train loss: 7.190781303734184e-10 at step: 218400 lr 0.00013122000000000003
2024_12_30_21_26_02 Train loss: 4.5386238411992963e-07 at step: 218800 lr 0.00013122000000000003
2024_12_30_21_26_38 Train loss: 8.60577742400892e-09 at step: 219200 lr 0.00013122000000000003
2024_12_30_21_27_14 Train loss: 1.0528975025181353e-08 at step: 219600 lr 0.00013122000000000003
2024_12_30_21_27_50 Train loss: 8.41848701860215e-10 at step: 220000 lr 0.00013122000000000003
2024_12_30_21_28_26 Train loss: 0.001127980649471283 at step: 220400 lr 0.00013122000000000003
(Val @ epoch 48) acc: 1.0; ap: 1.0
2024_12_30_21_29_04 Train loss: 6.80708325262458e-08 at step: 220800 lr 0.00013122000000000003
2024_12_30_21_29_41 Train loss: 8.940696716308594e-08 at step: 221200 lr 0.00013122000000000003
2024_12_30_21_30_17 Train loss: 6.285755915769187e-08 at step: 221600 lr 0.00013122000000000003
2024_12_30_21_30_53 Train loss: 1.4969181165724876e-06 at step: 222000 lr 0.00013122000000000003
2024_12_30_21_31_29 Train loss: 1.8775679109239718e-06 at step: 222400 lr 0.00013122000000000003
2024_12_30_21_32_05 Train loss: 3.5762786865234375e-07 at step: 222800 lr 0.00013122000000000003
2024_12_30_21_32_41 Train loss: 2.9802322387695312e-08 at step: 223200 lr 0.00013122000000000003
2024_12_30_21_33_18 Train loss: 3.074235976896489e-08 at step: 223600 lr 0.00013122000000000003
2024_12_30_21_33_54 Train loss: 9.238950156031933e-07 at step: 224000 lr 0.00013122000000000003
2024_12_30_21_34_30 Train loss: 6.721650436247728e-08 at step: 224400 lr 0.00013122000000000003
2024_12_30_21_35_06 Train loss: 3.4431781870125633e-09 at step: 224800 lr 0.00013122000000000003
(Val @ epoch 49) acc: 0.99875; ap: 1.0
*************************
2024_12_30_21_35_31
(0 progan    ) acc: 99.9; ap: 100.0
(1 stylegan  ) acc: 93.3; ap: 99.5
(2 stylegan2 ) acc: 96.8; ap: 99.9
(3 biggan    ) acc: 75.8; ap: 85.7
(4 cyclegan  ) acc: 79.8; ap: 98.0
(5 stargan   ) acc: 78.0; ap: 98.9
(6 gaugan    ) acc: 74.0; ap: 79.8
(7 deepfake  ) acc: 73.5; ap: 80.4
(8 Mean      ) acc: 83.9; ap: 92.8
*************************
2024_12_30_21_36_42
Saving model ./checkpoints/4class-resnet-car-cat-chair-horse2024_12_30_15_55_27/model_epoch_last.pth
