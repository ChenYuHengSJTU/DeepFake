train.py  --name  4class-resnet-car-cat-chair-horse  --dataroot  /home/chenyuheng/Code/LGrad/ForenSynths_4classtrain_val_test  --classes  car,cat,chair,horse  --batch_size  32  --delr_freq  10  --lr  0.001  --niter  85
==================== requires_grad Ture weight1
==================== requires_grad Ture bias1
==================== requires_grad Ture weight2
==================== requires_grad Ture bias2
==================== requires_grad Ture weight3
==================== requires_grad Ture bias3
==================== requires_grad Ture weight4
==================== requires_grad Ture bias4
==================== requires_grad Ture realconv1.weight
==================== requires_grad Ture imagconv1.weight
==================== requires_grad Ture realconv2.weight
==================== requires_grad Ture imagconv2.weight
==================== requires_grad Ture realconv3.weight
==================== requires_grad Ture imagconv3.weight
==================== requires_grad Ture realconv4.weight
==================== requires_grad Ture imagconv4.weight
==================== requires_grad Ture layer1.0.conv1.weight
==================== requires_grad Ture layer1.0.bn1.weight
==================== requires_grad Ture layer1.0.bn1.bias
==================== requires_grad Ture layer1.0.conv2.weight
==================== requires_grad Ture layer1.0.bn2.weight
==================== requires_grad Ture layer1.0.bn2.bias
==================== requires_grad Ture layer1.0.conv3.weight
==================== requires_grad Ture layer1.0.bn3.weight
==================== requires_grad Ture layer1.0.bn3.bias
==================== requires_grad Ture layer1.0.downsample.0.weight
==================== requires_grad Ture layer1.0.downsample.1.weight
==================== requires_grad Ture layer1.0.downsample.1.bias
==================== requires_grad Ture layer1.1.conv1.weight
==================== requires_grad Ture layer1.1.bn1.weight
==================== requires_grad Ture layer1.1.bn1.bias
==================== requires_grad Ture layer1.1.conv2.weight
==================== requires_grad Ture layer1.1.bn2.weight
==================== requires_grad Ture layer1.1.bn2.bias
==================== requires_grad Ture layer1.1.conv3.weight
==================== requires_grad Ture layer1.1.bn3.weight
==================== requires_grad Ture layer1.1.bn3.bias
==================== requires_grad Ture layer1.2.conv1.weight
==================== requires_grad Ture layer1.2.bn1.weight
==================== requires_grad Ture layer1.2.bn1.bias
==================== requires_grad Ture layer1.2.conv2.weight
==================== requires_grad Ture layer1.2.bn2.weight
==================== requires_grad Ture layer1.2.bn2.bias
==================== requires_grad Ture layer1.2.conv3.weight
==================== requires_grad Ture layer1.2.bn3.weight
==================== requires_grad Ture layer1.2.bn3.bias
==================== requires_grad Ture layer2.0.conv1.weight
==================== requires_grad Ture layer2.0.bn1.weight
==================== requires_grad Ture layer2.0.bn1.bias
==================== requires_grad Ture layer2.0.conv2.weight
==================== requires_grad Ture layer2.0.bn2.weight
==================== requires_grad Ture layer2.0.bn2.bias
==================== requires_grad Ture layer2.0.conv3.weight
==================== requires_grad Ture layer2.0.bn3.weight
==================== requires_grad Ture layer2.0.bn3.bias
==================== requires_grad Ture layer2.0.downsample.0.weight
==================== requires_grad Ture layer2.0.downsample.1.weight
==================== requires_grad Ture layer2.0.downsample.1.bias
==================== requires_grad Ture layer2.1.conv1.weight
==================== requires_grad Ture layer2.1.bn1.weight
==================== requires_grad Ture layer2.1.bn1.bias
==================== requires_grad Ture layer2.1.conv2.weight
==================== requires_grad Ture layer2.1.bn2.weight
==================== requires_grad Ture layer2.1.bn2.bias
==================== requires_grad Ture layer2.1.conv3.weight
==================== requires_grad Ture layer2.1.bn3.weight
==================== requires_grad Ture layer2.1.bn3.bias
==================== requires_grad Ture layer2.2.conv1.weight
==================== requires_grad Ture layer2.2.bn1.weight
==================== requires_grad Ture layer2.2.bn1.bias
==================== requires_grad Ture layer2.2.conv2.weight
==================== requires_grad Ture layer2.2.bn2.weight
==================== requires_grad Ture layer2.2.bn2.bias
==================== requires_grad Ture layer2.2.conv3.weight
==================== requires_grad Ture layer2.2.bn3.weight
==================== requires_grad Ture layer2.2.bn3.bias
==================== requires_grad Ture layer2.3.conv1.weight
==================== requires_grad Ture layer2.3.bn1.weight
==================== requires_grad Ture layer2.3.bn1.bias
==================== requires_grad Ture layer2.3.conv2.weight
==================== requires_grad Ture layer2.3.bn2.weight
==================== requires_grad Ture layer2.3.bn2.bias
==================== requires_grad Ture layer2.3.conv3.weight
==================== requires_grad Ture layer2.3.bn3.weight
==================== requires_grad Ture layer2.3.bn3.bias
==================== requires_grad Ture fc1.weight
==================== requires_grad Ture fc1.bias

cwd: /home/chenyuheng/Code/FreqNet-DeepfakeDetection
2025_03_02_19_47_08 Train loss: 0.6970304250717163 at step: 400 lr 0.001
2025_03_02_19_50_08 Train loss: 0.7058456540107727 at step: 800 lr 0.001
2025_03_02_19_53_04 Train loss: 0.6922115087509155 at step: 1200 lr 0.001
2025_03_02_19_56_01 Train loss: 0.7021877765655518 at step: 1600 lr 0.001
2025_03_02_19_58_56 Train loss: 0.6821236610412598 at step: 2000 lr 0.001
2025_03_02_20_01_52 Train loss: 0.22877360880374908 at step: 2400 lr 0.001
2025_03_02_20_04_14 Train loss: 0.24789418280124664 at step: 2800 lr 0.001
2025_03_02_20_06_28 Train loss: 0.022832609713077545 at step: 3200 lr 0.001
2025_03_02_20_08_51 Train loss: 0.06540316343307495 at step: 3600 lr 0.001
2025_03_02_20_11_47 Train loss: 0.013921543955802917 at step: 4000 lr 0.001
2025_03_02_20_14_43 Train loss: 0.004579258617013693 at step: 4400 lr 0.001
(Val @ epoch 0) acc: 0.951875; ap: 0.9994007938198566
2025_03_02_20_17_44 Train loss: 0.0755888968706131 at step: 4800 lr 0.001
2025_03_02_20_20_43 Train loss: 0.0029078004881739616 at step: 5200 lr 0.001
2025_03_02_20_23_39 Train loss: 0.004385794512927532 at step: 5600 lr 0.001
2025_03_02_20_26_16 Train loss: 0.02397395670413971 at step: 6000 lr 0.001
2025_03_02_20_28_29 Train loss: 0.003355493303388357 at step: 6400 lr 0.001
2025_03_02_20_30_43 Train loss: 0.00027146944194100797 at step: 6800 lr 0.001
2025_03_02_20_32_56 Train loss: 0.0005963589064776897 at step: 7200 lr 0.001
2025_03_02_20_35_10 Train loss: 0.010636048391461372 at step: 7600 lr 0.001
2025_03_02_20_37_24 Train loss: 0.0036967594642192125 at step: 8000 lr 0.001
2025_03_02_20_39_37 Train loss: 0.08135740458965302 at step: 8400 lr 0.001
2025_03_02_20_41_51 Train loss: 0.012260334566235542 at step: 8800 lr 0.001
(Val @ epoch 1) acc: 0.9975; ap: 0.9999843845971808
2025_03_02_20_44_12 Train loss: 0.03402196243405342 at step: 9200 lr 0.001
2025_03_02_20_46_25 Train loss: 0.005229175090789795 at step: 9600 lr 0.001
2025_03_02_20_48_39 Train loss: 0.049468062818050385 at step: 10000 lr 0.001
2025_03_02_20_50_53 Train loss: 0.016458826139569283 at step: 10400 lr 0.001
2025_03_02_20_53_06 Train loss: 0.04403691738843918 at step: 10800 lr 0.001
2025_03_02_20_55_20 Train loss: 0.019623922184109688 at step: 11200 lr 0.001
2025_03_02_20_57_34 Train loss: 0.002073348965495825 at step: 11600 lr 0.001
2025_03_02_20_59_47 Train loss: 0.0009486725903116167 at step: 12000 lr 0.001
2025_03_02_21_02_01 Train loss: 0.0005813470343127847 at step: 12400 lr 0.001
2025_03_02_21_04_15 Train loss: 0.0011768615804612637 at step: 12800 lr 0.001
2025_03_02_21_06_28 Train loss: 0.001532956026494503 at step: 13200 lr 0.001
(Val @ epoch 2) acc: 1.0; ap: 1.0
2025_03_02_21_08_49 Train loss: 0.0071213385090231895 at step: 13600 lr 0.001
2025_03_02_21_11_03 Train loss: 4.7047240514075384e-05 at step: 14000 lr 0.001
2025_03_02_21_13_16 Train loss: 0.00023649202194064856 at step: 14400 lr 0.001
2025_03_02_21_15_30 Train loss: 0.0009455219260416925 at step: 14800 lr 0.001
2025_03_02_21_17_44 Train loss: 0.002428472274914384 at step: 15200 lr 0.001
2025_03_02_21_19_57 Train loss: 2.85035275737755e-05 at step: 15600 lr 0.001
2025_03_02_21_22_11 Train loss: 4.615868965629488e-05 at step: 16000 lr 0.001
2025_03_02_21_24_25 Train loss: 0.0005995815736241639 at step: 16400 lr 0.001
2025_03_02_21_26_38 Train loss: 0.04877317324280739 at step: 16800 lr 0.001
2025_03_02_21_28_52 Train loss: 8.625224290881306e-05 at step: 17200 lr 0.001
2025_03_02_21_31_06 Train loss: 0.002603039611130953 at step: 17600 lr 0.001
2025_03_02_21_33_19 Train loss: 0.011663230136036873 at step: 18000 lr 0.001
(Val @ epoch 3) acc: 0.9975; ap: 0.9999937616943908
2025_03_02_21_35_40 Train loss: 0.0009601587662473321 at step: 18400 lr 0.001
2025_03_02_21_37_54 Train loss: 0.00011204995098523796 at step: 18800 lr 0.001
2025_03_02_21_40_07 Train loss: 0.01942834071815014 at step: 19200 lr 0.001
2025_03_02_21_42_21 Train loss: 1.644306757953018e-05 at step: 19600 lr 0.001
2025_03_02_21_44_35 Train loss: 0.0003883589815814048 at step: 20000 lr 0.001
2025_03_02_21_46_48 Train loss: 5.951525236014277e-05 at step: 20400 lr 0.001
2025_03_02_21_49_02 Train loss: 0.003761611646041274 at step: 20800 lr 0.001
2025_03_02_21_51_15 Train loss: 2.966256215586327e-05 at step: 21200 lr 0.001
2025_03_02_21_53_29 Train loss: 0.016149383038282394 at step: 21600 lr 0.001
2025_03_02_21_55_43 Train loss: 0.00036256431485526264 at step: 22000 lr 0.001
2025_03_02_21_57_56 Train loss: 0.00024377103545702994 at step: 22400 lr 0.001
(Val @ epoch 4) acc: 0.99875; ap: 0.9999984394506867
2025_03_02_22_00_17 Train loss: 0.00015409347543027252 at step: 22800 lr 0.001
2025_03_02_22_02_30 Train loss: 9.949275408871472e-06 at step: 23200 lr 0.001
2025_03_02_22_04_44 Train loss: 0.0004043962399009615 at step: 23600 lr 0.001
2025_03_02_22_06_57 Train loss: 4.77654357382562e-05 at step: 24000 lr 0.001
2025_03_02_22_09_11 Train loss: 0.00033011878258548677 at step: 24400 lr 0.001
2025_03_02_22_11_25 Train loss: 0.0001998821971938014 at step: 24800 lr 0.001
2025_03_02_22_13_38 Train loss: 2.1377505618147552e-05 at step: 25200 lr 0.001
2025_03_02_22_15_52 Train loss: 0.003608410246670246 at step: 25600 lr 0.001
2025_03_02_22_18_05 Train loss: 0.001609517727047205 at step: 26000 lr 0.001
2025_03_02_22_20_19 Train loss: 0.013337833806872368 at step: 26400 lr 0.001
2025_03_02_22_22_33 Train loss: 0.000756505411118269 at step: 26800 lr 0.001
(Val @ epoch 5) acc: 0.9975; ap: 0.9999783218026078
2025_03_02_22_24_53 Train loss: 7.6034980338590685e-06 at step: 27200 lr 0.001
2025_03_02_22_27_07 Train loss: 5.799889549962245e-06 at step: 27600 lr 0.001
2025_03_02_22_29_20 Train loss: 1.1302324310236145e-05 at step: 28000 lr 0.001
2025_03_02_22_31_34 Train loss: 7.788991752022412e-06 at step: 28400 lr 0.001
2025_03_02_22_33_47 Train loss: 0.0007164558628574014 at step: 28800 lr 0.001
2025_03_02_22_36_01 Train loss: 0.0016969885909929872 at step: 29200 lr 0.001
2025_03_02_22_38_14 Train loss: 4.9281487008556724e-05 at step: 29600 lr 0.001
2025_03_02_22_40_28 Train loss: 7.097544585121796e-05 at step: 30000 lr 0.001
2025_03_02_22_42_42 Train loss: 6.340266554616392e-05 at step: 30400 lr 0.001
2025_03_02_22_44_55 Train loss: 6.5907011048693676e-06 at step: 30800 lr 0.001
2025_03_02_22_47_09 Train loss: 0.00034339798730798066 at step: 31200 lr 0.001
(Val @ epoch 6) acc: 0.99875; ap: 0.9999984394506867
2025_03_02_22_49_29 Train loss: 0.0007779982988722622 at step: 31600 lr 0.001
2025_03_02_22_51_42 Train loss: 0.0033394917845726013 at step: 32000 lr 0.001
2025_03_02_22_53_56 Train loss: 0.06503097712993622 at step: 32400 lr 0.001
2025_03_02_22_56_09 Train loss: 0.0018812862690538168 at step: 32800 lr 0.001
2025_03_02_22_58_23 Train loss: 3.08240887534339e-05 at step: 33200 lr 0.001
2025_03_02_23_00_36 Train loss: 2.2028085368219763e-05 at step: 33600 lr 0.001
2025_03_02_23_02_50 Train loss: 0.0009462644811719656 at step: 34000 lr 0.001
2025_03_02_23_05_04 Train loss: 0.00012543771299533546 at step: 34400 lr 0.001
2025_03_02_23_07_17 Train loss: 0.0005783349624834955 at step: 34800 lr 0.001
2025_03_02_23_09_31 Train loss: 0.003202463500201702 at step: 35200 lr 0.001
2025_03_02_23_11_44 Train loss: 0.001105939270928502 at step: 35600 lr 0.001
2025_03_02_23_13_58 Train loss: 0.029144732281565666 at step: 36000 lr 0.001
(Val @ epoch 7) acc: 0.994375; ap: 0.9999922185945275
2025_03_02_23_16_18 Train loss: 0.00033998716389760375 at step: 36400 lr 0.001
2025_03_02_23_18_31 Train loss: 0.00045070063788443804 at step: 36800 lr 0.001
2025_03_02_23_20_45 Train loss: 0.0002839588560163975 at step: 37200 lr 0.001
2025_03_02_23_22_58 Train loss: 1.0244678378512617e-05 at step: 37600 lr 0.001
2025_03_02_23_25_12 Train loss: 8.307053212774917e-05 at step: 38000 lr 0.001
2025_03_02_23_27_25 Train loss: 8.675408025737852e-05 at step: 38400 lr 0.001
2025_03_02_23_29_39 Train loss: 0.0005331729189492762 at step: 38800 lr 0.001
2025_03_02_23_31_52 Train loss: 6.287149062700337e-06 at step: 39200 lr 0.001
2025_03_02_23_34_06 Train loss: 0.003882859367877245 at step: 39600 lr 0.001
2025_03_02_23_36_19 Train loss: 0.008793924003839493 at step: 40000 lr 0.001
2025_03_02_23_38_33 Train loss: 0.00011765456292778254 at step: 40400 lr 0.001
(Val @ epoch 8) acc: 1.0; ap: 1.0
2025_03_02_23_40_53 Train loss: 0.009853383526206017 at step: 40800 lr 0.001
2025_03_02_23_43_06 Train loss: 0.09619789570569992 at step: 41200 lr 0.001
2025_03_02_23_45_20 Train loss: 1.2317503887970815e-06 at step: 41600 lr 0.001
2025_03_02_23_47_33 Train loss: 0.00034517329186201096 at step: 42000 lr 0.001
2025_03_02_23_49_47 Train loss: 0.00017591800133232027 at step: 42400 lr 0.001
2025_03_02_23_52_00 Train loss: 0.00025896591250784695 at step: 42800 lr 0.001
2025_03_02_23_54_13 Train loss: 2.913212483690586e-05 at step: 43200 lr 0.001
2025_03_02_23_56_27 Train loss: 8.996274118544534e-06 at step: 43600 lr 0.001
2025_03_02_23_58_40 Train loss: 2.0437873899936676e-06 at step: 44000 lr 0.001
2025_03_03_00_00_54 Train loss: 0.0014524717116728425 at step: 44400 lr 0.001
2025_03_03_00_03_07 Train loss: 0.00010634591308189556 at step: 44800 lr 0.001
(Val @ epoch 9) acc: 0.999375; ap: 0.9999937810945274
2025_03_03_00_05_27 Train loss: 9.632790897740051e-05 at step: 45200 lr 0.001
2025_03_03_00_07_40 Train loss: 1.851059823820833e-05 at step: 45600 lr 0.001
2025_03_03_00_09_54 Train loss: 0.00016579485964030027 at step: 46000 lr 0.001
2025_03_03_00_12_07 Train loss: 1.8346496290178038e-05 at step: 46400 lr 0.001
2025_03_03_00_14_21 Train loss: 0.001468860893510282 at step: 46800 lr 0.001
2025_03_03_00_16_34 Train loss: 2.3065804271027446e-05 at step: 47200 lr 0.001
2025_03_03_00_18_48 Train loss: 1.2779966709786095e-05 at step: 47600 lr 0.001
2025_03_03_00_21_01 Train loss: 0.00014395487960428 at step: 48000 lr 0.001
2025_03_03_00_23_14 Train loss: 2.6942510885419324e-05 at step: 48400 lr 0.001
2025_03_03_00_25_28 Train loss: 0.008905366063117981 at step: 48800 lr 0.001
2025_03_03_00_27_41 Train loss: 1.7022042811731808e-05 at step: 49200 lr 0.001
2025_03_03_00_29_25 changing lr at the end of epoch 10, iters 49511
*************************
Changing lr from 0.001 to 0.0008
*************************
(Val @ epoch 10) acc: 1.0; ap: 1.0
2025_03_03_00_30_01 Train loss: 1.5576799341943115e-05 at step: 49600 lr 0.0008
2025_03_03_00_32_15 Train loss: 0.004124077502638102 at step: 50000 lr 0.0008
2025_03_03_00_34_28 Train loss: 2.2424458165914984e-06 at step: 50400 lr 0.0008
2025_03_03_00_36_41 Train loss: 1.8415208614896983e-05 at step: 50800 lr 0.0008
2025_03_03_00_38_55 Train loss: 0.0007209732430055737 at step: 51200 lr 0.0008
2025_03_03_00_41_08 Train loss: 0.0001390696270391345 at step: 51600 lr 0.0008
2025_03_03_00_43_22 Train loss: 7.98913674771029e-07 at step: 52000 lr 0.0008
2025_03_03_00_45_35 Train loss: 0.00011902848200406879 at step: 52400 lr 0.0008
2025_03_03_00_47_48 Train loss: 1.935694854182657e-05 at step: 52800 lr 0.0008
2025_03_03_00_50_02 Train loss: 8.471796718367841e-06 at step: 53200 lr 0.0008
2025_03_03_00_52_15 Train loss: 7.842325430829078e-06 at step: 53600 lr 0.0008
2025_03_03_00_54_29 Train loss: 0.00034733698703348637 at step: 54000 lr 0.0008
(Val @ epoch 11) acc: 1.0; ap: 1.0
2025_03_03_00_56_49 Train loss: 2.883647312046378e-06 at step: 54400 lr 0.0008
2025_03_03_00_59_02 Train loss: 1.4992190244811354e-06 at step: 54800 lr 0.0008
2025_03_03_01_01_16 Train loss: 3.513004998012548e-08 at step: 55200 lr 0.0008
2025_03_03_01_03_29 Train loss: 0.011784745380282402 at step: 55600 lr 0.0008
2025_03_03_01_05_42 Train loss: 1.2430200513335876e-05 at step: 56000 lr 0.0008
2025_03_03_01_07_56 Train loss: 5.6038952607195824e-06 at step: 56400 lr 0.0008
2025_03_03_01_10_09 Train loss: 0.00015474777319468558 at step: 56800 lr 0.0008
2025_03_03_01_12_23 Train loss: 1.1600782272580545e-06 at step: 57200 lr 0.0008
2025_03_03_01_14_36 Train loss: 5.744473310187459e-05 at step: 57600 lr 0.0008
2025_03_03_01_16_49 Train loss: 1.3169203612051206e-06 at step: 58000 lr 0.0008
2025_03_03_01_19_03 Train loss: 0.00398625573143363 at step: 58400 lr 0.0008
(Val @ epoch 12) acc: 1.0; ap: 1.0
2025_03_03_01_21_23 Train loss: 3.0063893063925207e-06 at step: 58800 lr 0.0008
2025_03_03_01_23_36 Train loss: 1.9420834718175684e-08 at step: 59200 lr 0.0008
2025_03_03_01_25_50 Train loss: 5.891931323276367e-06 at step: 59600 lr 0.0008
2025_03_03_01_28_03 Train loss: 7.242220817715861e-06 at step: 60000 lr 0.0008
2025_03_03_01_30_17 Train loss: 1.2051517614963814e-06 at step: 60400 lr 0.0008
2025_03_03_01_32_30 Train loss: 0.004921890329569578 at step: 60800 lr 0.0008
2025_03_03_01_34_44 Train loss: 1.0364431091147708e-06 at step: 61200 lr 0.0008
2025_03_03_01_36_57 Train loss: 0.000560039363335818 at step: 61600 lr 0.0008
2025_03_03_01_39_10 Train loss: 1.6816616152937058e-06 at step: 62000 lr 0.0008
2025_03_03_01_41_24 Train loss: 4.526483508016099e-07 at step: 62400 lr 0.0008
2025_03_03_01_43_37 Train loss: 2.018258555835928e-06 at step: 62800 lr 0.0008
(Val @ epoch 13) acc: 1.0; ap: 1.0
2025_03_03_01_45_57 Train loss: 3.141204433632083e-05 at step: 63200 lr 0.0008
2025_03_03_01_48_11 Train loss: 0.0012213191948831081 at step: 63600 lr 0.0008
2025_03_03_01_50_24 Train loss: 1.6295320165227167e-05 at step: 64000 lr 0.0008
2025_03_03_01_52_38 Train loss: 0.0010470983106642962 at step: 64400 lr 0.0008
2025_03_03_01_54_51 Train loss: 6.367613547553219e-09 at step: 64800 lr 0.0008
2025_03_03_01_57_04 Train loss: 5.933237389399437e-06 at step: 65200 lr 0.0008
2025_03_03_01_59_18 Train loss: 8.050809265114367e-05 at step: 65600 lr 0.0008
2025_03_03_02_01_31 Train loss: 0.0015986516373232007 at step: 66000 lr 0.0008
2025_03_03_02_03_45 Train loss: 6.2947242440714035e-06 at step: 66400 lr 0.0008
2025_03_03_02_05_58 Train loss: 1.3508723384347832e-07 at step: 66800 lr 0.0008
2025_03_03_02_08_12 Train loss: 7.179212389019085e-07 at step: 67200 lr 0.0008
(Val @ epoch 14) acc: 0.999375; ap: 0.9999984394506867
2025_03_03_02_10_32 Train loss: 2.9077088470330636e-07 at step: 67600 lr 0.0008
2025_03_03_02_12_45 Train loss: 0.00027226065867580473 at step: 68000 lr 0.0008
2025_03_03_02_14_59 Train loss: 1.1989533277301234e-06 at step: 68400 lr 0.0008
2025_03_03_02_17_12 Train loss: 2.306711940036621e-05 at step: 68800 lr 0.0008
2025_03_03_02_19_25 Train loss: 1.0478720469109248e-07 at step: 69200 lr 0.0008
2025_03_03_02_21_39 Train loss: 5.960467319710006e-08 at step: 69600 lr 0.0008
2025_03_03_02_23_52 Train loss: 5.662504349857045e-07 at step: 70000 lr 0.0008
2025_03_03_02_26_06 Train loss: 5.620028969133273e-05 at step: 70400 lr 0.0008
2025_03_03_02_28_19 Train loss: 0.000509098987095058 at step: 70800 lr 0.0008
2025_03_03_02_30_32 Train loss: 0.0005303040961734951 at step: 71200 lr 0.0008
2025_03_03_02_32_46 Train loss: 2.3300814063986763e-05 at step: 71600 lr 0.0008
2025_03_03_02_34_59 Train loss: 5.236624474491691e-06 at step: 72000 lr 0.0008
(Val @ epoch 15) acc: 1.0; ap: 1.0
2025_03_03_02_37_19 Train loss: 5.750493770051435e-08 at step: 72400 lr 0.0008
2025_03_03_02_39_33 Train loss: 5.008545485907234e-06 at step: 72800 lr 0.0008
2025_03_03_02_41_46 Train loss: 0.0007520784856751561 at step: 73200 lr 0.0008
2025_03_03_02_44_00 Train loss: 4.651739072869532e-05 at step: 73600 lr 0.0008
2025_03_03_02_46_13 Train loss: 1.0246022611681838e-05 at step: 74000 lr 0.0008
2025_03_03_02_48_27 Train loss: 0.00017906579887494445 at step: 74400 lr 0.0008
2025_03_03_02_50_40 Train loss: 2.6782177883433178e-05 at step: 74800 lr 0.0008
2025_03_03_02_52_53 Train loss: 0.0024195387959480286 at step: 75200 lr 0.0008
2025_03_03_02_55_07 Train loss: 1.5922553302516462e-06 at step: 75600 lr 0.0008
2025_03_03_02_57_20 Train loss: 0.00011366607213858515 at step: 76000 lr 0.0008
2025_03_03_02_59_34 Train loss: 1.8473228919901885e-06 at step: 76400 lr 0.0008
(Val @ epoch 16) acc: 1.0; ap: 1.0
2025_03_03_03_01_54 Train loss: 0.0005921798292547464 at step: 76800 lr 0.0008
2025_03_03_03_04_07 Train loss: 0.0004273944068700075 at step: 77200 lr 0.0008
2025_03_03_03_06_21 Train loss: 4.7081098273338284e-06 at step: 77600 lr 0.0008
2025_03_03_03_08_34 Train loss: 1.5078726391948294e-06 at step: 78000 lr 0.0008
2025_03_03_03_10_48 Train loss: 2.320209750905633e-05 at step: 78400 lr 0.0008
2025_03_03_03_13_01 Train loss: 0.00014591848594136536 at step: 78800 lr 0.0008
2025_03_03_03_15_15 Train loss: 1.8261420336784795e-05 at step: 79200 lr 0.0008
2025_03_03_03_17_28 Train loss: 2.747038024608628e-06 at step: 79600 lr 0.0008
2025_03_03_03_19_42 Train loss: 0.07074122875928879 at step: 80000 lr 0.0008
2025_03_03_03_21_55 Train loss: 0.00010848086822079495 at step: 80400 lr 0.0008
2025_03_03_03_24_08 Train loss: 8.679543680045754e-05 at step: 80800 lr 0.0008
(Val @ epoch 17) acc: 0.999375; ap: 1.0
2025_03_03_03_26_29 Train loss: 0.00010487040708540007 at step: 81200 lr 0.0008
2025_03_03_03_28_42 Train loss: 2.343754431421985e-06 at step: 81600 lr 0.0008
2025_03_03_03_30_55 Train loss: 0.001215670839883387 at step: 82000 lr 0.0008
2025_03_03_03_33_09 Train loss: 5.756417209568099e-08 at step: 82400 lr 0.0008
2025_03_03_03_35_22 Train loss: 3.9576537602670214e-09 at step: 82800 lr 0.0008
2025_03_03_03_37_36 Train loss: 0.004870730452239513 at step: 83200 lr 0.0008
2025_03_03_03_39_49 Train loss: 4.292489757062867e-05 at step: 83600 lr 0.0008
2025_03_03_03_42_03 Train loss: 1.0438178605909343e-06 at step: 84000 lr 0.0008
2025_03_03_03_44_16 Train loss: 0.0028127834666520357 at step: 84400 lr 0.0008
2025_03_03_03_46_29 Train loss: 4.533910396276042e-05 at step: 84800 lr 0.0008
2025_03_03_03_48_43 Train loss: 1.5736070224647847e-07 at step: 85200 lr 0.0008
(Val @ epoch 18) acc: 1.0; ap: 1.0
2025_03_03_03_51_03 Train loss: 3.1504841899732128e-06 at step: 85600 lr 0.0008
2025_03_03_03_53_16 Train loss: 0.0001433308352716267 at step: 86000 lr 0.0008
2025_03_03_03_55_30 Train loss: 3.215880860807374e-05 at step: 86400 lr 0.0008
2025_03_03_03_57_43 Train loss: 9.011512247525388e-07 at step: 86800 lr 0.0008
2025_03_03_03_59_57 Train loss: 3.1296326596930157e-06 at step: 87200 lr 0.0008
2025_03_03_04_02_10 Train loss: 1.1926525189664972e-07 at step: 87600 lr 0.0008
2025_03_03_04_04_24 Train loss: 0.04200258105993271 at step: 88000 lr 0.0008
2025_03_03_04_06_37 Train loss: 1.252925585504272e-06 at step: 88400 lr 0.0008
2025_03_03_04_08_50 Train loss: 0.05501866340637207 at step: 88800 lr 0.0008
2025_03_03_04_11_04 Train loss: 9.543375199427828e-06 at step: 89200 lr 0.0008
2025_03_03_04_13_17 Train loss: 7.098918285919353e-05 at step: 89600 lr 0.0008
2025_03_03_04_15_31 Train loss: 9.645965803883882e-08 at step: 90000 lr 0.0008
(Val @ epoch 19) acc: 0.99875; ap: 0.9999984394506867
2025_03_03_04_17_51 Train loss: 4.203853222861653e-06 at step: 90400 lr 0.0008
2025_03_03_04_20_04 Train loss: 7.157565846682701e-07 at step: 90800 lr 0.0008
2025_03_03_04_22_17 Train loss: 2.682262163489213e-07 at step: 91200 lr 0.0008
2025_03_03_04_24_31 Train loss: 0.005135504994541407 at step: 91600 lr 0.0008
2025_03_03_04_26_44 Train loss: 0.0006891371449455619 at step: 92000 lr 0.0008
2025_03_03_04_28_58 Train loss: 0.0005494694341905415 at step: 92400 lr 0.0008
2025_03_03_04_31_11 Train loss: 1.1884130799444392e-05 at step: 92800 lr 0.0008
2025_03_03_04_33_25 Train loss: 6.124551418906776e-06 at step: 93200 lr 0.0008
2025_03_03_04_35_38 Train loss: 0.0033659639302641153 at step: 93600 lr 0.0008
2025_03_03_04_37_51 Train loss: 7.861107587814331e-05 at step: 94000 lr 0.0008
2025_03_03_04_40_05 Train loss: 8.77480943017872e-06 at step: 94400 lr 0.0008
2025_03_03_04_40_45 changing lr at the end of epoch 20, iters 94521
*************************
Changing lr from 0.0008 to 0.00064
*************************
(Val @ epoch 20) acc: 1.0; ap: 1.0
2025_03_03_04_42_25 Train loss: 2.516374886241124e-10 at step: 94800 lr 0.00064
2025_03_03_04_44_39 Train loss: 3.002663106599357e-05 at step: 95200 lr 0.00064
2025_03_03_04_46_52 Train loss: 2.5389463189640082e-06 at step: 95600 lr 0.00064
2025_03_03_04_49_05 Train loss: 0.007700644433498383 at step: 96000 lr 0.00064
2025_03_03_04_51_19 Train loss: 3.0991495236776245e-07 at step: 96400 lr 0.00064
2025_03_03_04_53_32 Train loss: 1.059860437635507e-06 at step: 96800 lr 0.00064
2025_03_03_04_55_46 Train loss: 5.960487925449343e-08 at step: 97200 lr 0.00064
2025_03_03_04_57_59 Train loss: 9.937429013007204e-08 at step: 97600 lr 0.00064
2025_03_03_05_00_13 Train loss: 0.00010711247159633785 at step: 98000 lr 0.00064
2025_03_03_05_02_26 Train loss: 1.638108074075717e-06 at step: 98400 lr 0.00064
2025_03_03_05_04_39 Train loss: 3.1419149504330335e-09 at step: 98800 lr 0.00064
(Val @ epoch 21) acc: 1.0; ap: 1.0
2025_03_03_05_07_00 Train loss: 1.1163236194988713e-05 at step: 99200 lr 0.00064
2025_03_03_05_09_13 Train loss: 0.00048744678497314453 at step: 99600 lr 0.00064
2025_03_03_05_11_27 Train loss: 3.0130299819575157e-06 at step: 100000 lr 0.00064
2025_03_03_05_13_40 Train loss: 1.4687193470308557e-06 at step: 100400 lr 0.00064
2025_03_03_05_15_53 Train loss: 1.9803694158326834e-05 at step: 100800 lr 0.00064
2025_03_03_05_18_07 Train loss: 9.07604680833174e-06 at step: 101200 lr 0.00064
2025_03_03_05_20_20 Train loss: 0.0003332973865326494 at step: 101600 lr 0.00064
2025_03_03_05_22_34 Train loss: 4.118415745324455e-05 at step: 102000 lr 0.00064
2025_03_03_05_24_47 Train loss: 5.078114782008925e-07 at step: 102400 lr 0.00064
2025_03_03_05_27_01 Train loss: 7.170244771259604e-07 at step: 102800 lr 0.00064
2025_03_03_05_29_14 Train loss: 3.0069577405811287e-06 at step: 103200 lr 0.00064
(Val @ epoch 22) acc: 1.0; ap: 1.0
2025_03_03_05_31_34 Train loss: 1.1629456821538042e-05 at step: 103600 lr 0.00064
2025_03_03_05_33_47 Train loss: 7.323298234496178e-08 at step: 104000 lr 0.00064
2025_03_03_05_36_01 Train loss: 5.6611588661326095e-06 at step: 104400 lr 0.00064
2025_03_03_05_38_14 Train loss: 0.00013955042231827974 at step: 104800 lr 0.00064
2025_03_03_05_40_27 Train loss: 5.394660433921672e-07 at step: 105200 lr 0.00064
2025_03_03_05_42_41 Train loss: 2.706778357008943e-09 at step: 105600 lr 0.00064
2025_03_03_05_44_54 Train loss: 1.2840380350098712e-06 at step: 106000 lr 0.00064
2025_03_03_05_47_08 Train loss: 2.1434532015973673e-07 at step: 106400 lr 0.00064
2025_03_03_05_49_21 Train loss: 3.0425336916550805e-08 at step: 106800 lr 0.00064
2025_03_03_05_51_34 Train loss: 2.4437904357910156e-06 at step: 107200 lr 0.00064
2025_03_03_05_53_48 Train loss: 1.7881889391446748e-07 at step: 107600 lr 0.00064
2025_03_03_05_56_01 Train loss: 1.6056175127232564e-06 at step: 108000 lr 0.00064
(Val @ epoch 23) acc: 0.996875; ap: 0.9999366678843559
2025_03_03_05_58_21 Train loss: 9.508469702268485e-06 at step: 108400 lr 0.00064
2025_03_03_06_00_35 Train loss: 9.017533557198476e-06 at step: 108800 lr 0.00064
2025_03_03_06_02_48 Train loss: 0.00011540555715328082 at step: 109200 lr 0.00064
2025_03_03_06_05_01 Train loss: 4.055420868098736e-05 at step: 109600 lr 0.00064
2025_03_03_06_07_15 Train loss: 1.277547562494874e-05 at step: 110000 lr 0.00064
2025_03_03_06_09_28 Train loss: 3.730164280568715e-06 at step: 110400 lr 0.00064
2025_03_03_06_11_42 Train loss: 0.00015539237938355654 at step: 110800 lr 0.00064
2025_03_03_06_13_55 Train loss: 9.250846051145345e-05 at step: 111200 lr 0.00064
2025_03_03_06_16_08 Train loss: 8.896178769646212e-06 at step: 111600 lr 0.00064
2025_03_03_06_18_22 Train loss: 2.9585037282231497e-06 at step: 112000 lr 0.00064
2025_03_03_06_20_35 Train loss: 8.120253914967179e-05 at step: 112400 lr 0.00064
(Val @ epoch 24) acc: 1.0; ap: 1.0
2025_03_03_06_22_55 Train loss: 1.2188540843283135e-08 at step: 112800 lr 0.00064
2025_03_03_06_25_08 Train loss: 1.503671046521049e-05 at step: 113200 lr 0.00064
2025_03_03_06_27_22 Train loss: 1.182552568934625e-05 at step: 113600 lr 0.00064
2025_03_03_06_29_35 Train loss: 1.6512867659912445e-05 at step: 114000 lr 0.00064
2025_03_03_06_31_48 Train loss: 3.034579776794999e-06 at step: 114400 lr 0.00064
2025_03_03_06_34_02 Train loss: 3.916535661119269e-06 at step: 114800 lr 0.00064
2025_03_03_06_36_15 Train loss: 6.312053812962404e-08 at step: 115200 lr 0.00064
2025_03_03_06_38_28 Train loss: 0.003231060691177845 at step: 115600 lr 0.00064
2025_03_03_06_40_42 Train loss: 0.0010365857742726803 at step: 116000 lr 0.00064
2025_03_03_06_42_55 Train loss: 0.0003392994694877416 at step: 116400 lr 0.00064
2025_03_03_06_45_09 Train loss: 1.936156905912867e-07 at step: 116800 lr 0.00064
(Val @ epoch 25) acc: 0.999375; ap: 1.0
2025_03_03_06_47_29 Train loss: 1.529736073280219e-05 at step: 117200 lr 0.00064
2025_03_03_06_49_42 Train loss: 6.918583039805526e-08 at step: 117600 lr 0.00064
2025_03_03_06_51_55 Train loss: 6.353939596692726e-08 at step: 118000 lr 0.00064
2025_03_03_06_54_09 Train loss: 1.9375270312593784e-06 at step: 118400 lr 0.00064
2025_03_03_06_56_22 Train loss: 3.315182766527869e-06 at step: 118800 lr 0.00064
2025_03_03_06_58_35 Train loss: 3.395473413547734e-07 at step: 119200 lr 0.00064
2025_03_03_07_00_49 Train loss: 1.078516106645111e-06 at step: 119600 lr 0.00064
2025_03_03_07_03_02 Train loss: 5.5328036978608e-05 at step: 120000 lr 0.00064
2025_03_03_07_05_15 Train loss: 3.4489822553496197e-08 at step: 120400 lr 0.00064
2025_03_03_07_07_29 Train loss: 3.814736373897176e-06 at step: 120800 lr 0.00064
2025_03_03_07_09_42 Train loss: 2.778498264799367e-10 at step: 121200 lr 0.00064
(Val @ epoch 26) acc: 1.0; ap: 1.0
2025_03_03_07_12_02 Train loss: 8.456198884232435e-06 at step: 121600 lr 0.00064
2025_03_03_07_14_15 Train loss: 1.4865946695863386e-07 at step: 122000 lr 0.00064
2025_03_03_07_16_29 Train loss: 2.853927253454458e-05 at step: 122400 lr 0.00064
2025_03_03_07_18_42 Train loss: 0.00027818811940960586 at step: 122800 lr 0.00064
2025_03_03_07_20_55 Train loss: 3.311724867671728e-07 at step: 123200 lr 0.00064
2025_03_03_07_23_09 Train loss: 4.977207481715595e-06 at step: 123600 lr 0.00064
2025_03_03_07_25_22 Train loss: 5.5295569012514534e-08 at step: 124000 lr 0.00064
2025_03_03_07_27_35 Train loss: 4.103330502402969e-05 at step: 124400 lr 0.00064
2025_03_03_07_29_49 Train loss: 5.600576332653873e-05 at step: 124800 lr 0.00064
2025_03_03_07_32_02 Train loss: 2.624677108542528e-06 at step: 125200 lr 0.00064
2025_03_03_07_34_16 Train loss: 0.0004292447993066162 at step: 125600 lr 0.00064
2025_03_03_07_36_29 Train loss: 1.6987452227112954e-06 at step: 126000 lr 0.00064
(Val @ epoch 27) acc: 1.0; ap: 1.0
2025_03_03_07_38_49 Train loss: 4.685871317633428e-06 at step: 126400 lr 0.00064
2025_03_03_07_41_02 Train loss: 3.4241705293425184e-07 at step: 126800 lr 0.00064
2025_03_03_07_43_16 Train loss: 2.2244570629936788e-07 at step: 127200 lr 0.00064
2025_03_03_07_45_29 Train loss: 5.321952585291001e-07 at step: 127600 lr 0.00064
2025_03_03_07_47_42 Train loss: 8.172047273546923e-06 at step: 128000 lr 0.00064
2025_03_03_07_49_56 Train loss: 6.036363629391417e-05 at step: 128400 lr 0.00064
2025_03_03_07_52_09 Train loss: 0.00011685287608997896 at step: 128800 lr 0.00064
2025_03_03_07_54_22 Train loss: 1.813159542507492e-07 at step: 129200 lr 0.00064
2025_03_03_07_56_36 Train loss: 2.237771639101993e-07 at step: 129600 lr 0.00064
2025_03_03_07_58_49 Train loss: 2.9825685032847105e-08 at step: 130000 lr 0.00064
2025_03_03_08_01_02 Train loss: 1.7082214981201105e-05 at step: 130400 lr 0.00064
(Val @ epoch 28) acc: 1.0; ap: 1.0
2025_03_03_08_03_22 Train loss: 4.49706959670948e-07 at step: 130800 lr 0.00064
2025_03_03_08_05_36 Train loss: 1.006510075285405e-07 at step: 131200 lr 0.00064
2025_03_03_08_07_49 Train loss: 2.8917795134475455e-05 at step: 131600 lr 0.00064
2025_03_03_08_10_02 Train loss: 7.060853022267111e-06 at step: 132000 lr 0.00064
2025_03_03_08_12_16 Train loss: 1.0158917973512871e-07 at step: 132400 lr 0.00064
2025_03_03_08_14_29 Train loss: 6.770761729057995e-08 at step: 132800 lr 0.00064
2025_03_03_08_16_42 Train loss: 2.101586034086722e-07 at step: 133200 lr 0.00064
2025_03_03_08_18_56 Train loss: 1.957845370270661e-06 at step: 133600 lr 0.00064
2025_03_03_08_21_09 Train loss: 3.758730132696897e-11 at step: 134000 lr 0.00064
2025_03_03_08_23_23 Train loss: 3.763889253605157e-05 at step: 134400 lr 0.00064
2025_03_03_08_25_36 Train loss: 0.00030049681663513184 at step: 134800 lr 0.00064
(Val @ epoch 29) acc: 1.0; ap: 1.0
2025_03_03_08_27_56 Train loss: 4.233479194226675e-05 at step: 135200 lr 0.00064
2025_03_03_08_30_09 Train loss: 2.092196780267841e-07 at step: 135600 lr 0.00064
2025_03_03_08_32_23 Train loss: 1.0253576654406515e-08 at step: 136000 lr 0.00064
2025_03_03_08_34_36 Train loss: 5.896399670746177e-07 at step: 136400 lr 0.00064
2025_03_03_08_36_50 Train loss: 0.0005654973792843521 at step: 136800 lr 0.00064
2025_03_03_08_39_03 Train loss: 1.7881403380215488e-07 at step: 137200 lr 0.00064
2025_03_03_08_41_16 Train loss: 0.04228593781590462 at step: 137600 lr 0.00064
2025_03_03_08_43_30 Train loss: 2.09070935852651e-06 at step: 138000 lr 0.00064
2025_03_03_08_45_43 Train loss: 9.988960769646837e-09 at step: 138400 lr 0.00064
2025_03_03_08_47_56 Train loss: 0.00013602078252006322 at step: 138800 lr 0.00064
2025_03_03_08_50_10 Train loss: 8.482305133838963e-07 at step: 139200 lr 0.00064
2025_03_03_08_52_00 changing lr at the end of epoch 30, iters 139531
*************************
Changing lr from 0.00064 to 0.0005120000000000001
*************************
(Val @ epoch 30) acc: 1.0; ap: 1.0
2025_03_03_08_52_30 Train loss: 3.7448974126164103e-06 at step: 139600 lr 0.0005120000000000001
2025_03_03_08_54_43 Train loss: 9.869577866084e-08 at step: 140000 lr 0.0005120000000000001
2025_03_03_08_56_56 Train loss: 2.5861020276352065e-06 at step: 140400 lr 0.0005120000000000001
2025_03_03_08_59_10 Train loss: 1.49140078065102e-07 at step: 140800 lr 0.0005120000000000001
2025_03_03_09_01_23 Train loss: 3.315060212116805e-06 at step: 141200 lr 0.0005120000000000001
2025_03_03_09_03_36 Train loss: 4.4703892854158767e-07 at step: 141600 lr 0.0005120000000000001
2025_03_03_09_05_50 Train loss: 5.664613809130969e-07 at step: 142000 lr 0.0005120000000000001
2025_03_03_09_08_03 Train loss: 1.7886272019040916e-07 at step: 142400 lr 0.0005120000000000001
2025_03_03_09_10_16 Train loss: 3.6371238820720464e-07 at step: 142800 lr 0.0005120000000000001
2025_03_03_09_12_30 Train loss: 7.0733944994572084e-06 at step: 143200 lr 0.0005120000000000001
2025_03_03_09_14_43 Train loss: 1.7128864516005593e-12 at step: 143600 lr 0.0005120000000000001
2025_03_03_09_16_56 Train loss: 3.0210303521016613e-05 at step: 144000 lr 0.0005120000000000001
(Val @ epoch 31) acc: 1.0; ap: 1.0
2025_03_03_09_19_17 Train loss: 5.4222377912083175e-06 at step: 144400 lr 0.0005120000000000001
2025_03_03_09_21_30 Train loss: 1.4306325510915485e-06 at step: 144800 lr 0.0005120000000000001
2025_03_03_09_23_43 Train loss: 1.7567811028129654e-06 at step: 145200 lr 0.0005120000000000001
2025_03_03_09_25_57 Train loss: 5.502476483343344e-07 at step: 145600 lr 0.0005120000000000001
2025_03_03_09_28_10 Train loss: 1.8597658709040843e-05 at step: 146000 lr 0.0005120000000000001
2025_03_03_09_30_23 Train loss: 2.7423063784226542e-06 at step: 146400 lr 0.0005120000000000001
2025_03_03_09_32_37 Train loss: 3.534433190566233e-08 at step: 146800 lr 0.0005120000000000001
2025_03_03_09_34_50 Train loss: 5.233380306890467e-06 at step: 147200 lr 0.0005120000000000001
2025_03_03_09_37_04 Train loss: 7.748603820800781e-07 at step: 147600 lr 0.0005120000000000001
2025_03_03_09_39_17 Train loss: 4.01203942601569e-06 at step: 148000 lr 0.0005120000000000001
2025_03_03_09_41_30 Train loss: 2.9866857431670724e-08 at step: 148400 lr 0.0005120000000000001
(Val @ epoch 32) acc: 1.0; ap: 1.0
2025_03_03_09_43_50 Train loss: 9.780912478163373e-06 at step: 148800 lr 0.0005120000000000001
2025_03_03_09_46_04 Train loss: 9.079570872927434e-08 at step: 149200 lr 0.0005120000000000001
2025_03_03_09_48_17 Train loss: 4.084281954419566e-06 at step: 149600 lr 0.0005120000000000001
2025_03_03_09_50_31 Train loss: 2.9836048298648166e-08 at step: 150000 lr 0.0005120000000000001
2025_03_03_09_52_44 Train loss: 2.44379270952777e-06 at step: 150400 lr 0.0005120000000000001
2025_03_03_09_54_57 Train loss: 6.556579137395602e-07 at step: 150800 lr 0.0005120000000000001
2025_03_03_09_57_11 Train loss: 1.4906348155818705e-07 at step: 151200 lr 0.0005120000000000001
2025_03_03_09_59_24 Train loss: 3.572158675524406e-06 at step: 151600 lr 0.0005120000000000001
2025_03_03_10_01_37 Train loss: 1.2874651474703569e-05 at step: 152000 lr 0.0005120000000000001
2025_03_03_10_03_51 Train loss: 2.896904504473552e-12 at step: 152400 lr 0.0005120000000000001
2025_03_03_10_06_04 Train loss: 5.171590373720392e-07 at step: 152800 lr 0.0005120000000000001
(Val @ epoch 33) acc: 1.0; ap: 1.0
2025_03_03_10_08_24 Train loss: 2.6917655604474788e-11 at step: 153200 lr 0.0005120000000000001
2025_03_03_10_10_38 Train loss: 1.2239181046425074e-07 at step: 153600 lr 0.0005120000000000001
2025_03_03_10_12_51 Train loss: 1.2641401099244831e-06 at step: 154000 lr 0.0005120000000000001
2025_03_03_10_15_04 Train loss: 3.7849404179723933e-06 at step: 154400 lr 0.0005120000000000001
2025_03_03_10_17_18 Train loss: 1.4782028301851824e-05 at step: 154800 lr 0.0005120000000000001
2025_03_03_10_19_31 Train loss: 1.0431816690470441e-06 at step: 155200 lr 0.0005120000000000001
2025_03_03_10_21_45 Train loss: 3.271180730735068e-06 at step: 155600 lr 0.0005120000000000001
2025_03_03_10_23_58 Train loss: 7.09283867195154e-08 at step: 156000 lr 0.0005120000000000001
2025_03_03_10_26_12 Train loss: 2.6910722226602957e-05 at step: 156400 lr 0.0005120000000000001
2025_03_03_10_28_25 Train loss: 8.254412023234181e-06 at step: 156800 lr 0.0005120000000000001
2025_03_03_10_30_38 Train loss: 6.00741714151809e-08 at step: 157200 lr 0.0005120000000000001
(Val @ epoch 34) acc: 1.0; ap: 1.0
2025_03_03_10_32_59 Train loss: 0.006332028657197952 at step: 157600 lr 0.0005120000000000001
2025_03_03_10_35_12 Train loss: 1.3755490044786711e-06 at step: 158000 lr 0.0005120000000000001
2025_03_03_10_37_25 Train loss: 3.164554527756991e-06 at step: 158400 lr 0.0005120000000000001
2025_03_03_10_39_39 Train loss: 4.5867494691265165e-08 at step: 158800 lr 0.0005120000000000001
2025_03_03_10_41_52 Train loss: 1.109297764756434e-09 at step: 159200 lr 0.0005120000000000001
2025_03_03_10_44_06 Train loss: 8.674944096398463e-10 at step: 159600 lr 0.0005120000000000001
2025_03_03_10_46_19 Train loss: 1.2194813052701647e-06 at step: 160000 lr 0.0005120000000000001
2025_03_03_10_48_59 Train loss: 5.670352152264968e-07 at step: 160400 lr 0.0005120000000000001
2025_03_03_10_51_59 Train loss: 2.280658301101468e-12 at step: 160800 lr 0.0005120000000000001
2025_03_03_10_54_59 Train loss: 1.192093463942001e-07 at step: 161200 lr 0.0005120000000000001
2025_03_03_10_57_57 Train loss: 3.4380975648673484e-06 at step: 161600 lr 0.0005120000000000001
2025_03_03_11_00_51 Train loss: 5.324614349433432e-08 at step: 162000 lr 0.0005120000000000001
(Val @ epoch 35) acc: 1.0; ap: 1.0
2025_03_03_11_03_57 Train loss: 0.0013864211505278945 at step: 162400 lr 0.0005120000000000001
2025_03_03_11_06_51 Train loss: 5.896269499316986e-07 at step: 162800 lr 0.0005120000000000001
2025_03_03_11_09_49 Train loss: 2.985456859505575e-08 at step: 163200 lr 0.0005120000000000001
2025_03_03_11_12_47 Train loss: 2.9055689765300485e-07 at step: 163600 lr 0.0005120000000000001
2025_03_03_11_15_47 Train loss: 1.4134846182400906e-08 at step: 164000 lr 0.0005120000000000001
2025_03_03_11_18_43 Train loss: 2.9540036848629825e-05 at step: 164400 lr 0.0005120000000000001
2025_03_03_11_21_36 Train loss: 1.932316973096704e-09 at step: 164800 lr 0.0005120000000000001
2025_03_03_11_24_33 Train loss: 4.5242859414429404e-07 at step: 165200 lr 0.0005120000000000001
2025_03_03_11_27_30 Train loss: 1.1901738483643953e-09 at step: 165600 lr 0.0005120000000000001
2025_03_03_11_30_25 Train loss: 2.980851832035114e-08 at step: 166000 lr 0.0005120000000000001
2025_03_03_11_33_21 Train loss: 5.732696990889963e-06 at step: 166400 lr 0.0005120000000000001
(Val @ epoch 36) acc: 1.0; ap: 1.0
2025_03_03_11_36_25 Train loss: 5.5372976930812e-05 at step: 166800 lr 0.0005120000000000001
2025_03_03_11_39_19 Train loss: 1.1950971384067088e-05 at step: 167200 lr 0.0005120000000000001
2025_03_03_11_42_16 Train loss: 5.136771719094213e-08 at step: 167600 lr 0.0005120000000000001
2025_03_03_11_45_14 Train loss: 3.9012248986836084e-10 at step: 168000 lr 0.0005120000000000001
2025_03_03_11_48_10 Train loss: 1.0720825116550259e-07 at step: 168400 lr 0.0005120000000000001
2025_03_03_11_51_06 Train loss: 2.5890301458275644e-07 at step: 168800 lr 0.0005120000000000001
2025_03_03_11_54_00 Train loss: 3.5167744499631226e-05 at step: 169200 lr 0.0005120000000000001
2025_03_03_11_56_53 Train loss: 2.096148591590463e-06 at step: 169600 lr 0.0005120000000000001
2025_03_03_11_59_49 Train loss: 1.4448878573602997e-06 at step: 170000 lr 0.0005120000000000001
2025_03_03_12_02_46 Train loss: 2.862580572582374e-07 at step: 170400 lr 0.0005120000000000001
2025_03_03_12_05_42 Train loss: 2.0948456835867546e-07 at step: 170800 lr 0.0005120000000000001
(Val @ epoch 37) acc: 1.0; ap: 1.0
2025_03_03_12_08_42 Train loss: 2.980232238769531e-07 at step: 171200 lr 0.0005120000000000001
2025_03_03_12_11_28 Train loss: 6.550234684254974e-05 at step: 171600 lr 0.0005120000000000001
2025_03_03_12_14_24 Train loss: 2.0864570160483709e-07 at step: 172000 lr 0.0005120000000000001
2025_03_03_12_17_20 Train loss: 8.6254082987125e-08 at step: 172400 lr 0.0005120000000000001
2025_03_03_12_20_14 Train loss: 6.039435618276912e-08 at step: 172800 lr 0.0005120000000000001
2025_03_03_12_23_12 Train loss: 5.972057692815724e-08 at step: 173200 lr 0.0005120000000000001
2025_03_03_12_26_08 Train loss: 6.497482900158502e-06 at step: 173600 lr 0.0005120000000000001
2025_03_03_12_28_57 Train loss: 1.1120620868609876e-08 at step: 174000 lr 0.0005120000000000001
2025_03_03_12_32_00 Train loss: 5.587935447692871e-05 at step: 174400 lr 0.0005120000000000001
2025_03_03_12_34_57 Train loss: 0.00011673188419081271 at step: 174800 lr 0.0005120000000000001
2025_03_03_12_37_52 Train loss: 6.446268230320129e-07 at step: 175200 lr 0.0005120000000000001
(Val @ epoch 38) acc: 1.0; ap: 1.0
2025_03_03_12_40_55 Train loss: 8.940696716308594e-08 at step: 175600 lr 0.0005120000000000001
2025_03_03_12_43_51 Train loss: 5.751564913225593e-07 at step: 176000 lr 0.0005120000000000001
2025_03_03_12_46_21 Train loss: 0.02680690959095955 at step: 176400 lr 0.0005120000000000001
2025_03_03_12_48_35 Train loss: 3.644965573812442e-08 at step: 176800 lr 0.0005120000000000001
2025_03_03_12_50_48 Train loss: 2.0608852402403954e-09 at step: 177200 lr 0.0005120000000000001
2025_03_03_12_53_02 Train loss: 2.168470382457599e-05 at step: 177600 lr 0.0005120000000000001
2025_03_03_12_55_15 Train loss: 1.9096830783382757e-06 at step: 178000 lr 0.0005120000000000001
2025_03_03_12_58_12 Train loss: 7.264500254677841e-07 at step: 178400 lr 0.0005120000000000001
2025_03_03_13_01_12 Train loss: 6.855883611933677e-07 at step: 178800 lr 0.0005120000000000001
2025_03_03_13_04_10 Train loss: 1.5473865244697294e-10 at step: 179200 lr 0.0005120000000000001
2025_03_03_13_07_03 Train loss: 3.5327645253602213e-09 at step: 179600 lr 0.0005120000000000001
2025_03_03_13_09_57 Train loss: 3.2417497095593717e-06 at step: 180000 lr 0.0005120000000000001
(Val @ epoch 39) acc: 1.0; ap: 1.0
2025_03_03_13_13_01 Train loss: 7.927487644110442e-08 at step: 180400 lr 0.0005120000000000001
2025_03_03_13_15_54 Train loss: 6.100896143834689e-08 at step: 180800 lr 0.0005120000000000001
2025_03_03_13_18_50 Train loss: 1.1329409232985199e-07 at step: 181200 lr 0.0005120000000000001
2025_03_03_13_21_47 Train loss: 1.3742886039835867e-05 at step: 181600 lr 0.0005120000000000001
2025_03_03_13_24_47 Train loss: 1.3482696912864214e-14 at step: 182000 lr 0.0005120000000000001
2025_03_03_13_27_18 Train loss: 3.2884641676389492e-09 at step: 182400 lr 0.0005120000000000001
2025_03_03_13_29_32 Train loss: 9.508646144240629e-06 at step: 182800 lr 0.0005120000000000001
2025_03_03_13_31_45 Train loss: 1.5211512049972953e-07 at step: 183200 lr 0.0005120000000000001
2025_03_03_13_33_59 Train loss: 8.871384693520667e-08 at step: 183600 lr 0.0005120000000000001
2025_03_03_13_36_12 Train loss: 3.8543927161072133e-08 at step: 184000 lr 0.0005120000000000001
2025_03_03_13_38_26 Train loss: 1.2769984722438288e-12 at step: 184400 lr 0.0005120000000000001
2025_03_03_13_39_13 changing lr at the end of epoch 40, iters 184541
*************************
Changing lr from 0.0005120000000000001 to 0.0004096000000000001
*************************
(Val @ epoch 40) acc: 1.0; ap: 1.0
2025_03_03_13_40_46 Train loss: 5.644675184157677e-05 at step: 184800 lr 0.0004096000000000001
2025_03_03_13_43_00 Train loss: 7.870596618886339e-09 at step: 185200 lr 0.0004096000000000001
2025_03_03_13_45_13 Train loss: 1.220107606059173e-07 at step: 185600 lr 0.0004096000000000001
2025_03_03_13_47_27 Train loss: 6.281624820303477e-08 at step: 186000 lr 0.0004096000000000001
2025_03_03_13_49_40 Train loss: 7.152605121518718e-07 at step: 186400 lr 0.0004096000000000001
2025_03_03_13_51_54 Train loss: 6.556534231094702e-07 at step: 186800 lr 0.0004096000000000001
2025_03_03_13_54_07 Train loss: 2.780556860670913e-05 at step: 187200 lr 0.0004096000000000001
2025_03_03_13_56_21 Train loss: 5.961430815659696e-08 at step: 187600 lr 0.0004096000000000001
2025_03_03_13_58_34 Train loss: 1.0686347195587587e-05 at step: 188000 lr 0.0004096000000000001
2025_03_03_14_00_48 Train loss: 1.6754086118453415e-07 at step: 188400 lr 0.0004096000000000001
2025_03_03_14_03_20 Train loss: 5.274171144264983e-06 at step: 188800 lr 0.0004096000000000001
(Val @ epoch 41) acc: 0.999375; ap: 1.0
2025_03_03_14_06_19 Train loss: 2.1662194740201812e-06 at step: 189200 lr 0.0004096000000000001
2025_03_03_14_09_12 Train loss: 1.6879379472811706e-05 at step: 189600 lr 0.0004096000000000001
2025_03_03_14_12_09 Train loss: 1.3888281500840094e-05 at step: 190000 lr 0.0004096000000000001
2025_03_03_14_15_04 Train loss: 3.132849712983443e-08 at step: 190400 lr 0.0004096000000000001
2025_03_03_14_17_59 Train loss: 5.961222626638119e-08 at step: 190800 lr 0.0004096000000000001
2025_03_03_14_20_55 Train loss: 2.3911815105748246e-07 at step: 191200 lr 0.0004096000000000001
2025_03_03_14_23_57 Train loss: 1.716165209941778e-09 at step: 191600 lr 0.0004096000000000001
2025_03_03_14_26_51 Train loss: 1.8959924830141972e-07 at step: 192000 lr 0.0004096000000000001
2025_03_03_14_29_53 Train loss: 2.4602859411970712e-06 at step: 192400 lr 0.0004096000000000001
2025_03_03_14_32_47 Train loss: 5.174194406265542e-10 at step: 192800 lr 0.0004096000000000001
2025_03_03_14_35_46 Train loss: 2.384185791015625e-07 at step: 193200 lr 0.0004096000000000001
(Val @ epoch 42) acc: 1.0; ap: 1.0
2025_03_03_14_38_53 Train loss: 1.1637914099082991e-07 at step: 193600 lr 0.0004096000000000001
2025_03_03_14_41_50 Train loss: 3.02314084876798e-09 at step: 194000 lr 0.0004096000000000001
2025_03_03_14_45_35 Train loss: 3.4353246292084805e-07 at step: 194400 lr 0.0004096000000000001
2025_03_03_14_49_26 Train loss: 2.682211572846427e-07 at step: 194800 lr 0.0004096000000000001
2025_03_03_14_53_16 Train loss: 1.90098978225528e-11 at step: 195200 lr 0.0004096000000000001
2025_03_03_14_56_34 Train loss: 6.073092606584396e-08 at step: 195600 lr 0.0004096000000000001
2025_03_03_15_00_13 Train loss: 6.647903205703187e-07 at step: 196000 lr 0.0004096000000000001
2025_03_03_15_03_14 Train loss: 1.547133671175871e-10 at step: 196400 lr 0.0004096000000000001
2025_03_03_15_06_13 Train loss: 2.54065879445875e-10 at step: 196800 lr 0.0004096000000000001
2025_03_03_15_08_27 Train loss: 5.173613772058161e-06 at step: 197200 lr 0.0004096000000000001
2025_03_03_15_11_10 Train loss: 6.823291376356622e-10 at step: 197600 lr 0.0004096000000000001
2025_03_03_15_14_58 Train loss: 6.066044022645656e-08 at step: 198000 lr 0.0004096000000000001
(Val @ epoch 43) acc: 0.999375; ap: 1.0
2025_03_03_15_19_03 Train loss: 1.1324892739139614e-06 at step: 198400 lr 0.0004096000000000001
2025_03_03_15_22_37 Train loss: 0.0002266052324557677 at step: 198800 lr 0.0004096000000000001
2025_03_03_15_25_51 Train loss: 3.8543602337570704e-11 at step: 199200 lr 0.0004096000000000001
2025_03_03_15_29_41 Train loss: 9.005781720361483e-08 at step: 199600 lr 0.0004096000000000001
2025_03_03_15_33_30 Train loss: 1.6043757988803975e-10 at step: 200000 lr 0.0004096000000000001
2025_03_03_15_37_10 Train loss: 1.6093255226223846e-06 at step: 200400 lr 0.0004096000000000001
2025_03_03_15_40_30 Train loss: 2.374976881824864e-09 at step: 200800 lr 0.0004096000000000001
2025_03_03_15_44_20 Train loss: 8.475495860693627e-09 at step: 201200 lr 0.0004096000000000001
2025_03_03_15_48_04 Train loss: 1.2537341165241855e-09 at step: 201600 lr 0.0004096000000000001
2025_03_03_15_51_45 Train loss: 1.4342212750761973e-08 at step: 202000 lr 0.0004096000000000001
2025_03_03_15_55_05 Train loss: 1.9993305500065617e-07 at step: 202400 lr 0.0004096000000000001
(Val @ epoch 44) acc: 1.0; ap: 1.0
2025_03_03_15_59_03 Train loss: 1.0904408452583336e-11 at step: 202800 lr 0.0004096000000000001
2025_03_03_16_02_51 Train loss: 0.0001850604166975245 at step: 203200 lr 0.0004096000000000001
2025_03_03_16_06_27 Train loss: 2.0260978672581587e-08 at step: 203600 lr 0.0004096000000000001
2025_03_03_16_09_55 Train loss: 1.745261073438087e-07 at step: 204000 lr 0.0004096000000000001
2025_03_03_16_13_44 Train loss: 8.046738457778702e-07 at step: 204400 lr 0.0004096000000000001
2025_03_03_16_17_33 Train loss: 4.515267326699046e-11 at step: 204800 lr 0.0004096000000000001
2025_03_03_16_20_58 Train loss: 3.658692321550916e-06 at step: 205200 lr 0.0004096000000000001
2025_03_03_16_24_03 Train loss: 8.723947952926636e-11 at step: 205600 lr 0.0004096000000000001
2025_03_03_16_27_49 Train loss: 3.040127793951797e-08 at step: 206000 lr 0.0004096000000000001
2025_03_03_16_31_38 Train loss: 2.5765434656932484e-06 at step: 206400 lr 0.0004096000000000001
2025_03_03_16_35_26 Train loss: 3.1025026601128047e-06 at step: 206800 lr 0.0004096000000000001
(Val @ epoch 45) acc: 1.0; ap: 1.0
2025_03_03_16_38_28 Train loss: 2.980232594040899e-08 at step: 207200 lr 0.0004096000000000001
2025_03_03_16_41_38 Train loss: 2.74182457360439e-06 at step: 207600 lr 0.0004096000000000001
2025_03_03_16_44_42 Train loss: 1.7397157325849122e-10 at step: 208000 lr 0.0004096000000000001
2025_03_03_16_47_54 Train loss: 1.806228056011605e-07 at step: 208400 lr 0.0004096000000000001
2025_03_03_16_51_20 Train loss: 5.415482519310899e-05 at step: 208800 lr 0.0004096000000000001
2025_03_03_16_54_31 Train loss: 2.980233304583635e-08 at step: 209200 lr 0.0004096000000000001
2025_03_03_16_58_01 Train loss: 4.484757312184229e-07 at step: 209600 lr 0.0004096000000000001
2025_03_03_17_01_09 Train loss: 7.22395643304452e-10 at step: 210000 lr 0.0004096000000000001
2025_03_03_17_04_40 Train loss: 3.117227656557198e-11 at step: 210400 lr 0.0004096000000000001
2025_03_03_17_07_47 Train loss: 0.028359392657876015 at step: 210800 lr 0.0004096000000000001
2025_03_03_17_11_03 Train loss: 6.383386874642127e-11 at step: 211200 lr 0.0004096000000000001
(Val @ epoch 46) acc: 1.0; ap: 1.0
2025_03_03_17_14_29 Train loss: 1.7090854953494272e-08 at step: 211600 lr 0.0004096000000000001
2025_03_03_17_17_27 Train loss: 3.126414682697032e-08 at step: 212000 lr 0.0004096000000000001
2025_03_03_17_20_26 Train loss: 1.2132400684095046e-07 at step: 212400 lr 0.0004096000000000001
2025_03_03_17_23_21 Train loss: 1.0585983146657856e-14 at step: 212800 lr 0.0004096000000000001
2025_03_03_17_26_19 Train loss: 3.098081791108598e-08 at step: 213200 lr 0.0004096000000000001
2025_03_03_17_29_16 Train loss: 2.065300941467285e-05 at step: 213600 lr 0.0004096000000000001
2025_03_03_17_32_16 Train loss: 3.499773129078676e-07 at step: 214000 lr 0.0004096000000000001
2025_03_03_17_35_15 Train loss: 3.7810416415595682e-06 at step: 214400 lr 0.0004096000000000001
2025_03_03_17_38_12 Train loss: 2.5041643758605403e-10 at step: 214800 lr 0.0004096000000000001
2025_03_03_17_41_13 Train loss: 5.987184437117321e-08 at step: 215200 lr 0.0004096000000000001
2025_03_03_17_44_11 Train loss: 3.5004390497306304e-08 at step: 215600 lr 0.0004096000000000001
2025_03_03_17_47_09 Train loss: 0.0001252741931239143 at step: 216000 lr 0.0004096000000000001
(Val @ epoch 47) acc: 1.0; ap: 1.0
2025_03_03_17_50_12 Train loss: 1.4908452783402026e-07 at step: 216400 lr 0.0004096000000000001
2025_03_03_17_53_11 Train loss: 8.148038546096359e-07 at step: 216800 lr 0.0004096000000000001
2025_03_03_17_56_03 Train loss: 2.861088373596471e-13 at step: 217200 lr 0.0004096000000000001
2025_03_03_17_59_06 Train loss: 1.2089088216065053e-15 at step: 217600 lr 0.0004096000000000001
2025_03_03_18_02_06 Train loss: 4.0243416238316954e-13 at step: 218000 lr 0.0004096000000000001
2025_03_03_18_22_10 Train loss: 4.738095292088247e-14 at step: 218400 lr 0.0004096000000000001
2025_03_03_18_25_16 Train loss: 7.638421084266156e-05 at step: 218800 lr 0.0004096000000000001
2025_03_03_18_27_59 Train loss: 5.233914271229878e-05 at step: 219200 lr 0.0004096000000000001
2025_03_03_18_30_15 Train loss: 3.292404404664495e-10 at step: 219600 lr 0.0004096000000000001
2025_03_03_18_32_56 Train loss: 0.0003736720827873796 at step: 220000 lr 0.0004096000000000001
2025_03_03_18_35_13 Train loss: 4.5898405005573295e-06 at step: 220400 lr 0.0004096000000000001
(Val @ epoch 48) acc: 1.0; ap: 1.0
2025_03_03_18_38_08 Train loss: 1.0435368267280865e-06 at step: 220800 lr 0.0004096000000000001
2025_03_03_18_41_12 Train loss: 9.995924088457286e-09 at step: 221200 lr 0.0004096000000000001
2025_03_03_18_44_19 Train loss: 0.00015263259410858154 at step: 221600 lr 0.0004096000000000001
2025_03_03_18_47_17 Train loss: 1.3546470889469475e-10 at step: 222000 lr 0.0004096000000000001
2025_03_03_18_50_22 Train loss: 6.490354280686006e-06 at step: 222400 lr 0.0004096000000000001
2025_03_03_18_53_00 Train loss: 7.699382104098262e-11 at step: 222800 lr 0.0004096000000000001
2025_03_03_18_55_17 Train loss: 4.046521340228537e-08 at step: 223200 lr 0.0004096000000000001
2025_03_03_18_57_34 Train loss: 5.646927254954453e-16 at step: 223600 lr 0.0004096000000000001
2025_03_03_18_59_48 Train loss: 3.5878485959983664e-07 at step: 224000 lr 0.0004096000000000001
2025_03_03_19_02_05 Train loss: 4.400508013535642e-10 at step: 224400 lr 0.0004096000000000001
2025_03_03_19_04_22 Train loss: 3.712314367021463e-08 at step: 224800 lr 0.0004096000000000001
(Val @ epoch 49) acc: 1.0; ap: 1.0
2025_03_03_19_06_45 Train loss: 4.968608983316079e-20 at step: 225200 lr 0.0004096000000000001
2025_03_03_19_09_02 Train loss: 1.1146313227072824e-05 at step: 225600 lr 0.0004096000000000001
2025_03_03_19_11_16 Train loss: 6.277411750132966e-12 at step: 226000 lr 0.0004096000000000001
2025_03_03_19_13_33 Train loss: 4.173737977453129e-08 at step: 226400 lr 0.0004096000000000001
2025_03_03_19_15_50 Train loss: 2.0496511154988184e-09 at step: 226800 lr 0.0004096000000000001
2025_03_03_19_18_06 Train loss: 0.00028682511765509844 at step: 227200 lr 0.0004096000000000001
2025_03_03_19_20_23 Train loss: 4.198502434604734e-09 at step: 227600 lr 0.0004096000000000001
2025_03_03_19_22_37 Train loss: 2.0152258173311566e-07 at step: 228000 lr 0.0004096000000000001
2025_03_03_19_24_54 Train loss: 2.866244130927953e-07 at step: 228400 lr 0.0004096000000000001
2025_03_03_19_27_11 Train loss: 3.161971986326284e-12 at step: 228800 lr 0.0004096000000000001
2025_03_03_19_29_28 Train loss: 6.085020487489601e-08 at step: 229200 lr 0.0004096000000000001
2025_03_03_19_31_28 changing lr at the end of epoch 50, iters 229551
*************************
Changing lr from 0.00040960000000000014 to 0.0003276800000000001
*************************
(Val @ epoch 50) acc: 1.0; ap: 1.0
2025_03_03_19_31_51 Train loss: 3.5762786865234375e-07 at step: 229600 lr 0.0003276800000000001
2025_03_03_19_34_05 Train loss: 3.815390243744332e-07 at step: 230000 lr 0.0003276800000000001
2025_03_03_19_36_22 Train loss: 5.863279195494897e-09 at step: 230400 lr 0.0003276800000000001
2025_03_03_19_38_38 Train loss: 5.962604632259172e-08 at step: 230800 lr 0.0003276800000000001
2025_03_03_19_40_57 Train loss: 3.727600130787323e-07 at step: 231200 lr 0.0003276800000000001
2025_03_03_19_44_02 Train loss: 1.8291385073254673e-10 at step: 231600 lr 0.0003276800000000001
2025_03_03_19_47_03 Train loss: 2.980244673267407e-08 at step: 232000 lr 0.0003276800000000001
2025_03_03_19_50_05 Train loss: 1.206165052280994e-07 at step: 232400 lr 0.0003276800000000001
2025_03_03_19_53_10 Train loss: 1.0671870505352121e-11 at step: 232800 lr 0.0003276800000000001
2025_03_03_19_56_11 Train loss: 5.960477977851042e-08 at step: 233200 lr 0.0003276800000000001
2025_03_03_19_58_35 Train loss: 3.6788273816146244e-11 at step: 233600 lr 0.0003276800000000001
2025_03_03_20_00_52 Train loss: 4.1533294421659155e-17 at step: 234000 lr 0.0003276800000000001
(Val @ epoch 51) acc: 1.0; ap: 1.0
2025_03_03_20_03_15 Train loss: 1.4700884953811055e-10 at step: 234400 lr 0.0003276800000000001
2025_03_03_20_05_32 Train loss: 0.007054538931697607 at step: 234800 lr 0.0003276800000000001
2025_03_03_20_08_02 Train loss: 5.960528426385281e-08 at step: 235200 lr 0.0003276800000000001
2025_03_03_20_10_18 Train loss: 2.697337890822382e-07 at step: 235600 lr 0.0003276800000000001
2025_03_03_20_12_50 Train loss: 1.5598460023635852e-10 at step: 236000 lr 0.0003276800000000001
2025_03_03_20_15_21 Train loss: 1.8461989270690538e-07 at step: 236400 lr 0.0003276800000000001
2025_03_03_20_17_36 Train loss: 1.577567299015925e-09 at step: 236800 lr 0.0003276800000000001
2025_03_03_20_20_07 Train loss: 2.980232238769531e-07 at step: 237200 lr 0.0003276800000000001
2025_03_03_20_22_21 Train loss: 0.0014117881655693054 at step: 237600 lr 0.0003276800000000001
2025_03_03_20_24_53 Train loss: 2.7432361093815416e-07 at step: 238000 lr 0.0003276800000000001
2025_03_03_20_27_07 Train loss: 0.0002369135618209839 at step: 238400 lr 0.0003276800000000001
(Val @ epoch 52) acc: 1.0; ap: 1.0
2025_03_03_20_29_45 Train loss: 1.313988065625377e-14 at step: 238800 lr 0.0003276800000000001
2025_03_03_20_31_59 Train loss: 5.96046660916727e-08 at step: 239200 lr 0.0003276800000000001
2025_03_03_20_34_31 Train loss: 2.705253798751528e-10 at step: 239600 lr 0.0003276800000000001
2025_03_03_20_37_01 Train loss: 1.5795394574524835e-06 at step: 240000 lr 0.0003276800000000001
2025_03_03_20_39_17 Train loss: 2.9802322387695312e-08 at step: 240400 lr 0.0003276800000000001
2025_03_03_20_41_49 Train loss: 8.021984743145083e-10 at step: 240800 lr 0.0003276800000000001
2025_03_03_20_44_21 Train loss: 1.981539753614925e-06 at step: 241200 lr 0.0003276800000000001
2025_03_03_20_46_35 Train loss: 1.4901171141445957e-07 at step: 241600 lr 0.0003276800000000001
2025_03_03_20_49_07 Train loss: 2.8209533775754025e-13 at step: 242000 lr 0.0003276800000000001
2025_03_03_20_51_36 Train loss: 2.6520262963458663e-06 at step: 242400 lr 0.0003276800000000001
2025_03_03_20_53_53 Train loss: 9.616379914589857e-10 at step: 242800 lr 0.0003276800000000001
(Val @ epoch 53) acc: 1.0; ap: 1.0
2025_03_03_20_56_32 Train loss: 0.007924904115498066 at step: 243200 lr 0.0003276800000000001
2025_03_03_20_59_03 Train loss: 5.48268963029841e-06 at step: 243600 lr 0.0003276800000000001
2025_03_03_21_01_17 Train loss: 2.3619728395374295e-09 at step: 244000 lr 0.0003276800000000001
2025_03_03_21_03_30 Train loss: 2.4202407189477526e-07 at step: 244400 lr 0.0003276800000000001
2025_03_03_21_05_44 Train loss: 7.562670650308745e-14 at step: 244800 lr 0.0003276800000000001
2025_03_03_21_07_57 Train loss: 2.1549496909134547e-10 at step: 245200 lr 0.0003276800000000001
2025_03_03_21_10_11 Train loss: 0.0003739893436431885 at step: 245600 lr 0.0003276800000000001
2025_03_03_21_12_24 Train loss: 1.7610952030445298e-13 at step: 246000 lr 0.0003276800000000001
2025_03_03_21_14_38 Train loss: 2.7275117986391706e-07 at step: 246400 lr 0.0003276800000000001
2025_03_03_21_16_51 Train loss: 3.7940417030313256e-08 at step: 246800 lr 0.0003276800000000001
2025_03_03_21_19_05 Train loss: 1.1160172178753536e-12 at step: 247200 lr 0.0003276800000000001
(Val @ epoch 54) acc: 1.0; ap: 1.0
2025_03_03_21_21_25 Train loss: 1.1622905731201172e-06 at step: 247600 lr 0.0003276800000000001
2025_03_03_21_23_39 Train loss: 2.629419848087372e-13 at step: 248000 lr 0.0003276800000000001
2025_03_03_21_25_52 Train loss: 8.344651405423065e-07 at step: 248400 lr 0.0003276800000000001
2025_03_03_21_28_06 Train loss: 3.558197363418003e-07 at step: 248800 lr 0.0003276800000000001
2025_03_03_21_30_19 Train loss: 3.154519017962293e-13 at step: 249200 lr 0.0003276800000000001
2025_03_03_21_32_33 Train loss: 2.980239699468257e-08 at step: 249600 lr 0.0003276800000000001
2025_03_03_21_34_46 Train loss: 1.0660614796123959e-15 at step: 250000 lr 0.0003276800000000001
2025_03_03_21_37_00 Train loss: 7.118578708777427e-17 at step: 250400 lr 0.0003276800000000001
2025_03_03_21_39_13 Train loss: 2.2562296475570065e-09 at step: 250800 lr 0.0003276800000000001
2025_03_03_21_41_27 Train loss: 4.1657717314742826e-13 at step: 251200 lr 0.0003276800000000001
2025_03_03_21_43_40 Train loss: 2.9802425416392e-08 at step: 251600 lr 0.0003276800000000001
2025_03_03_21_45_54 Train loss: 6.069508344808128e-07 at step: 252000 lr 0.0003276800000000001
(Val @ epoch 55) acc: 1.0; ap: 1.0
2025_03_03_21_48_14 Train loss: 2.9802322387695312e-08 at step: 252400 lr 0.0003276800000000001
2025_03_03_21_50_28 Train loss: 3.978388747327699e-07 at step: 252800 lr 0.0003276800000000001
2025_03_03_21_52_41 Train loss: 2.980384294914984e-08 at step: 253200 lr 0.0003276800000000001
2025_03_03_21_54_55 Train loss: 1.6996255791656978e-10 at step: 253600 lr 0.0003276800000000001
2025_03_03_21_57_09 Train loss: 1.906905961802409e-14 at step: 254000 lr 0.0003276800000000001
2025_03_03_21_59_22 Train loss: 6.821752322139218e-05 at step: 254400 lr 0.0003276800000000001
2025_03_03_22_01_36 Train loss: 1.1257497156691443e-08 at step: 254800 lr 0.0003276800000000001
2025_03_03_22_04_01 Train loss: 5.610507891606886e-11 at step: 255200 lr 0.0003276800000000001
2025_03_03_22_07_11 Train loss: 2.0809225870221937e-11 at step: 255600 lr 0.0003276800000000001
2025_03_03_22_10_20 Train loss: 2.4501366647361467e-12 at step: 256000 lr 0.0003276800000000001
2025_03_03_22_13_22 Train loss: 7.253402045659385e-11 at step: 256400 lr 0.0003276800000000001
(Val @ epoch 56) acc: 1.0; ap: 1.0
2025_03_03_22_15_42 Train loss: 5.655068861104562e-12 at step: 256800 lr 0.0003276800000000001
2025_03_03_22_18_20 Train loss: 2.1069714169989068e-14 at step: 257200 lr 0.0003276800000000001
2025_03_03_22_21_29 Train loss: 1.7881407643471903e-07 at step: 257600 lr 0.0003276800000000001
2025_03_03_22_24_05 Train loss: 5.676927970806118e-17 at step: 258000 lr 0.0003276800000000001
2025_03_03_22_26_19 Train loss: 9.083031926593321e-08 at step: 258400 lr 0.0003276800000000001
2025_03_03_22_28_32 Train loss: 0.00012323899136390537 at step: 258800 lr 0.0003276800000000001
2025_03_03_22_30_45 Train loss: 4.0127648426313556e-10 at step: 259200 lr 0.0003276800000000001
2025_03_03_22_33_16 Train loss: 1.1266790451991415e-12 at step: 259600 lr 0.0003276800000000001
2025_03_03_22_36_25 Train loss: 5.621012336143316e-11 at step: 260000 lr 0.0003276800000000001
2025_03_03_22_39_32 Train loss: 3.598896181067346e-10 at step: 260400 lr 0.0003276800000000001
2025_03_03_22_42_38 Train loss: 8.888017058450437e-12 at step: 260800 lr 0.0003276800000000001
(Val @ epoch 57) acc: 1.0; ap: 1.0
2025_03_03_22_45_53 Train loss: 2.7236095601068655e-09 at step: 261200 lr 0.0003276800000000001
2025_03_03_22_48_59 Train loss: 2.4844156246150995e-11 at step: 261600 lr 0.0003276800000000001
2025_03_03_22_52_04 Train loss: 7.317222348868224e-12 at step: 262000 lr 0.0003276800000000001
2025_03_03_22_55_10 Train loss: 1.4229590812944726e-12 at step: 262400 lr 0.0003276800000000001
2025_03_03_22_58_17 Train loss: 7.321269979154721e-11 at step: 262800 lr 0.0003276800000000001
2025_03_03_23_01_24 Train loss: 2.1537845896091312e-05 at step: 263200 lr 0.0003276800000000001
2025_03_03_23_04_30 Train loss: 4.642443721114553e-11 at step: 263600 lr 0.0003276800000000001
2025_03_03_23_06_46 Train loss: 3.064471229663468e-06 at step: 264000 lr 0.0003276800000000001
2025_03_03_23_08_59 Train loss: 4.2787523524268067e-10 at step: 264400 lr 0.0003276800000000001
2025_03_03_23_11_13 Train loss: 1.1320041721774032e-06 at step: 264800 lr 0.0003276800000000001
2025_03_03_23_13_26 Train loss: 2.832833412291791e-10 at step: 265200 lr 0.0003276800000000001
(Val @ epoch 58) acc: 1.0; ap: 1.0
2025_03_03_23_15_47 Train loss: 4.056853413203498e-06 at step: 265600 lr 0.0003276800000000001
2025_03_03_23_18_00 Train loss: 1.6084081630582045e-14 at step: 266000 lr 0.0003276800000000001
2025_03_03_23_20_14 Train loss: 1.5482264992608408e-12 at step: 266400 lr 0.0003276800000000001
2025_03_03_23_22_27 Train loss: 3.041111112933592e-11 at step: 266800 lr 0.0003276800000000001
2025_03_03_23_24_41 Train loss: 1.9917847405786864e-16 at step: 267200 lr 0.0003276800000000001
2025_03_03_23_26_54 Train loss: 1.0782721346913604e-06 at step: 267600 lr 0.0003276800000000001
2025_03_03_23_29_08 Train loss: 5.091738330520457e-06 at step: 268000 lr 0.0003276800000000001
2025_03_03_23_31_22 Train loss: 9.80970416009086e-10 at step: 268400 lr 0.0003276800000000001
2025_03_03_23_33_35 Train loss: 2.240010087140032e-11 at step: 268800 lr 0.0003276800000000001
2025_03_03_23_35_49 Train loss: 1.152533277148926e-11 at step: 269200 lr 0.0003276800000000001
2025_03_03_23_38_02 Train loss: 4.3660824955438216e-11 at step: 269600 lr 0.0003276800000000001
2025_03_03_23_40_16 Train loss: 1.1920928955078125e-07 at step: 270000 lr 0.0003276800000000001
(Val @ epoch 59) acc: 1.0; ap: 1.0
2025_03_03_23_42_36 Train loss: 2.9802322387695312e-08 at step: 270400 lr 0.0003276800000000001
2025_03_03_23_44_49 Train loss: 5.059735047390976e-14 at step: 270800 lr 0.0003276800000000001
2025_03_03_23_47_03 Train loss: 2.734013833105564e-07 at step: 271200 lr 0.0003276800000000001
2025_03_03_23_49_17 Train loss: 4.467547967124119e-07 at step: 271600 lr 0.0003276800000000001
2025_03_03_23_51_30 Train loss: 2.2670468280416856e-10 at step: 272000 lr 0.0003276800000000001
2025_03_03_23_53_44 Train loss: 1.2536742477475826e-12 at step: 272400 lr 0.0003276800000000001
2025_03_03_23_55_57 Train loss: 2.9858310313102265e-07 at step: 272800 lr 0.0003276800000000001
2025_03_03_23_58_11 Train loss: 4.023377186967991e-06 at step: 273200 lr 0.0003276800000000001
2025_03_04_00_00_24 Train loss: 3.000290504928671e-08 at step: 273600 lr 0.0003276800000000001
2025_03_04_00_02_38 Train loss: 0.15844108164310455 at step: 274000 lr 0.0003276800000000001
2025_03_04_00_04_51 Train loss: 3.5866665193395875e-10 at step: 274400 lr 0.0003276800000000001
2025_03_04_00_05_45 changing lr at the end of epoch 60, iters 274561
*************************
Changing lr from 0.0003276800000000001 to 0.0002621440000000001
*************************
(Val @ epoch 60) acc: 1.0; ap: 1.0
2025_03_04_00_07_11 Train loss: 6.785910045437049e-06 at step: 274800 lr 0.0002621440000000001
2025_03_04_00_09_25 Train loss: 1.7852404277096667e-10 at step: 275200 lr 0.0002621440000000001
2025_03_04_00_11_38 Train loss: 3.653875069176138e-07 at step: 275600 lr 0.0002621440000000001
2025_03_04_00_13_52 Train loss: 3.6200931390339974e-06 at step: 276000 lr 0.0002621440000000001
2025_03_04_00_16_06 Train loss: 8.675614004971521e-09 at step: 276400 lr 0.0002621440000000001
2025_03_04_00_18_19 Train loss: 1.460322494040156e-07 at step: 276800 lr 0.0002621440000000001
2025_03_04_00_20_33 Train loss: 7.86421761045375e-11 at step: 277200 lr 0.0002621440000000001
2025_03_04_00_22_46 Train loss: 3.0413165319487234e-08 at step: 277600 lr 0.0002621440000000001
2025_03_04_00_25_00 Train loss: 5.961014437616541e-08 at step: 278000 lr 0.0002621440000000001
2025_03_04_00_27_13 Train loss: 7.604494117741112e-11 at step: 278400 lr 0.0002621440000000001
2025_03_04_00_29_27 Train loss: 1.1159574569319375e-05 at step: 278800 lr 0.0002621440000000001
(Val @ epoch 61) acc: 1.0; ap: 1.0
2025_03_04_00_31_47 Train loss: 1.762995860410399e-09 at step: 279200 lr 0.0002621440000000001
2025_03_04_00_34_01 Train loss: 4.7591913049371826e-11 at step: 279600 lr 0.0002621440000000001
2025_03_04_00_36_14 Train loss: 5.960467319710006e-08 at step: 280000 lr 0.0002621440000000001
2025_03_04_00_38_28 Train loss: 1.620202756680289e-11 at step: 280400 lr 0.0002621440000000001
2025_03_04_00_40_41 Train loss: 3.285553873411118e-07 at step: 280800 lr 0.0002621440000000001
2025_03_04_00_42_55 Train loss: 3.4165423823395713e-09 at step: 281200 lr 0.0002621440000000001
2025_03_04_00_45_08 Train loss: 1.1931858523439587e-07 at step: 281600 lr 0.0002621440000000001
2025_03_04_00_47_22 Train loss: 6.67572021484375e-06 at step: 282000 lr 0.0002621440000000001
2025_03_04_00_49_35 Train loss: 5.726676022277388e-07 at step: 282400 lr 0.0002621440000000001
2025_03_04_00_51_49 Train loss: 2.5094040179141075e-09 at step: 282800 lr 0.0002621440000000001
2025_03_04_00_54_02 Train loss: 2.9802343703977385e-08 at step: 283200 lr 0.0002621440000000001
(Val @ epoch 62) acc: 1.0; ap: 1.0
2025_03_04_00_56_23 Train loss: 2.280626425557597e-12 at step: 283600 lr 0.0002621440000000001
2025_03_04_00_58_36 Train loss: 1.790512627053431e-09 at step: 284000 lr 0.0002621440000000001
2025_03_04_01_00_50 Train loss: 4.0951975144309927e-10 at step: 284400 lr 0.0002621440000000001
2025_03_04_01_03_03 Train loss: 1.894885463116225e-05 at step: 284800 lr 0.0002621440000000001
2025_03_04_01_05_17 Train loss: 5.735860744704446e-11 at step: 285200 lr 0.0002621440000000001
2025_03_04_01_07_30 Train loss: 4.565445799187273e-09 at step: 285600 lr 0.0002621440000000001
2025_03_04_01_09_44 Train loss: 6.717140732526161e-10 at step: 286000 lr 0.0002621440000000001
2025_03_04_01_11_57 Train loss: 2.377390229124643e-10 at step: 286400 lr 0.0002621440000000001
2025_03_04_01_14_11 Train loss: 1.8558665715318057e-10 at step: 286800 lr 0.0002621440000000001
2025_03_04_01_16_24 Train loss: 9.501052167237845e-10 at step: 287200 lr 0.0002621440000000001
2025_03_04_01_18_38 Train loss: 1.4901235090292175e-07 at step: 287600 lr 0.0002621440000000001
2025_03_04_01_20_51 Train loss: 2.140722928783978e-12 at step: 288000 lr 0.0002621440000000001
(Val @ epoch 63) acc: 1.0; ap: 1.0
2025_03_04_01_23_12 Train loss: 6.123898697169006e-12 at step: 288400 lr 0.0002621440000000001
2025_03_04_01_25_25 Train loss: 1.835186580478876e-08 at step: 288800 lr 0.0002621440000000001
2025_03_04_01_27_39 Train loss: 1.6047787543271852e-09 at step: 289200 lr 0.0002621440000000001
2025_03_04_01_29_52 Train loss: 5.861201302082009e-09 at step: 289600 lr 0.0002621440000000001
2025_03_04_01_32_06 Train loss: 2.137019944684071e-12 at step: 290000 lr 0.0002621440000000001
2025_03_04_01_34_19 Train loss: 8.285473640512464e-09 at step: 290400 lr 0.0002621440000000001
2025_03_04_01_36_33 Train loss: 1.9728658173789881e-07 at step: 290800 lr 0.0002621440000000001
2025_03_04_01_38_46 Train loss: 0.002018500352278352 at step: 291200 lr 0.0002621440000000001
2025_03_04_01_41_00 Train loss: 1.5863514928743383e-12 at step: 291600 lr 0.0002621440000000001
2025_03_04_01_43_13 Train loss: 8.081486868594112e-11 at step: 292000 lr 0.0002621440000000001
2025_03_04_01_45_27 Train loss: 3.310729024263992e-09 at step: 292400 lr 0.0002621440000000001
(Val @ epoch 64) acc: 1.0; ap: 1.0
2025_03_04_01_47_47 Train loss: 3.552224114855562e-08 at step: 292800 lr 0.0002621440000000001
2025_03_04_01_50_01 Train loss: 5.969972249886268e-08 at step: 293200 lr 0.0002621440000000001
2025_03_04_01_52_14 Train loss: 1.6266120672625561e-09 at step: 293600 lr 0.0002621440000000001
2025_03_04_01_54_28 Train loss: 1.4980245776480272e-12 at step: 294000 lr 0.0002621440000000001
2025_03_04_01_56_41 Train loss: 2.9802322387695312e-08 at step: 294400 lr 0.0002621440000000001
2025_03_04_01_58_55 Train loss: 4.2407627631746436e-08 at step: 294800 lr 0.0002621440000000001
2025_03_04_02_01_08 Train loss: 9.66941806836985e-05 at step: 295200 lr 0.0002621440000000001
2025_03_04_02_03_22 Train loss: 3.864791542351753e-10 at step: 295600 lr 0.0002621440000000001
2025_03_04_02_05_35 Train loss: 2.7676618592417657e-13 at step: 296000 lr 0.0002621440000000001
2025_03_04_02_07_49 Train loss: 2.110154092476238e-13 at step: 296400 lr 0.0002621440000000001
2025_03_04_02_10_02 Train loss: 7.753409363431274e-07 at step: 296800 lr 0.0002621440000000001
(Val @ epoch 65) acc: 1.0; ap: 1.0
2025_03_04_02_12_23 Train loss: 1.6199827541640843e-06 at step: 297200 lr 0.0002621440000000001
2025_03_04_02_14_36 Train loss: 1.922596942760535e-10 at step: 297600 lr 0.0002621440000000001
2025_03_04_02_16_50 Train loss: 5.9162119647737654e-12 at step: 298000 lr 0.0002621440000000001
2025_03_04_02_19_03 Train loss: 8.065835499504459e-11 at step: 298400 lr 0.0002621440000000001
2025_03_04_02_21_17 Train loss: 0.0004235804080963135 at step: 298800 lr 0.0002621440000000001
2025_03_04_02_23_30 Train loss: 2.542299259999936e-09 at step: 299200 lr 0.0002621440000000001
2025_03_04_02_25_44 Train loss: 9.031760115339895e-17 at step: 299600 lr 0.0002621440000000001
2025_03_04_02_27_57 Train loss: 8.988636556264435e-15 at step: 300000 lr 0.0002621440000000001
2025_03_04_02_30_11 Train loss: 4.79916704521164e-14 at step: 300400 lr 0.0002621440000000001
2025_03_04_02_32_24 Train loss: 4.074581655150872e-13 at step: 300800 lr 0.0002621440000000001
2025_03_04_02_34_38 Train loss: 5.981927131415432e-08 at step: 301200 lr 0.0002621440000000001
(Val @ epoch 66) acc: 1.0; ap: 1.0
2025_03_04_02_36_58 Train loss: 1.3273401475166757e-12 at step: 301600 lr 0.0002621440000000001
2025_03_04_02_39_11 Train loss: 2.2984294467676136e-09 at step: 302000 lr 0.0002621440000000001
2025_03_04_02_41_25 Train loss: 1.7091784801706589e-12 at step: 302400 lr 0.0002621440000000001
2025_03_04_02_43_38 Train loss: 4.365168200049039e-15 at step: 302800 lr 0.0002621440000000001
2025_03_04_02_45_52 Train loss: 0.001217106357216835 at step: 303200 lr 0.0002621440000000001
2025_03_04_02_48_05 Train loss: 0.0002678334712982178 at step: 303600 lr 0.0002621440000000001
2025_03_04_02_50_19 Train loss: 8.628536253552088e-12 at step: 304000 lr 0.0002621440000000001
2025_03_04_02_52_32 Train loss: 9.192997107169663e-20 at step: 304400 lr 0.0002621440000000001
2025_03_04_02_54_45 Train loss: 7.932307696974228e-13 at step: 304800 lr 0.0002621440000000001
2025_03_04_02_56_59 Train loss: 8.976339810340406e-11 at step: 305200 lr 0.0002621440000000001
2025_03_04_02_59_12 Train loss: 1.402637121034056e-11 at step: 305600 lr 0.0002621440000000001
2025_03_04_03_01_26 Train loss: 4.9316717376513e-10 at step: 306000 lr 0.0002621440000000001
(Val @ epoch 67) acc: 1.0; ap: 1.0
2025_03_04_03_03_46 Train loss: 8.842632355510194e-12 at step: 306400 lr 0.0002621440000000001
2025_03_04_03_05_59 Train loss: 3.6618923171527484e-11 at step: 306800 lr 0.0002621440000000001
2025_03_04_03_08_13 Train loss: 1.0447969122891546e-13 at step: 307200 lr 0.0002621440000000001
2025_03_04_03_10_26 Train loss: 3.008586648566594e-16 at step: 307600 lr 0.0002621440000000001
2025_03_04_03_12_40 Train loss: 1.6778378686410633e-09 at step: 308000 lr 0.0002621440000000001
2025_03_04_03_14_53 Train loss: 1.28767100606407e-13 at step: 308400 lr 0.0002621440000000001
2025_03_04_03_17_07 Train loss: 5.345649478627479e-10 at step: 308800 lr 0.0002621440000000001
2025_03_04_03_19_20 Train loss: 4.1723268395799096e-07 at step: 309200 lr 0.0002621440000000001
2025_03_04_03_21_34 Train loss: 6.883720295305451e-15 at step: 309600 lr 0.0002621440000000001
2025_03_04_03_23_47 Train loss: 2.0432123874758457e-12 at step: 310000 lr 0.0002621440000000001
2025_03_04_03_26_00 Train loss: 1.1743804861907847e-05 at step: 310400 lr 0.0002621440000000001
(Val @ epoch 68) acc: 1.0; ap: 1.0
2025_03_04_03_28_21 Train loss: 1.2323138776082487e-07 at step: 310800 lr 0.0002621440000000001
2025_03_04_03_30_34 Train loss: 3.7936505492552897e-08 at step: 311200 lr 0.0002621440000000001
2025_03_04_03_32_48 Train loss: 3.572061808669591e-13 at step: 311600 lr 0.0002621440000000001
2025_03_04_03_35_01 Train loss: 9.620482985554443e-14 at step: 312000 lr 0.0002621440000000001
2025_03_04_03_37_14 Train loss: 1.261627469162363e-09 at step: 312400 lr 0.0002621440000000001
2025_03_04_03_39_28 Train loss: 8.136817872858148e-15 at step: 312800 lr 0.0002621440000000001
2025_03_04_03_41_41 Train loss: 2.2198526179674667e-14 at step: 313200 lr 0.0002621440000000001
2025_03_04_03_43_55 Train loss: 3.172859924013144e-13 at step: 313600 lr 0.0002621440000000001
2025_03_04_03_46_08 Train loss: 1.4540530238194016e-12 at step: 314000 lr 0.0002621440000000001
2025_03_04_03_48_22 Train loss: 3.887987987116759e-10 at step: 314400 lr 0.0002621440000000001
2025_03_04_03_50_35 Train loss: 3.96514973632911e-11 at step: 314800 lr 0.0002621440000000001
(Val @ epoch 69) acc: 1.0; ap: 1.0
2025_03_04_03_52_55 Train loss: 2.679044833198141e-11 at step: 315200 lr 0.0002621440000000001
2025_03_04_03_55_09 Train loss: 1.7259608184935077e-13 at step: 315600 lr 0.0002621440000000001
2025_03_04_03_57_22 Train loss: 1.0365153394187931e-19 at step: 316000 lr 0.0002621440000000001
2025_03_04_03_59_35 Train loss: 8.979387671164467e-17 at step: 316400 lr 0.0002621440000000001
2025_03_04_04_01_49 Train loss: 4.650505813774686e-10 at step: 316800 lr 0.0002621440000000001
2025_03_04_04_04_02 Train loss: 4.474928254013556e-14 at step: 317200 lr 0.0002621440000000001
2025_03_04_04_06_15 Train loss: 6.7331944464399385e-09 at step: 317600 lr 0.0002621440000000001
2025_03_04_04_08_29 Train loss: 2.4397547804255737e-07 at step: 318000 lr 0.0002621440000000001
2025_03_04_04_10_42 Train loss: 1.2838375695878312e-08 at step: 318400 lr 0.0002621440000000001
2025_03_04_04_12_56 Train loss: 1.2477472388638944e-09 at step: 318800 lr 0.0002621440000000001
2025_03_04_04_15_09 Train loss: 1.0671932088035518e-12 at step: 319200 lr 0.0002621440000000001
2025_03_04_04_17_13 changing lr at the end of epoch 70, iters 319571
*************************
Changing lr from 0.0002621440000000001 to 0.00020971520000000012
*************************
(Val @ epoch 70) acc: 1.0; ap: 1.0
2025_03_04_04_17_29 Train loss: 1.0392179916607347e-07 at step: 319600 lr 0.00020971520000000012
2025_03_04_04_19_43 Train loss: 1.5204777541188985e-12 at step: 320000 lr 0.00020971520000000012
2025_03_04_04_21_56 Train loss: 1.0341562895881682e-15 at step: 320400 lr 0.00020971520000000012
2025_03_04_04_24_10 Train loss: 6.147229925090869e-08 at step: 320800 lr 0.00020971520000000012
2025_03_04_04_26_23 Train loss: 1.5126441765822563e-11 at step: 321200 lr 0.00020971520000000012
2025_03_04_04_28_36 Train loss: 3.572802817486398e-11 at step: 321600 lr 0.00020971520000000012
2025_03_04_04_30_50 Train loss: 2.6206847022341995e-12 at step: 322000 lr 0.00020971520000000012
2025_03_04_04_33_03 Train loss: 3.445237650723243e-09 at step: 322400 lr 0.00020971520000000012
2025_03_04_04_35_17 Train loss: 2.2796931443275215e-12 at step: 322800 lr 0.00020971520000000012
2025_03_04_04_37_30 Train loss: 7.576434427481859e-16 at step: 323200 lr 0.00020971520000000012
2025_03_04_04_39_43 Train loss: 5.387168846866103e-14 at step: 323600 lr 0.00020971520000000012
2025_03_04_04_41_57 Train loss: 5.961446447599883e-08 at step: 324000 lr 0.00020971520000000012
(Val @ epoch 71) acc: 1.0; ap: 1.0
2025_03_04_04_44_17 Train loss: 9.546603958743294e-12 at step: 324400 lr 0.00020971520000000012
2025_03_04_04_46_30 Train loss: 1.7121998534697785e-14 at step: 324800 lr 0.00020971520000000012
2025_03_04_04_48_44 Train loss: 1.85955478717182e-14 at step: 325200 lr 0.00020971520000000012
2025_03_04_04_50_57 Train loss: 1.4425844963028567e-09 at step: 325600 lr 0.00020971520000000012
2025_03_04_04_53_11 Train loss: 6.980847011561764e-09 at step: 326000 lr 0.00020971520000000012
2025_03_04_04_55_24 Train loss: 2.9802322387695312e-08 at step: 326400 lr 0.00020971520000000012
2025_03_04_04_57_38 Train loss: 1.221941384882408e-13 at step: 326800 lr 0.00020971520000000012
2025_03_04_04_59_51 Train loss: 1.730490950535568e-08 at step: 327200 lr 0.00020971520000000012
2025_03_04_05_02_05 Train loss: 1.1920937481590954e-07 at step: 327600 lr 0.00020971520000000012
2025_03_04_05_04_18 Train loss: 1.710481235817074e-11 at step: 328000 lr 0.00020971520000000012
2025_03_04_05_06_31 Train loss: 1.2647727917064477e-12 at step: 328400 lr 0.00020971520000000012
(Val @ epoch 72) acc: 1.0; ap: 1.0
2025_03_04_05_08_52 Train loss: 9.616553430305541e-13 at step: 328800 lr 0.00020971520000000012
2025_03_04_05_11_05 Train loss: 2.1601124444714515e-07 at step: 329200 lr 0.00020971520000000012
2025_03_04_05_13_18 Train loss: 3.2782554626464844e-07 at step: 329600 lr 0.00020971520000000012
2025_03_04_05_15_32 Train loss: 5.249497121440072e-07 at step: 330000 lr 0.00020971520000000012
2025_03_04_05_17_45 Train loss: 4.364539532195587e-14 at step: 330400 lr 0.00020971520000000012
2025_03_04_05_19_59 Train loss: 1.7116964556862513e-07 at step: 330800 lr 0.00020971520000000012
2025_03_04_05_22_12 Train loss: 8.99538363796637e-08 at step: 331200 lr 0.00020971520000000012
2025_03_04_05_24_26 Train loss: 5.991741238770996e-15 at step: 331600 lr 0.00020971520000000012
2025_03_04_05_26_39 Train loss: 7.323920726776123e-05 at step: 332000 lr 0.00020971520000000012
2025_03_04_05_28_52 Train loss: 6.934050222895394e-09 at step: 332400 lr 0.00020971520000000012
2025_03_04_05_31_06 Train loss: 2.9791023814507156e-11 at step: 332800 lr 0.00020971520000000012
(Val @ epoch 73) acc: 1.0; ap: 1.0
2025_03_04_05_33_26 Train loss: 8.398495657604976e-13 at step: 333200 lr 0.00020971520000000012
2025_03_04_05_35_39 Train loss: 5.968649929855019e-08 at step: 333600 lr 0.00020971520000000012
2025_03_04_05_37_53 Train loss: 1.0044852949420147e-08 at step: 334000 lr 0.00020971520000000012
2025_03_04_05_40_06 Train loss: 4.743050570354956e-13 at step: 334400 lr 0.00020971520000000012
2025_03_04_05_42_20 Train loss: 1.1920931797249068e-07 at step: 334800 lr 0.00020971520000000012
2025_03_04_05_44_33 Train loss: 2.1770295788314797e-08 at step: 335200 lr 0.00020971520000000012
2025_03_04_05_46_47 Train loss: 4.86638346046675e-05 at step: 335600 lr 0.00020971520000000012
2025_03_04_05_49_00 Train loss: 9.16780453635857e-12 at step: 336000 lr 0.00020971520000000012
2025_03_04_05_51_14 Train loss: 6.282111332869822e-13 at step: 336400 lr 0.00020971520000000012
2025_03_04_05_53_27 Train loss: 3.954239158332484e-09 at step: 336800 lr 0.00020971520000000012
2025_03_04_05_55_41 Train loss: 3.5479799862514483e-06 at step: 337200 lr 0.00020971520000000012
(Val @ epoch 74) acc: 1.0; ap: 1.0
2025_03_04_05_58_01 Train loss: 5.934446676292282e-07 at step: 337600 lr 0.00020971520000000012
2025_03_04_06_00_14 Train loss: 1.4384206499518104e-11 at step: 338000 lr 0.00020971520000000012
2025_03_04_06_02_28 Train loss: 3.8743215213798976e-07 at step: 338400 lr 0.00020971520000000012
2025_03_04_06_04_41 Train loss: 2.552876576800145e-09 at step: 338800 lr 0.00020971520000000012
2025_03_04_06_06_54 Train loss: 2.0996099824033276e-12 at step: 339200 lr 0.00020971520000000012
2025_03_04_06_09_08 Train loss: 2.6940008925925574e-13 at step: 339600 lr 0.00020971520000000012
2025_03_04_06_11_21 Train loss: 6.904578991884236e-11 at step: 340000 lr 0.00020971520000000012
2025_03_04_06_13_35 Train loss: 1.109765523921169e-08 at step: 340400 lr 0.00020971520000000012
2025_03_04_06_15_48 Train loss: 7.381783444053436e-13 at step: 340800 lr 0.00020971520000000012
2025_03_04_06_18_02 Train loss: 8.335633615644267e-13 at step: 341200 lr 0.00020971520000000012
2025_03_04_06_20_15 Train loss: 9.694425262551931e-10 at step: 341600 lr 0.00020971520000000012
2025_03_04_06_22_28 Train loss: 7.600377168988717e-14 at step: 342000 lr 0.00020971520000000012
(Val @ epoch 75) acc: 1.0; ap: 1.0
2025_03_04_06_24_49 Train loss: 3.605412357554627e-10 at step: 342400 lr 0.00020971520000000012
2025_03_04_06_27_02 Train loss: 8.802811994959581e-13 at step: 342800 lr 0.00020971520000000012
2025_03_04_06_29_15 Train loss: 5.313188222721976e-10 at step: 343200 lr 0.00020971520000000012
2025_03_04_06_31_29 Train loss: 5.381666987047673e-11 at step: 343600 lr 0.00020971520000000012
2025_03_04_06_33_42 Train loss: 7.355260572426605e-15 at step: 344000 lr 0.00020971520000000012
2025_03_04_06_35_56 Train loss: 2.8136733394579228e-12 at step: 344400 lr 0.00020971520000000012
2025_03_04_06_38_09 Train loss: 3.777539479138392e-12 at step: 344800 lr 0.00020971520000000012
2025_03_04_06_40_22 Train loss: 3.3480913597117024e-09 at step: 345200 lr 0.00020971520000000012
2025_03_04_06_42_36 Train loss: 4.313703474284614e-12 at step: 345600 lr 0.00020971520000000012
2025_03_04_06_44_49 Train loss: 1.7683859585004846e-11 at step: 346000 lr 0.00020971520000000012
2025_03_04_06_47_03 Train loss: 1.8287784481207936e-12 at step: 346400 lr 0.00020971520000000012
(Val @ epoch 76) acc: 1.0; ap: 1.0
2025_03_04_06_49_23 Train loss: 1.7521795125929884e-09 at step: 346800 lr 0.00020971520000000012
2025_03_04_06_51_36 Train loss: 1.4634291567732112e-14 at step: 347200 lr 0.00020971520000000012
2025_03_04_06_53_49 Train loss: 4.989509917426374e-14 at step: 347600 lr 0.00020971520000000012
2025_03_04_06_56_03 Train loss: 1.8821704088622937e-06 at step: 348000 lr 0.00020971520000000012
2025_03_04_06_58_16 Train loss: 4.8099795257883216e-09 at step: 348400 lr 0.00020971520000000012
2025_03_04_07_00_30 Train loss: 7.763303888630446e-10 at step: 348800 lr 0.00020971520000000012
2025_03_04_07_02_43 Train loss: 2.3086443867725848e-09 at step: 349200 lr 0.00020971520000000012
2025_03_04_07_04_57 Train loss: 3.749668744311386e-16 at step: 349600 lr 0.00020971520000000012
2025_03_04_07_07_10 Train loss: 2.297131668910213e-13 at step: 350000 lr 0.00020971520000000012
2025_03_04_07_09_23 Train loss: 1.429123200090757e-15 at step: 350400 lr 0.00020971520000000012
2025_03_04_07_11_37 Train loss: 3.4329520080255706e-16 at step: 350800 lr 0.00020971520000000012
(Val @ epoch 77) acc: 1.0; ap: 1.0
2025_03_04_07_13_57 Train loss: 5.128816127353275e-08 at step: 351200 lr 0.00020971520000000012
2025_03_04_07_16_10 Train loss: 2.9802322387695312e-08 at step: 351600 lr 0.00020971520000000012
2025_03_04_07_18_24 Train loss: 1.5281792563491743e-15 at step: 352000 lr 0.00020971520000000012
2025_03_04_07_20_37 Train loss: 1.591126813638155e-10 at step: 352400 lr 0.00020971520000000012
2025_03_04_07_22_51 Train loss: 0.005955483764410019 at step: 352800 lr 0.00020971520000000012
2025_03_04_07_25_04 Train loss: 1.0824865072508408e-14 at step: 353200 lr 0.00020971520000000012
2025_03_04_07_27_17 Train loss: 3.108661417172698e-08 at step: 353600 lr 0.00020971520000000012
2025_03_04_07_29_31 Train loss: 1.9684433937072754e-05 at step: 354000 lr 0.00020971520000000012
2025_03_04_07_31_44 Train loss: 1.7504035720872224e-10 at step: 354400 lr 0.00020971520000000012
2025_03_04_07_33_58 Train loss: 4.93057688861348e-13 at step: 354800 lr 0.00020971520000000012
2025_03_04_07_36_11 Train loss: 1.1926256604510854e-07 at step: 355200 lr 0.00020971520000000012
(Val @ epoch 78) acc: 1.0; ap: 1.0
2025_03_04_07_38_31 Train loss: 1.2348180637722361e-11 at step: 355600 lr 0.00020971520000000012
2025_03_04_07_40_45 Train loss: 5.801711608910409e-07 at step: 356000 lr 0.00020971520000000012
2025_03_04_07_42_58 Train loss: 5.3853158911432164e-12 at step: 356400 lr 0.00020971520000000012
2025_03_04_07_45_11 Train loss: 1.2865108978132866e-10 at step: 356800 lr 0.00020971520000000012
2025_03_04_07_47_25 Train loss: 2.6113999851418157e-09 at step: 357200 lr 0.00020971520000000012
2025_03_04_07_49_38 Train loss: 4.5444166008393183e-13 at step: 357600 lr 0.00020971520000000012
2025_03_04_07_51_52 Train loss: 8.94069742685133e-08 at step: 358000 lr 0.00020971520000000012
2025_03_04_07_54_05 Train loss: 4.2629869079213734e-10 at step: 358400 lr 0.00020971520000000012
2025_03_04_07_56_19 Train loss: 1.4196005482247642e-09 at step: 358800 lr 0.00020971520000000012
2025_03_04_07_58_32 Train loss: 6.323316537049095e-09 at step: 359200 lr 0.00020971520000000012
2025_03_04_08_00_45 Train loss: 5.534023057041375e-16 at step: 359600 lr 0.00020971520000000012
2025_03_04_08_02_59 Train loss: 9.681780388254985e-14 at step: 360000 lr 0.00020971520000000012
(Val @ epoch 79) acc: 1.0; ap: 1.0
2025_03_04_08_05_19 Train loss: 1.179321174049619e-08 at step: 360400 lr 0.00020971520000000012
2025_03_04_08_07_32 Train loss: 4.275159653371885e-12 at step: 360800 lr 0.00020971520000000012
2025_03_04_08_09_46 Train loss: 8.97191443181974e-15 at step: 361200 lr 0.00020971520000000012
2025_03_04_08_11_59 Train loss: 3.5860442219870503e-12 at step: 361600 lr 0.00020971520000000012
2025_03_04_08_14_13 Train loss: 9.06324544303061e-08 at step: 362000 lr 0.00020971520000000012
2025_03_04_08_16_26 Train loss: 3.084249244889037e-10 at step: 362400 lr 0.00020971520000000012
2025_03_04_08_18_40 Train loss: 1.0033453928803837e-10 at step: 362800 lr 0.00020971520000000012
2025_03_04_08_20_53 Train loss: 2.4099961137835635e-08 at step: 363200 lr 0.00020971520000000012
2025_03_04_08_23_07 Train loss: 5.565099107689359e-17 at step: 363600 lr 0.00020971520000000012
2025_03_04_08_25_20 Train loss: 7.131584180035384e-19 at step: 364000 lr 0.00020971520000000012
2025_03_04_08_27_33 Train loss: 1.1057075255394011e-09 at step: 364400 lr 0.00020971520000000012
2025_03_04_08_28_34 changing lr at the end of epoch 80, iters 364581
*************************
Changing lr from 0.00020971520000000012 to 0.0001677721600000001
*************************
(Val @ epoch 80) acc: 1.0; ap: 1.0
2025_03_04_08_29_53 Train loss: 5.156836985540857e-25 at step: 364800 lr 0.0001677721600000001
2025_03_04_08_32_07 Train loss: 2.7217150652377245e-15 at step: 365200 lr 0.0001677721600000001
2025_03_04_08_34_20 Train loss: 9.290042298525902e-18 at step: 365600 lr 0.0001677721600000001
2025_03_04_08_36_34 Train loss: 1.8065281529814486e-14 at step: 366000 lr 0.0001677721600000001
2025_03_04_08_38_47 Train loss: 1.3266634318886972e-11 at step: 366400 lr 0.0001677721600000001
2025_03_04_08_41_01 Train loss: 1.877909596714744e-07 at step: 366800 lr 0.0001677721600000001
2025_03_04_08_43_14 Train loss: 3.171612549413699e-14 at step: 367200 lr 0.0001677721600000001
2025_03_04_08_45_28 Train loss: 4.1130360848650285e-16 at step: 367600 lr 0.0001677721600000001
2025_03_04_08_47_41 Train loss: 2.7737294683305436e-09 at step: 368000 lr 0.0001677721600000001
2025_03_04_08_49_54 Train loss: 9.633981641415067e-15 at step: 368400 lr 0.0001677721600000001
2025_03_04_08_52_08 Train loss: 2.1580447082669707e-06 at step: 368800 lr 0.0001677721600000001
(Val @ epoch 81) acc: 1.0; ap: 1.0
2025_03_04_08_54_28 Train loss: 7.430180914980156e-10 at step: 369200 lr 0.0001677721600000001
2025_03_04_08_56_41 Train loss: 1.2809340448147882e-13 at step: 369600 lr 0.0001677721600000001
2025_03_04_08_58_55 Train loss: 3.193369268290284e-16 at step: 370000 lr 0.0001677721600000001
2025_03_04_09_01_08 Train loss: 6.003906139612214e-13 at step: 370400 lr 0.0001677721600000001
2025_03_04_09_03_22 Train loss: 1.3152542316507564e-16 at step: 370800 lr 0.0001677721600000001
2025_03_04_09_05_35 Train loss: 3.080069185812384e-12 at step: 371200 lr 0.0001677721600000001
2025_03_04_09_07_49 Train loss: 1.7642927049865698e-11 at step: 371600 lr 0.0001677721600000001
2025_03_04_09_10_02 Train loss: 2.9802322387695312e-08 at step: 372000 lr 0.0001677721600000001
2025_03_04_09_12_15 Train loss: 8.327637455215597e-15 at step: 372400 lr 0.0001677721600000001
2025_03_04_09_14_29 Train loss: 2.0245164052135056e-12 at step: 372800 lr 0.0001677721600000001
2025_03_04_09_16_42 Train loss: 1.656195078061737e-20 at step: 373200 lr 0.0001677721600000001
(Val @ epoch 82) acc: 1.0; ap: 1.0
2025_03_04_09_19_02 Train loss: 6.74440587479478e-17 at step: 373600 lr 0.0001677721600000001
2025_03_04_09_21_16 Train loss: 8.17239765443789e-12 at step: 374000 lr 0.0001677721600000001
2025_03_04_09_23_29 Train loss: 6.784560435159348e-14 at step: 374400 lr 0.0001677721600000001
2025_03_04_09_25_43 Train loss: 5.264313127606715e-20 at step: 374800 lr 0.0001677721600000001
2025_03_04_09_27_56 Train loss: 1.8256524070281444e-10 at step: 375200 lr 0.0001677721600000001
2025_03_04_09_30_09 Train loss: 1.7927826378386774e-13 at step: 375600 lr 0.0001677721600000001
2025_03_04_09_32_23 Train loss: 2.0145144233318923e-12 at step: 376000 lr 0.0001677721600000001
2025_03_04_09_34_36 Train loss: 5.266600483088031e-14 at step: 376400 lr 0.0001677721600000001
2025_03_04_09_36_50 Train loss: 1.6328653923772252e-14 at step: 376800 lr 0.0001677721600000001
2025_03_04_09_39_03 Train loss: 5.456074638312061e-14 at step: 377200 lr 0.0001677721600000001
2025_03_04_09_41_17 Train loss: 3.08310924106086e-22 at step: 377600 lr 0.0001677721600000001
2025_03_04_09_43_30 Train loss: 2.2923867597462837e-14 at step: 378000 lr 0.0001677721600000001
(Val @ epoch 83) acc: 1.0; ap: 1.0
2025_03_04_09_45_50 Train loss: 3.630937217558028e-11 at step: 378400 lr 0.0001677721600000001
2025_03_04_09_48_04 Train loss: 1.6958515147713626e-10 at step: 378800 lr 0.0001677721600000001
2025_03_04_09_50_17 Train loss: 5.95760333427231e-12 at step: 379200 lr 0.0001677721600000001
2025_03_04_09_52_30 Train loss: 1.277918499908992e-06 at step: 379600 lr 0.0001677721600000001
2025_03_04_09_54_44 Train loss: 2.0855747684400683e-12 at step: 380000 lr 0.0001677721600000001
2025_03_04_09_56_57 Train loss: 2.8318749955157067e-17 at step: 380400 lr 0.0001677721600000001
2025_03_04_09_59_11 Train loss: 6.325361762149484e-11 at step: 380800 lr 0.0001677721600000001
2025_03_04_10_01_24 Train loss: 1.4207738480989055e-14 at step: 381200 lr 0.0001677721600000001
2025_03_04_10_03_38 Train loss: 1.1920928955078125e-07 at step: 381600 lr 0.0001677721600000001
2025_03_04_10_05_51 Train loss: 5.051339002707907e-15 at step: 382000 lr 0.0001677721600000001
2025_03_04_10_08_04 Train loss: 6.557458569200758e-12 at step: 382400 lr 0.0001677721600000001
(Val @ epoch 84) acc: 1.0; ap: 1.0
*************************
2025_03_04_10_09_13
(0 progan    ) acc: 99.8; ap: 100.0
(1 stylegan  ) acc: 94.2; ap: 99.8
(2 stylegan2 ) acc: 90.3; ap: 99.6
(3 biggan    ) acc: 89.3; ap: 94.7
(4 cyclegan  ) acc: 90.7; ap: 97.7
(5 stargan   ) acc: 95.4; ap: 100.0
(6 gaugan    ) acc: 87.4; ap: 96.1
(7 deepfake  ) acc: 62.8; ap: 77.3
(8 Mean      ) acc: 88.7; ap: 95.7
*************************
2025_03_04_10_14_52
Saving model ./checkpoints/4class-resnet-car-cat-chair-horse2025_03_02_19_44_43/model_epoch_last.pth
